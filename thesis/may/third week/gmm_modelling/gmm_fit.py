# -*- coding: utf-8 -*-
"""gmm_fit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u0H8ZiiswwOFITCLOnw8JYvz_p9u06tE
"""

import numpy as np
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import vtk
from tqdm import tqdm

def fit_1d_gmm_pytorch(
    data,
    n_components=3,
    max_iter=50,
    tol=1e-6,
    device="cuda"
):
    """
    Fit a 1D GMM with `n_components` on `data` (a 1D numpy array) using EM.
    Returns (weights, means, stds), each of size n_components.
    """
    data = torch.tensor(data, dtype=torch.float32, device=device)
    N = data.shape[0]

    # Initialization: uniform weights, random means near data mean, and stds based on data std.
    torch.manual_seed(123)
    weights = (torch.ones(n_components, device=device) / n_components)
    means = torch.randn(n_components, device=device) * data.std() + data.mean()
    stds = (torch.ones(n_components, device=device) * data.std())

    prev_log_likelihood = -np.inf

    for iteration in range(max_iter):
        # E-step: Compute responsibilities for each component
        data_expanded = data.unsqueeze(1)  # shape (N,1)
        log_probs = -0.5 * np.log(2.0 * np.pi) - torch.log(stds) - 0.5 * ((data_expanded - means)**2 / (stds**2)) #shape(N,3)
        log_probs += torch.log(weights)#(N,3)

        # Normalize using log-sum-exp trick for numerical stability
        max_log = torch.max(log_probs, dim=1, keepdim=True)[0] #(N,1)
        log_probs_normalized = log_probs - max_log#(N,3)
        resp_unnormalized = torch.exp(log_probs_normalized)#(N,3)
        resp_denom = torch.sum(resp_unnormalized, dim=1, keepdim=True)#(N,1)
        responsibilities = resp_unnormalized / resp_denom#(N,3)

        # M-step: Update parameters based on responsibilities
        Nk = torch.sum(responsibilities, dim=0)#(1,3)
        weights = Nk / N#(1,3)
        means = torch.sum(responsibilities * data_expanded, dim=0) / Nk #(1,3)
        var = torch.sum(responsibilities * (data_expanded - means)**2, dim=0) / Nk #(1,3)
        stds = torch.sqrt(var) #(1,3)
        stds = torch.clamp(stds, min=1e-8) #(!,3)

        # Convergence check using log-likelihood
        log_likelihood = torch.sum(torch.log(torch.sum(torch.exp(log_probs_normalized), dim=1)) + max_log.squeeze())
        if torch.abs(log_likelihood - prev_log_likelihood) < tol:
            break
        prev_log_likelihood = log_likelihood

    # Return parameters as numpy arrays
    return weights.detach().cpu().numpy(), means.detach().cpu().numpy(), stds.detach().cpu().numpy()

# Prepare output arrays for the 9 GMM parameters: 3 means, 3 std devs, 3 mixing coefficients.
def create_vtk_array(name):
        arr = vtk.vtkFloatArray()
        arr.SetName(name)
        arr.SetNumberOfComponents(1)
        arr.SetNumberOfTuples(num_points)
        return arr

# Set parameters for sample generation and scaling
scale_factor = 2.0      # Multiply the standard deviation by this factor.
n_samples_per_point = 500

# Specify your input and output file names
input_filename = "isabel_gaussian.vti"
output_filename = "isabel_gmm.vti"

# Read the input .vti file using VTK
reader = vtk.vtkXMLImageDataReader()
reader.SetFileName(input_filename)
reader.Update()
image_data = reader.GetOutput()  # vtkImageData

dims = image_data.GetDimensions()  # e.g., (64, 64, 64)
num_points = dims[0] * dims[1] * dims[2]

# Retrieve the "Average" and "StandardDeviation" arrays from PointData
pd = image_data.GetPointData()
array_names = [pd.GetArrayName(i) for i in range(pd.GetNumberOfArrays())]
variables = set()
for name in array_names:
    if name.endswith("_Mean"):
        base = name.replace("_Mean", "")
        if f"{base}_Std" in array_names:
            variables.add(base)
t=""
print(f"Detected variables: {variables}")
for var in variables:
    print(f"\nProcessing variable: {var}")
    mean_array_vtk = pd.GetArray(var+"_Mean")
    std_array_vtk = pd.GetArray(var+"_Std")

    if mean_array_vtk is None or std_array_vtk is None:
        raise ValueError("Could not find 'Average' or 'Standard_Deviation' in input .vti")

    gmm_mean0_vtk = create_vtk_array(var+"_GMM_Mean0")
    gmm_mean1_vtk = create_vtk_array(var+"_GMM_Mean1")
    gmm_mean2_vtk = create_vtk_array(var+"_GMM_Mean2")
    gmm_std0_vtk  = create_vtk_array(var+"_GMM_Std0")
    gmm_std1_vtk  = create_vtk_array(var+"_GMM_Std1")
    gmm_std2_vtk  = create_vtk_array(var+"_GMM_Std2")
    gmm_w0_vtk    = create_vtk_array(var+"_GMM_Weight0")
    gmm_w1_vtk    = create_vtk_array(var+"_GMM_Weight1")
    gmm_w2_vtk    = create_vtk_array(var+"_GMM_Weight2")
    # Loop over each grid point in the volume.
    for pt_id in tqdm(range(num_points)):
        mu = mean_array_vtk.GetValue(pt_id)
        stdv = std_array_vtk.GetValue(pt_id)

        # Scale up the standard deviation
        scaled_std = scale_factor * stdv

        # Generate 500 samples from N(mu, (scaled_std)^2)
        samples = np.random.normal(loc=mu, scale=scaled_std, size=n_samples_per_point)

        # Fit a 3-component 1D GMM using the EM algorithm (implemented with PyTorch)
        weights, means, stds = fit_1d_gmm_pytorch(samples, n_components=3, max_iter=50)

        # Optional: Sort components by mean to get consistent ordering
        order = np.argsort(means)
        weights = weights[order]
        means = means[order]
        stds = stds[order]

        # Store the fitted parameters in the respective VTK arrays
        gmm_mean0_vtk.SetValue(pt_id, means[0])
        gmm_mean1_vtk.SetValue(pt_id, means[1])
        gmm_mean2_vtk.SetValue(pt_id, means[2])

        gmm_std0_vtk.SetValue(pt_id, stds[0])
        gmm_std1_vtk.SetValue(pt_id, stds[1])
        gmm_std2_vtk.SetValue(pt_id, stds[2])

        gmm_w0_vtk.SetValue(pt_id, weights[0])
        gmm_w1_vtk.SetValue(pt_id, weights[1])
        gmm_w2_vtk.SetValue(pt_id, weights[2])
       # Add the 9 new arrays to the PointData of the image
    pd.AddArray(gmm_mean0_vtk)
    pd.AddArray(gmm_mean1_vtk)
    pd.AddArray(gmm_mean2_vtk)
    pd.AddArray(gmm_std0_vtk)
    pd.AddArray(gmm_std1_vtk)
    pd.AddArray(gmm_std2_vtk)
    pd.AddArray(gmm_w0_vtk)
    pd.AddArray(gmm_w1_vtk)
    pd.AddArray(gmm_w2_vtk)

    pd.RemoveArray(var+"_Mean")
    pd.RemoveArray(var+"_Std")

    t=var

# Write the updated vtkImageData to a new .vti file
writer = vtk.vtkXMLImageDataWriter()
writer.SetFileName(output_filename)
writer.SetInputData(image_data)
writer.Write()

print(f"Done. Output written to: {output_filename}")

# Choose a grid point to visualize (e.g., the center of the volume)
center_i = (dims[0] + 4) // 2
center_j = dims[1] // 2
center_k = dims[2] // 2
center_pt_id = (center_k * dims[1] + center_j) * dims[0] + center_i

# Retrieve original parameters at the selected grid point
orig_mu = mean_array_vtk.GetValue(center_pt_id)
orig_std = std_array_vtk.GetValue(center_pt_id)
scaled_std = scale_factor * orig_std

# Generate samples for visualization
samples_center = np.random.normal(loc=orig_mu, scale=scaled_std, size=n_samples_per_point)

# Retrieve the fitted GMM parameters from the VTK arrays
mix = [
    gmm_w0_vtk.GetValue(center_pt_id),
    gmm_w1_vtk.GetValue(center_pt_id),
    gmm_w2_vtk.GetValue(center_pt_id)
]
comps_mean = [
    gmm_mean0_vtk.GetValue(center_pt_id),
    gmm_mean1_vtk.GetValue(center_pt_id),
    gmm_mean2_vtk.GetValue(center_pt_id)
]
comps_std = [
    gmm_std0_vtk.GetValue(center_pt_id),
    gmm_std1_vtk.GetValue(center_pt_id),
    gmm_std2_vtk.GetValue(center_pt_id)
]

# ----- Visualization 1: Single Scaled Gaussian -----
plt.figure()
plt.hist(samples_center, bins=30, density=True, alpha=0.5)

x_values = np.linspace(samples_center.min()-1, samples_center.max()+1, 200)
single_gaussian_pdf = (
    1.0 / (np.sqrt(2.0 * np.pi) * scaled_std) *
    np.exp(-(x_values - orig_mu)**2 / (2.0 * scaled_std**2))
)
plt.plot(x_values, single_gaussian_pdf)
plt.title("Histogram + Single Scaled Gaussian (Grid Point)for",t)
plt.xlabel("x")
plt.ylabel("Density")
plt.tight_layout()
plt.savefig(f"{t}_single_scaled_gaussian.png") # Save the figure
plt.close()

# ----- Visualization 2: Fitted 3-Component GMM -----
plt.figure()
plt.hist(samples_center, bins=30, density=True, alpha=0.5)

# Compute and plot the mixture PDF from the fitted GMM components
gmm_pdf = np.zeros_like(x_values)
for w, m, s in zip(mix, comps_mean, comps_std):
    component_pdf = (
        1.0 / (np.sqrt(2.0 * np.pi) * s) *
        np.exp(-(x_values - m)**2 / (2.0 * s**2))
    )
    gmm_pdf += w * component_pdf

plt.plot(x_values, gmm_pdf)
plt.title("Histogram + Fitted 3-Component GMM (Grid Point) for",t)
plt.xlabel("x")
plt.ylabel("Density")
plt.tight_layout()
plt.savefig(f"{t}_Fitted 3-Component GMM (Grid Point).png")  # Save the figure
plt.close()

# Visualization: Plot each individual GMM component with different colors.
plt.figure()

# Plot histogram of the samples (density normalized)
plt.hist(samples_center, bins=30, density=True, alpha=0.5, label="Samples Histogram")

# Generate x values for plotting the PDFs
x_values = np.linspace(samples_center.min()-1, samples_center.max()+1, 200)

# Plot each Gaussian component separately
for i, (w, m, s) in enumerate(zip(mix, comps_mean, comps_std)):
    component_pdf = w * (1.0 / (np.sqrt(2.0 * np.pi) * s)) * np.exp(-((x_values - m)**2) / (2.0 * s**2))
    plt.plot(x_values, component_pdf, label=f"Component {i}")

plt.title("Histogram + Individual GMM Components for",t)
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.tight_layout()
plt.savefig(f"{t}_Individual GMM Components.png")  # Save the figure
plt.close()