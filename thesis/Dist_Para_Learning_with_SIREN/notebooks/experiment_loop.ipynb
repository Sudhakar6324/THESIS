{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11041818,"sourceType":"datasetVersion","datasetId":6878023},{"sourceId":11251078,"sourceType":"datasetVersion","datasetId":7030699}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn, optim\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport vtk\nfrom vtk import *\nfrom vtk.util.numpy_support import vtk_to_numpy\nimport random\nimport os\nimport sys\nimport time\nimport csv\nfrom argparse import Namespace","metadata":{"_uuid":"d33f75d4-5445-4e48-8179-f0c5dd333f79","_cell_guid":"240889c7-74eb-4fd4-a5b1-ec2aead81dac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:42.682342Z","iopub.execute_input":"2025-04-15T12:48:42.682560Z","iopub.status.idle":"2025-04-15T12:48:54.853757Z","shell.execute_reply.started":"2025-04-15T12:48:42.682537Z","shell.execute_reply":"2025-04-15T12:48:54.853206Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint('Device running:', device)","metadata":{"_uuid":"43d958cf-562d-4fb8-9670-43075fd82174","_cell_guid":"bda30f3b-5c85-404f-8cdc-861e65b7ae69","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:54.854398Z","iopub.execute_input":"2025-04-15T12:48:54.854800Z","iopub.status.idle":"2025-04-15T12:48:54.950244Z","shell.execute_reply.started":"2025-04-15T12:48:54.854780Z","shell.execute_reply":"2025-04-15T12:48:54.949590Z"}},"outputs":[{"name":"stdout","text":"Device running: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class SineLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n        super().__init__()\n        self.omega_0 = omega_0\n        self.is_first = is_first\n        # self.enable_dropout = enable_dropout\n        # self.dropout_prob = dropout_prob\n        self.in_features = in_features\n        # if enable_dropout:\n        #     if not self.is_first:\n        #         self.dropout = nn.Dropout(dropout_prob)\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n        self.init_weights()\n\n    def init_weights(self):\n        with torch.no_grad():\n            if self.is_first:\n                self.linear.weight.uniform_(-1 / self.in_features,\n                                             1 / self.in_features)\n            else:\n                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n                                             np.sqrt(6 / self.in_features) / self.omega_0)\n\n\n    def forward(self, x):\n        x = self.linear(x)\n        # if self.enable_dropout:\n        #     if not self.is_first:\n        #         x = self.dropout(x)\n        return torch.sin(self.omega_0 * x)","metadata":{"_uuid":"c6d00f8a-3e62-40d1-81e8-b30fd44e994f","_cell_guid":"47639dbe-7c29-4716-ad12-c245edbd4925","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:54.952078Z","iopub.execute_input":"2025-04-15T12:48:54.952265Z","iopub.status.idle":"2025-04-15T12:48:54.968655Z","shell.execute_reply.started":"2025-04-15T12:48:54.952250Z","shell.execute_reply":"2025-04-15T12:48:54.968114Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ResidualSineLayer(nn.Module):\n    def __init__(self, features, bias=True, ave_first=False, ave_second=False, omega_0=30):\n        super().__init__()\n        self.omega_0 = omega_0\n        # self.enable_dropout = enable_dropout\n        # self.dropout_prob = dropout_prob\n        self.features = features\n        # if enable_dropout:\n        #     self.dropout_1 = nn.Dropout(dropout_prob)\n        self.linear_1 = nn.Linear(features, features, bias=bias)\n        self.linear_2 = nn.Linear(features, features, bias=bias)\n        self.weight_1 = .5 if ave_first else 1\n        self.weight_2 = .5 if ave_second else 1\n\n        self.init_weights()\n\n\n    def init_weights(self):\n        with torch.no_grad():\n            self.linear_1.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n                                           np.sqrt(6 / self.features) / self.omega_0)\n            self.linear_2.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n                                           np.sqrt(6 / self.features) / self.omega_0)\n\n    def forward(self, input):\n        linear_1 = self.linear_1(self.weight_1*input)\n        # if self.enable_dropout:\n        #     linear_1 = self.dropout_1(linear_1)\n        sine_1 = torch.sin(self.omega_0 * linear_1)\n        sine_2 = torch.sin(self.omega_0 * self.linear_2(sine_1))\n        return self.weight_2*(input+sine_2)","metadata":{"_uuid":"286fbfd2-ba2c-4fbe-8ac6-cfebd4a21e55","_cell_guid":"ef7cdfa6-ccb1-405d-a306-c83170e479b4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:54.969359Z","iopub.execute_input":"2025-04-15T12:48:54.969558Z","iopub.status.idle":"2025-04-15T12:48:54.987036Z","shell.execute_reply.started":"2025-04-15T12:48:54.969542Z","shell.execute_reply":"2025-04-15T12:48:54.986396Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MyResidualSirenNet(nn.Module):\n    def __init__(self, obj):\n        super(MyResidualSirenNet, self).__init__()\n        # self.enable_dropout = obj['enable_dropout']\n        # self.dropout_prob = obj['dropout_prob']\n        self.Omega_0=30\n        self.n_layers = obj['n_layers']\n        self.input_dim = obj['dim']\n        self.output_dim = obj['total_vars']\n        self.neurons_per_layer = obj['n_neurons']\n        self.layers = [self.input_dim]\n        for i in range(self.n_layers-1):\n            self.layers.append(self.neurons_per_layer)\n        self.layers.append(self.output_dim)\n        self.net_layers = nn.ModuleList()\n        for idx in np.arange(self.n_layers):\n            layer_in = self.layers[idx]\n            layer_out = self.layers[idx+1]\n            ## if not the final layer\n            if idx != self.n_layers-1:\n                ## if first layer\n                if idx==0:\n                    self.net_layers.append(SineLayer(layer_in,layer_out,bias=True,is_first=idx==0))\n                ## if an intermdeiate layer\n                else:\n                    self.net_layers.append(ResidualSineLayer(layer_in,bias=True,ave_first=idx>1,ave_second=idx==(self.n_layers-2)))\n            ## if final layer   \n            else:\n                final_linear = nn.Linear(layer_in,layer_out)\n                ## initialize weights for the final layer\n                with torch.no_grad():\n                    final_linear.weight.uniform_(-np.sqrt(6 / (layer_in)) / self.Omega_0, np.sqrt(6 / (layer_in)) / self.Omega_0)\n                self.net_layers.append(final_linear)\n\n    def forward(self,x):\n        for net_layer in self.net_layers:\n            x = net_layer(x)\n        return x","metadata":{"_uuid":"e1400ad9-b3ed-49fc-81e0-929816a0aaf2","_cell_guid":"807361ca-2c71-4799-83d0-2da9ccf9fac0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:54.987665Z","iopub.execute_input":"2025-04-15T12:48:54.987902Z","iopub.status.idle":"2025-04-15T12:48:55.014589Z","shell.execute_reply.started":"2025-04-15T12:48:54.987879Z","shell.execute_reply":"2025-04-15T12:48:55.013930Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def size_of_network(n_layers, n_neurons, d_in, d_out, is_residual = True):\n    # Adding input layer\n    layers = [d_in]\n    # layers = [3]\n\n    # Adding hidden layers\n    layers.extend([n_neurons]*n_layers)\n    # layers = [3, 5, 5, 5]\n\n    # Adding output layer\n    layers.append(d_out)\n    # layers = [3, 5, 5, 5, 1]\n\n    # Number of steps \n    n_layers = len(layers)-1\n    # n_layers = 5 - 1 = 4\n\n    n_params = 0\n\n    # np.arange(4) = [0, 1, 2, 3]\n    for ndx in np.arange(n_layers):\n\n        # number of neurons in below layer\n        layer_in = layers[ndx]\n\n        # number of neurons in above layer\n        layer_out = layers[ndx+1]\n\n        # max number of neurons in both the layer\n        og_layer_in = max(layer_in,layer_out)\n\n        # if lower layer is the input layer \n        # or the upper layer is the output layer\n        if ndx==0 or ndx==(n_layers-1):\n            # Adding weight corresponding to every neuron for every input neuron\n            # Adding bias for every neuron in the upper layer\n            n_params += ((layer_in+1)*layer_out)\n        \n        else:\n\n            # If the layer is residual then proceed as follows as there will be more weights if residual layer is included\n            if is_residual:\n                # doubt in the following two lines\n                n_params += (layer_in*og_layer_in)+og_layer_in\n                n_params += (og_layer_in*layer_out)+layer_out\n\n            # if the layer is non residual then simply add number of weights and biases as follows\n            else:\n                n_params += ((layer_in+1)*layer_out)\n            #\n        #\n    #\n\n    return n_params","metadata":{"_uuid":"49e23bf9-3de6-417b-8de1-53aaf02fd323","_cell_guid":"7342b7cd-612f-4c56-ad94-ecd81203519a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:55.015290Z","iopub.execute_input":"2025-04-15T12:48:55.015462Z","iopub.status.idle":"2025-04-15T12:48:55.036101Z","shell.execute_reply.started":"2025-04-15T12:48:55.015448Z","shell.execute_reply":"2025-04-15T12:48:55.035591Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def compute_PSNR(arrgt,arr_recon):\n    diff = arrgt - arr_recon\n    sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n    snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n    return snr","metadata":{"_uuid":"4bee2b5f-d501-4d1c-b226-410b2624c2d5","_cell_guid":"c8d04a55-6de3-4520-9cf7-45a568420b31","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:55.036787Z","iopub.execute_input":"2025-04-15T12:48:55.037012Z","iopub.status.idle":"2025-04-15T12:48:55.072213Z","shell.execute_reply.started":"2025-04-15T12:48:55.036990Z","shell.execute_reply":"2025-04-15T12:48:55.071775Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def srs(numOfPoints, valid_pts, percentage, isMaskPresent, mask_array):\n    \n    # getting total number of sampled points\n    numberOfSampledPoints = int((valid_pts/100) * percentage)\n\n    # storing corner indices in indices variable\n    indices = set()\n    \n    # As long as we don't get the required amount of sample points keep finding the random numbers\n    while(len(indices) < numberOfSampledPoints):\n        rp = random.randint(0, numOfPoints-1)\n        if isMaskPresent and mask_array[rp] == 0:\n            continue\n        indices.add(rp)\n\n    # return indices\n    return indices","metadata":{"_uuid":"9b219fbe-1596-4e3b-a0a7-6eb88db2f52d","_cell_guid":"415f4118-71de-4504-b39f-a7bd40cdcc02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:55.072821Z","iopub.execute_input":"2025-04-15T12:48:55.073017Z","iopub.status.idle":"2025-04-15T12:48:55.094114Z","shell.execute_reply.started":"2025-04-15T12:48:55.073002Z","shell.execute_reply":"2025-04-15T12:48:55.093503Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def findMultiVariatePSNR(var_name, total_vars, actual, pred):\n    # print('Printing PSNR')\n    tot = 0\n    psnr_list = []\n    for j in range(total_vars):\n        psnr = compute_PSNR(actual[:,j], pred[:,j])\n        psnr_list.append(psnr)\n        tot += psnr \n        print(var_name[j], ' PSNR:', psnr)\n    avg_psnr = tot/total_vars\n    print('\\nAverage psnr : ', avg_psnr)\n     #this function is calculating the psnr of final epoch (or whenever it is called) of each variable and then averaging it\n     #Thus individual epochs psnr is not calculated\n                                         \n    return psnr_list, avg_psnr","metadata":{"_uuid":"964befc2-020d-46d4-bd1b-ef061a65b9e8","_cell_guid":"c20e48d5-ebad-408b-b564-627ba81d02f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:55.096025Z","iopub.execute_input":"2025-04-15T12:48:55.096381Z","iopub.status.idle":"2025-04-15T12:48:55.114062Z","shell.execute_reply.started":"2025-04-15T12:48:55.096365Z","shell.execute_reply":"2025-04-15T12:48:55.113532Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def compute_rmse(actual, predicted):\n    mse = np.mean((actual - predicted) ** 2)\n    return np.sqrt(mse)\n\ndef denormalizeValue(total_vars, to, ref):\n    to_arr = np.array(to)\n    for i in range(total_vars):\n        min_data = np.min(ref[:, i])\n        max_data = np.max(ref[:, i])\n        to_arr[:, i] = (((to[:, i] * 0.5) + 0.5) * (max_data - min_data)) + min_data\n    return to_arr","metadata":{"_uuid":"2face64a-1559-415e-9527-5a581fab1db5","_cell_guid":"716ccaed-b6c8-4cc4-83a6-643495093c02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:55.114631Z","iopub.execute_input":"2025-04-15T12:48:55.114914Z","iopub.status.idle":"2025-04-15T12:48:55.132271Z","shell.execute_reply.started":"2025-04-15T12:48:55.114899Z","shell.execute_reply":"2025-04-15T12:48:55.131588Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def makeVTI(data, val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name, normalizedVersion = False):\n    nn_predictions = denormalizeValue(total_vars, n_predictions, val) if not normalizedVersion else n_predictions\n    writer = vtkXMLImageDataWriter()\n    writer.SetFileName(vti_path + vti_name)\n    img = vtkImageData()\n    img.CopyStructure(data)\n    if not isMaskPresent:\n        for i in range(total_vars):\n            f = var_name[i]\n            temp = nn_predictions[:, i]\n            arr = vtkFloatArray()\n            for j in range(n_pts):\n                arr.InsertNextValue(temp[j])\n            arr.SetName(f)\n            img.GetPointData().AddArray(arr)\n        # print(img)\n        writer.SetInputData(img)\n        writer.Write()\n        print(f'Vti File written successfully at {vti_path}{vti_name}')\n    else:\n        for i in range(total_vars):\n            f = var_name[i]\n            temp = nn_predictions[:, i]\n            idx = 0\n            arr = vtkFloatArray()\n            for j in range(n_pts):\n                if(mask_arr[j] == 1):\n                    arr.InsertNextValue(temp[idx])\n                    idx += 1\n                else:\n                    arr.InsertNextValue(0.0)\n            arr.SetName('p_' + f)\n            data.GetPointData().AddArray(arr)\n        # print(data)\n        writer.SetInputData(data)\n        writer.Write()\n        print(f'Vti File written successfully at {vti_path}{vti_name}')","metadata":{"_uuid":"88d4128a-9484-48bb-b8cd-21abe4801eb1","_cell_guid":"4d173df2-a1ab-442d-8eeb-bd0f1e71b9ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:55.133080Z","iopub.execute_input":"2025-04-15T12:48:55.133276Z","iopub.status.idle":"2025-04-15T12:48:55.152438Z","shell.execute_reply.started":"2025-04-15T12:48:55.133260Z","shell.execute_reply":"2025-04-15T12:48:55.151762Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def getImageData(actual_img, val, n_pts, var_name, isMaskPresent, mask_arr):\n    img = vtkImageData()\n    img.CopyStructure(actual_img)\n    # if isMaskPresent:\n    #     img.DeepCopy(actual_img)\n    # img.SetDimensions(dim)\n    # img.SetOrigin(actual_img.GetOrigin())\n    # img.SetSpacing(actual_img.GetSpacing())\n    if not isMaskPresent:\n        f = var_name\n        data = val\n        arr = vtkFloatArray()\n        for j in range(n_pts):\n            arr.InsertNextValue(data[j])\n        arr.SetName(f)\n        img.GetPointData().SetScalars(arr)\n    else:\n        f = var_name\n        data = val\n        idx = 0\n        arr = vtkFloatArray()\n        for j in range(n_pts):\n            if(mask_arr[j] == 1):\n                arr.InsertNextValue(data[idx])\n                idx += 1\n            else:\n                arr.InsertNextValue(0.0)\n        arr.SetName(f)\n        img.GetPointData().SetScalars(arr)\n    return img","metadata":{"_uuid":"99a2cbb5-4737-4a6d-b398-c37e117de5a7","_cell_guid":"7042e60f-eafc-446d-87ab-ec4298dd9276","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T12:48:55.153125Z","iopub.execute_input":"2025-04-15T12:48:55.153331Z","iopub.status.idle":"2025-04-15T12:48:55.174530Z","shell.execute_reply.started":"2025-04-15T12:48:55.153308Z","shell.execute_reply":"2025-04-15T12:48:55.173891Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# 1. Setup the parameter lists you want to iterate over\n# ----------------------------------------------------------------------------\nn_neurons_list = [480]\nn_layers_list = [4, 8]      # These are the hidden layers (the code adds +2 internally)\nbatch_size_list = [512, 1024]\nlearning_rate_list = [5e-5, 1e-5, 5e-6]\n\n# You can also configure other fixed parameters here if you wish\nMAX_EPOCH = 100\ndecay = True \ndecay_rate = 0.8\ndecay_at_equal_interval = True\ndecay_interval = 15\n\n# Paths\ndatapath = '/kaggle/input/fit-gmm3/gmm3.vti'  # Update to your data path\noutpath = './models/'\ndataset_name = '3d_data'\nvti_name = 'predicted_vti'\nvti_path = './data/pred.vti'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:48:55.175340Z","iopub.execute_input":"2025-04-15T12:48:55.175573Z","iopub.status.idle":"2025-04-15T12:48:55.199260Z","shell.execute_reply.started":"2025-04-15T12:48:55.175553Z","shell.execute_reply":"2025-04-15T12:48:55.198774Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print(\"Reading .vti file from:\", datapath)\nreader = vtk.vtkXMLImageDataReader()\nreader.SetFileName(datapath)\nreader.Update()\n\ndata = reader.GetOutput()\npdata = data.GetPointData()\n\nn_pts = data.GetNumberOfPoints()\ndim = data.GetDimensions()\nn_dim = len(dim)\ntotal_arr = pdata.GetNumberOfArrays()\n\nprint(\"n_pts:\", n_pts, \"dim:\", dim, \"n_dim:\", n_dim, \"total_arr:\", total_arr)\n\nvar_name = []\ndata_array = []\n\n# --- Extract arrays from .vti ---\nfor i in range(total_arr):\n    a_name = pdata.GetArrayName(i)\n    cur_arr = pdata.GetArray(a_name)\n    n_components = cur_arr.GetNumberOfComponents()\n    \n    if n_components == 1:\n        var_name.append(a_name)\n        data_array.append(vtk_to_numpy(cur_arr))\n    else:\n        # If array has multiple components (e.g. vector), split them\n        component_names = [f\"{a_name}_{c}\" for c in ['x','y','z'][:n_components]]\n        var_name.extend(component_names)\n        for c in range(n_components):\n            c_data = [cur_arr.GetComponent(j, c) for j in range(n_pts)]\n            data_array.append(np.array(c_data))\n\ntotal_vars = len(var_name)\nunivariate = (total_vars == 1)\n\n# Prepare coordinate and value arrays\ncord = np.zeros((n_pts, n_dim), dtype=np.float32)\nval = np.zeros((n_pts, total_vars), dtype=np.float32)\n\nfor i in range(n_pts):\n    pt = data.GetPoint(i)  # (x, y, z) or (x, y) etc. if 2D\n    cord[i, :] = pt\n    val[i, :] = [arr[i] for arr in data_array]\n\nprint(\"Total Variables:\", total_vars)\nprint(\"Univariate:\", univariate)\nprint(\"Coordinates Shape:\", cord.shape)\nprint(\"Values Shape:\", val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:48:55.199878Z","iopub.execute_input":"2025-04-15T12:48:55.200103Z","iopub.status.idle":"2025-04-15T12:48:56.257406Z","shell.execute_reply.started":"2025-04-15T12:48:55.200083Z","shell.execute_reply":"2025-04-15T12:48:56.256801Z"}},"outputs":[{"name":"stdout","text":"Reading .vti file from: /kaggle/input/fit-gmm3/gmm3.vti\nn_pts: 262144 dim: (64, 64, 64) n_dim: 3 total_arr: 9\nTotal Variables: 9\nUnivariate: False\nCoordinates Shape: (262144, 3)\nValues Shape: (262144, 9)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# --- Normalize values to [-1, 1] ---\nfor i in range(total_vars):\n    min_data = np.min(val[:, i])\n    max_data = np.max(val[:, i])\n    val[:, i] = 2.0 * ((val[:, i] - min_data) / (max_data - min_data) - 0.5)\n\n# --- Normalize coordinates to [-1, 1] ---\nfor i in range(n_dim):\n    # Range is [0, dim[i]-1], so we divide by (dim[i]-1)\n    cord[:, i] = 2.0 * (cord[:, i] / (dim[i] - 1) - 0.5)\n\n# Convert numpy arrays to PyTorch tensors\ntorch_coords_full = torch.from_numpy(cord)\ntorch_vals_full = torch.from_numpy(val)\n\nfull_dataset = TensorDataset(torch_coords_full, torch_vals_full)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:48:56.258118Z","iopub.execute_input":"2025-04-15T12:48:56.258334Z","iopub.status.idle":"2025-04-15T12:48:56.322559Z","shell.execute_reply.started":"2025-04-15T12:48:56.258317Z","shell.execute_reply":"2025-04-15T12:48:56.322084Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:48:56.323616Z","iopub.execute_input":"2025-04-15T12:48:56.323876Z","iopub.status.idle":"2025-04-15T12:48:56.459121Z","shell.execute_reply.started":"2025-04-15T12:48:56.323854Z","shell.execute_reply":"2025-04-15T12:48:56.458169Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"results_file = \"experiment_results.csv\"\n# Check if results file exists to know if we need a header\nwrite_header = not os.path.isfile(results_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:48:56.460129Z","iopub.execute_input":"2025-04-15T12:48:56.460416Z","iopub.status.idle":"2025-04-15T12:48:56.464292Z","shell.execute_reply.started":"2025-04-15T12:48:56.460391Z","shell.execute_reply":"2025-04-15T12:48:56.463633Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Create (or append) to CSV\nwith open(results_file, 'a', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    if write_header:\n        writer.writerow([\"n_neurons\", \"n_layers_total\", \"batch_size\", \"learning_rate\", \"psnr\", \"rmse\"])\n    \n    # Nested loops for each hyperparam\n    for n_neurons in n_neurons_list:\n        for hidden_layers in n_layers_list:\n            for batch_size in batch_size_list:\n                for lr in learning_rate_list:\n\n                    # ----------------------------------------------------------------\n                    # 4. Set up a fresh training run for this particular combination\n                    # ----------------------------------------------------------------\n                    \n                    # Here we simulate argparse with the chosen hyperparams\n                    # We'll store `hidden_layers` in args.n_layers.\n                    # The original script then does `n_layers = args.n_layers + 2`.\n                    # That means total layers = hidden_layers + 2 (input + output).\n                    args = Namespace(\n                        n_neurons = n_neurons,\n                        n_layers = hidden_layers,       # note: the code adds +2 inside\n                        epochs = MAX_EPOCH,\n                        batchsize = batch_size,\n                        lr = lr,\n                        no_decay = (not decay),\n                        decay_rate = decay_rate,\n                        decay_at_interval = decay_at_equal_interval,\n                        decay_interval = decay_interval,\n                        \n                        datapath = datapath,\n                        outpath = outpath,\n                        exp_path = './logs/',  # or wherever you want\n                        modified_data_path = './data/',\n                        dataset_name = dataset_name,\n                        vti_name = vti_name,\n                        vti_path = vti_path\n                    )\n\n                    print(\"\\n====================================================\")\n                    print(\"Starting training with hyperparams:\")\n                    print(\"n_neurons =\", n_neurons)\n                    print(\"hidden_layers =\", hidden_layers, \"(+2 = total layers in the net)\")\n                    print(\"batch_size =\", batch_size)\n                    print(\"learning_rate =\", lr)\n                    print(\"====================================================\\n\")\n\n                    # ----------------\n                    # Setup Dataloader\n                    # ----------------\n                    train_dataloader = DataLoader(\n                        full_dataset,\n                        batch_size=batch_size,\n                        pin_memory=True,\n                        shuffle=True,\n                        num_workers=4\n                    )\n\n                    # Because the original code references these variables inside:\n                    # We'll replicate the snippet that extracts them from `args`.\n                    decay = not args.no_decay\n                    MAX_EPOCH = args.epochs\n                    n_layers_total = args.n_layers + 2  # input + hidden + output\n                    BATCH_SIZE = args.batchsize\n                    LR = args.lr\n\n                    # Build the model config dictionary\n                    model_config = {\n                        'total_vars': total_vars,\n                        'dim': n_dim,\n                        'n_neurons': args.n_neurons,\n                        'n_layers': n_layers_total\n                    }\n\n                    # Create model/optimizer/loss fresh each time\n                    model = MyResidualSirenNet(model_config).to(device)\n                    optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999))\n                    criterion = nn.MSELoss()\n\n                    # Training loop\n                    best_loss = 1e8\n                    best_epoch = -1\n                    train_loss_list = []\n\n                    for epoch in range(MAX_EPOCH):\n                        model.train()\n                        temp_loss_list = []\n                        start = time.time()\n\n                        for X_train, y_train in train_dataloader:\n                            X_train = X_train.type(torch.float32).to(device)\n                            y_train = y_train.type(torch.float32).to(device)\n\n                            if univariate:\n                                y_train = y_train.squeeze()\n\n                            optimizer.zero_grad()\n                            predictions = model(X_train)\n                            predictions = predictions.squeeze()\n                            loss = criterion(predictions, y_train)\n                            loss.backward()\n                            optimizer.step()\n\n                            temp_loss_list.append(loss.detach().cpu().numpy())\n\n                        epoch_loss = np.mean(temp_loss_list)\n\n                        # Learning rate decay logic\n                        if decay:\n                            if decay_at_equal_interval:\n                                if epoch >= decay_interval and epoch % decay_interval == 0:\n                                    for param_group in optimizer.param_groups:\n                                        param_group['lr'] *= decay_rate\n                            else:\n                                if epoch > 0 and epoch_loss > train_loss_list[-1]:\n                                    for param_group in optimizer.param_groups:\n                                        param_group['lr'] *= decay_rate\n\n                        train_loss_list.append(epoch_loss)\n                        if epoch_loss < best_loss:\n                            best_loss = epoch_loss\n                            best_epoch = epoch + 1\n\n                        end = time.time()\n                        print(f\"[Epoch {epoch+1}/{MAX_EPOCH}] Loss: {epoch_loss:.6f} | \"\n                              f\"Time: {end-start:.2f}s | LR: {optimizer.param_groups[0]['lr']:.8f}\")\n\n                    # After training finishes, do final predictions\n                    print(\"\\nFinished training.\")\n                    print(\"Best epoch:\", best_epoch, \"with loss:\", best_loss)\n\n                    # Prediction phase\n                    model.eval()\n                    prediction_list = [[] for _ in range(total_vars)]\n\n                    group_size = 5000\n                    with torch.no_grad():\n                        for i in range(0, torch_coords_full.shape[0], group_size):\n                            coords = torch_coords_full[i:i+group_size].type(torch.float32).to(device)\n                            vals = model(coords)\n                            vals = vals.cpu()\n\n                            for j in range(total_vars):\n                                prediction_list[j].append(vals[:, j])\n\n                    extracted_list = [[] for _ in range(total_vars)]\n                    for i in range(len(prediction_list[0])):\n                        for j in range(total_vars):\n                            el = prediction_list[j][i].detach().numpy()\n                            extracted_list[j].append(el)\n                    for j in range(total_vars):\n                        extracted_list[j] = np.concatenate(extracted_list[j], dtype='float32')\n\n                    n_predictions = np.array(extracted_list).T  # (n_pts, total_vars)\n\n                    # Compute PSNR and RMSE (assuming these return numeric values)\n                    psnr_value = findMultiVariatePSNR(var_name, total_vars, torch_vals_full.numpy(), n_predictions)\n                    rmse_value = compute_rmse(torch_vals_full.numpy(), n_predictions)\n\n                    print(\"PSNR:\", psnr_value)\n                    print(\"RMSE:\", rmse_value)\n\n                    # ------------------------------------------------------------\n                    # Append one row to experiment_results.csv for this experiment\n                    # ------------------------------------------------------------\n                    writer.writerow([\n                        n_neurons,\n                        n_layers_total,  # total layers after +2\n                        batch_size,\n                        lr,\n                        psnr_value,\n                        rmse_value\n                    ])\n\n                    print(\"\\n*** Results appended to 'experiment_results.csv' ***\\n\")\n                    # End of hyperparameter combination\n\n# End of script\nprint(\"All experiments completed. Check 'experiment_results.csv' for results.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T12:48:56.464995Z","iopub.execute_input":"2025-04-15T12:48:56.465217Z","iopub.status.idle":"2025-04-15T13:53:45.194267Z","shell.execute_reply.started":"2025-04-15T12:48:56.465197Z","shell.execute_reply":"2025-04-15T13:53:45.193511Z"}},"outputs":[{"name":"stdout","text":"\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 4 (+2 = total layers in the net)\nbatch_size = 512\nlearning_rate = 5e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.005885 | Time: 4.07s | LR: 0.00005000\n[Epoch 2/100] Loss: 0.002798 | Time: 3.08s | LR: 0.00005000\n[Epoch 3/100] Loss: 0.002720 | Time: 3.19s | LR: 0.00005000\n[Epoch 4/100] Loss: 0.002666 | Time: 3.49s | LR: 0.00005000\n[Epoch 5/100] Loss: 0.002629 | Time: 3.37s | LR: 0.00005000\n[Epoch 6/100] Loss: 0.002587 | Time: 3.38s | LR: 0.00005000\n[Epoch 7/100] Loss: 0.002566 | Time: 3.46s | LR: 0.00005000\n[Epoch 8/100] Loss: 0.002536 | Time: 3.38s | LR: 0.00005000\n[Epoch 9/100] Loss: 0.002531 | Time: 3.36s | LR: 0.00005000\n[Epoch 10/100] Loss: 0.002507 | Time: 3.45s | LR: 0.00005000\n[Epoch 11/100] Loss: 0.002484 | Time: 3.23s | LR: 0.00005000\n[Epoch 12/100] Loss: 0.002482 | Time: 3.13s | LR: 0.00005000\n[Epoch 13/100] Loss: 0.002477 | Time: 2.99s | LR: 0.00005000\n[Epoch 14/100] Loss: 0.002477 | Time: 3.14s | LR: 0.00005000\n[Epoch 15/100] Loss: 0.002481 | Time: 2.97s | LR: 0.00005000\n[Epoch 16/100] Loss: 0.002487 | Time: 3.18s | LR: 0.00004000\n[Epoch 17/100] Loss: 0.002359 | Time: 3.12s | LR: 0.00004000\n[Epoch 18/100] Loss: 0.002270 | Time: 2.98s | LR: 0.00004000\n[Epoch 19/100] Loss: 0.002257 | Time: 3.01s | LR: 0.00004000\n[Epoch 20/100] Loss: 0.002249 | Time: 3.08s | LR: 0.00004000\n[Epoch 21/100] Loss: 0.002247 | Time: 2.92s | LR: 0.00004000\n[Epoch 22/100] Loss: 0.002257 | Time: 3.08s | LR: 0.00004000\n[Epoch 23/100] Loss: 0.002254 | Time: 3.11s | LR: 0.00004000\n[Epoch 24/100] Loss: 0.002297 | Time: 3.03s | LR: 0.00004000\n[Epoch 25/100] Loss: 0.002255 | Time: 2.95s | LR: 0.00004000\n[Epoch 26/100] Loss: 0.002246 | Time: 2.93s | LR: 0.00004000\n[Epoch 27/100] Loss: 0.002245 | Time: 3.04s | LR: 0.00004000\n[Epoch 28/100] Loss: 0.002250 | Time: 2.92s | LR: 0.00004000\n[Epoch 29/100] Loss: 0.002250 | Time: 2.94s | LR: 0.00004000\n[Epoch 30/100] Loss: 0.002255 | Time: 2.99s | LR: 0.00004000\n[Epoch 31/100] Loss: 0.002294 | Time: 2.96s | LR: 0.00003200\n[Epoch 32/100] Loss: 0.002185 | Time: 2.94s | LR: 0.00003200\n[Epoch 33/100] Loss: 0.002096 | Time: 3.22s | LR: 0.00003200\n[Epoch 34/100] Loss: 0.002062 | Time: 2.89s | LR: 0.00003200\n[Epoch 35/100] Loss: 0.002051 | Time: 2.94s | LR: 0.00003200\n[Epoch 36/100] Loss: 0.002039 | Time: 2.93s | LR: 0.00003200\n[Epoch 37/100] Loss: 0.002039 | Time: 3.07s | LR: 0.00003200\n[Epoch 38/100] Loss: 0.002046 | Time: 2.95s | LR: 0.00003200\n[Epoch 39/100] Loss: 0.002043 | Time: 2.94s | LR: 0.00003200\n[Epoch 40/100] Loss: 0.002063 | Time: 3.03s | LR: 0.00003200\n[Epoch 41/100] Loss: 0.002066 | Time: 2.93s | LR: 0.00003200\n[Epoch 42/100] Loss: 0.002064 | Time: 3.01s | LR: 0.00003200\n[Epoch 43/100] Loss: 0.002066 | Time: 3.03s | LR: 0.00003200\n[Epoch 44/100] Loss: 0.002073 | Time: 3.26s | LR: 0.00003200\n[Epoch 45/100] Loss: 0.002079 | Time: 2.97s | LR: 0.00003200\n[Epoch 46/100] Loss: 0.002082 | Time: 2.91s | LR: 0.00002560\n[Epoch 47/100] Loss: 0.001999 | Time: 3.03s | LR: 0.00002560\n[Epoch 48/100] Loss: 0.001942 | Time: 2.95s | LR: 0.00002560\n[Epoch 49/100] Loss: 0.001950 | Time: 2.92s | LR: 0.00002560\n[Epoch 50/100] Loss: 0.001950 | Time: 3.01s | LR: 0.00002560\n[Epoch 51/100] Loss: 0.001930 | Time: 2.96s | LR: 0.00002560\n[Epoch 52/100] Loss: 0.001945 | Time: 2.94s | LR: 0.00002560\n[Epoch 53/100] Loss: 0.001933 | Time: 3.02s | LR: 0.00002560\n[Epoch 54/100] Loss: 0.001935 | Time: 3.10s | LR: 0.00002560\n[Epoch 55/100] Loss: 0.001927 | Time: 3.01s | LR: 0.00002560\n[Epoch 56/100] Loss: 0.001932 | Time: 2.95s | LR: 0.00002560\n[Epoch 57/100] Loss: 0.001932 | Time: 3.02s | LR: 0.00002560\n[Epoch 58/100] Loss: 0.001953 | Time: 2.95s | LR: 0.00002560\n[Epoch 59/100] Loss: 0.001926 | Time: 3.00s | LR: 0.00002560\n[Epoch 60/100] Loss: 0.001936 | Time: 3.03s | LR: 0.00002560\n[Epoch 61/100] Loss: 0.001947 | Time: 3.05s | LR: 0.00002048\n[Epoch 62/100] Loss: 0.001851 | Time: 2.92s | LR: 0.00002048\n[Epoch 63/100] Loss: 0.001733 | Time: 3.02s | LR: 0.00002048\n[Epoch 64/100] Loss: 0.001684 | Time: 3.00s | LR: 0.00002048\n[Epoch 65/100] Loss: 0.001676 | Time: 3.14s | LR: 0.00002048\n[Epoch 66/100] Loss: 0.001649 | Time: 2.94s | LR: 0.00002048\n[Epoch 67/100] Loss: 0.001647 | Time: 3.05s | LR: 0.00002048\n[Epoch 68/100] Loss: 0.001641 | Time: 2.95s | LR: 0.00002048\n[Epoch 69/100] Loss: 0.001617 | Time: 2.91s | LR: 0.00002048\n[Epoch 70/100] Loss: 0.001587 | Time: 3.05s | LR: 0.00002048\n[Epoch 71/100] Loss: 0.001591 | Time: 2.96s | LR: 0.00002048\n[Epoch 72/100] Loss: 0.001571 | Time: 2.95s | LR: 0.00002048\n[Epoch 73/100] Loss: 0.001556 | Time: 3.00s | LR: 0.00002048\n[Epoch 74/100] Loss: 0.001542 | Time: 2.94s | LR: 0.00002048\n[Epoch 75/100] Loss: 0.001536 | Time: 3.01s | LR: 0.00002048\n[Epoch 76/100] Loss: 0.001502 | Time: 3.11s | LR: 0.00001638\n[Epoch 77/100] Loss: 0.001384 | Time: 3.02s | LR: 0.00001638\n[Epoch 78/100] Loss: 0.001210 | Time: 2.95s | LR: 0.00001638\n[Epoch 79/100] Loss: 0.001087 | Time: 2.93s | LR: 0.00001638\n[Epoch 80/100] Loss: 0.001014 | Time: 2.99s | LR: 0.00001638\n[Epoch 81/100] Loss: 0.000999 | Time: 2.95s | LR: 0.00001638\n[Epoch 82/100] Loss: 0.001000 | Time: 3.00s | LR: 0.00001638\n[Epoch 83/100] Loss: 0.000988 | Time: 2.96s | LR: 0.00001638\n[Epoch 84/100] Loss: 0.000958 | Time: 2.99s | LR: 0.00001638\n[Epoch 85/100] Loss: 0.000914 | Time: 2.97s | LR: 0.00001638\n[Epoch 86/100] Loss: 0.000874 | Time: 3.14s | LR: 0.00001638\n[Epoch 87/100] Loss: 0.000835 | Time: 3.04s | LR: 0.00001638\n[Epoch 88/100] Loss: 0.000809 | Time: 2.93s | LR: 0.00001638\n[Epoch 89/100] Loss: 0.000786 | Time: 2.97s | LR: 0.00001638\n[Epoch 90/100] Loss: 0.000767 | Time: 3.00s | LR: 0.00001638\n[Epoch 91/100] Loss: 0.000755 | Time: 2.96s | LR: 0.00001311\n[Epoch 92/100] Loss: 0.000656 | Time: 2.99s | LR: 0.00001311\n[Epoch 93/100] Loss: 0.000528 | Time: 3.00s | LR: 0.00001311\n[Epoch 94/100] Loss: 0.000477 | Time: 2.96s | LR: 0.00001311\n[Epoch 95/100] Loss: 0.000473 | Time: 2.94s | LR: 0.00001311\n[Epoch 96/100] Loss: 0.000480 | Time: 2.93s | LR: 0.00001311\n[Epoch 97/100] Loss: 0.000480 | Time: 3.19s | LR: 0.00001311\n[Epoch 98/100] Loss: 0.000464 | Time: 2.93s | LR: 0.00001311\n[Epoch 99/100] Loss: 0.000440 | Time: 2.93s | LR: 0.00001311\n[Epoch 100/100] Loss: 0.000421 | Time: 2.99s | LR: 0.00001311\n\nFinished training.\nBest epoch: 100 with loss: 0.00042086188\nGMM_Mean0  PSNR: 49.73309756959813\nGMM_Mean1  PSNR: 49.70641709686624\nGMM_Mean2  PSNR: 49.63938709063001\nGMM_Std0  PSNR: 38.3388772667113\nGMM_Std1  PSNR: 38.56607505670282\nGMM_Std2  PSNR: 38.281532224454786\nGMM_Weight0  PSNR: 39.1351654446771\nGMM_Weight1  PSNR: 39.15589417304506\nGMM_Weight2  PSNR: 39.03488499820439\n\nAverage psnr :  42.39903676898776\nPSNR: ([49.73309756959813, 49.70641709686624, 49.63938709063001, 38.3388772667113, 38.56607505670282, 38.281532224454786, 39.1351654446771, 39.15589417304506, 39.03488499820439], 42.39903676898776)\nRMSE: 0.019262254\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 4 (+2 = total layers in the net)\nbatch_size = 512\nlearning_rate = 1e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.012909 | Time: 2.99s | LR: 0.00001000\n[Epoch 2/100] Loss: 0.002856 | Time: 2.98s | LR: 0.00001000\n[Epoch 3/100] Loss: 0.002733 | Time: 2.97s | LR: 0.00001000\n[Epoch 4/100] Loss: 0.002679 | Time: 2.95s | LR: 0.00001000\n[Epoch 5/100] Loss: 0.002639 | Time: 2.97s | LR: 0.00001000\n[Epoch 6/100] Loss: 0.002608 | Time: 2.99s | LR: 0.00001000\n[Epoch 7/100] Loss: 0.002581 | Time: 3.14s | LR: 0.00001000\n[Epoch 8/100] Loss: 0.002549 | Time: 3.05s | LR: 0.00001000\n[Epoch 9/100] Loss: 0.002521 | Time: 2.94s | LR: 0.00001000\n[Epoch 10/100] Loss: 0.002495 | Time: 3.03s | LR: 0.00001000\n[Epoch 11/100] Loss: 0.002466 | Time: 2.99s | LR: 0.00001000\n[Epoch 12/100] Loss: 0.002439 | Time: 2.95s | LR: 0.00001000\n[Epoch 13/100] Loss: 0.002405 | Time: 3.01s | LR: 0.00001000\n[Epoch 14/100] Loss: 0.002373 | Time: 2.99s | LR: 0.00001000\n[Epoch 15/100] Loss: 0.002340 | Time: 2.94s | LR: 0.00001000\n[Epoch 16/100] Loss: 0.002305 | Time: 2.93s | LR: 0.00000800\n[Epoch 17/100] Loss: 0.002237 | Time: 3.02s | LR: 0.00000800\n[Epoch 18/100] Loss: 0.002203 | Time: 3.16s | LR: 0.00000800\n[Epoch 19/100] Loss: 0.002172 | Time: 2.98s | LR: 0.00000800\n[Epoch 20/100] Loss: 0.002137 | Time: 3.01s | LR: 0.00000800\n[Epoch 21/100] Loss: 0.002103 | Time: 3.02s | LR: 0.00000800\n[Epoch 22/100] Loss: 0.002068 | Time: 2.97s | LR: 0.00000800\n[Epoch 23/100] Loss: 0.002032 | Time: 2.99s | LR: 0.00000800\n[Epoch 24/100] Loss: 0.001997 | Time: 2.97s | LR: 0.00000800\n[Epoch 25/100] Loss: 0.001960 | Time: 2.96s | LR: 0.00000800\n[Epoch 26/100] Loss: 0.001921 | Time: 2.95s | LR: 0.00000800\n[Epoch 27/100] Loss: 0.001886 | Time: 3.05s | LR: 0.00000800\n[Epoch 28/100] Loss: 0.001848 | Time: 2.99s | LR: 0.00000800\n[Epoch 29/100] Loss: 0.001812 | Time: 3.16s | LR: 0.00000800\n[Epoch 30/100] Loss: 0.001772 | Time: 2.99s | LR: 0.00000800\n[Epoch 31/100] Loss: 0.001733 | Time: 2.96s | LR: 0.00000640\n[Epoch 32/100] Loss: 0.001653 | Time: 2.97s | LR: 0.00000640\n[Epoch 33/100] Loss: 0.001611 | Time: 3.01s | LR: 0.00000640\n[Epoch 34/100] Loss: 0.001572 | Time: 2.95s | LR: 0.00000640\n[Epoch 35/100] Loss: 0.001532 | Time: 2.96s | LR: 0.00000640\n[Epoch 36/100] Loss: 0.001488 | Time: 2.98s | LR: 0.00000640\n[Epoch 37/100] Loss: 0.001444 | Time: 2.99s | LR: 0.00000640\n[Epoch 38/100] Loss: 0.001397 | Time: 2.93s | LR: 0.00000640\n[Epoch 39/100] Loss: 0.001349 | Time: 3.11s | LR: 0.00000640\n[Epoch 40/100] Loss: 0.001298 | Time: 3.05s | LR: 0.00000640\n[Epoch 41/100] Loss: 0.001245 | Time: 2.97s | LR: 0.00000640\n[Epoch 42/100] Loss: 0.001191 | Time: 2.99s | LR: 0.00000640\n[Epoch 43/100] Loss: 0.001137 | Time: 3.00s | LR: 0.00000640\n[Epoch 44/100] Loss: 0.001081 | Time: 3.02s | LR: 0.00000640\n[Epoch 45/100] Loss: 0.001026 | Time: 2.99s | LR: 0.00000640\n[Epoch 46/100] Loss: 0.000971 | Time: 3.01s | LR: 0.00000512\n[Epoch 47/100] Loss: 0.000866 | Time: 3.05s | LR: 0.00000512\n[Epoch 48/100] Loss: 0.000812 | Time: 2.98s | LR: 0.00000512\n[Epoch 49/100] Loss: 0.000768 | Time: 3.00s | LR: 0.00000512\n[Epoch 50/100] Loss: 0.000727 | Time: 3.22s | LR: 0.00000512\n[Epoch 51/100] Loss: 0.000686 | Time: 2.94s | LR: 0.00000512\n[Epoch 52/100] Loss: 0.000648 | Time: 2.97s | LR: 0.00000512\n[Epoch 53/100] Loss: 0.000612 | Time: 2.99s | LR: 0.00000512\n[Epoch 54/100] Loss: 0.000579 | Time: 2.99s | LR: 0.00000512\n[Epoch 55/100] Loss: 0.000548 | Time: 2.99s | LR: 0.00000512\n[Epoch 56/100] Loss: 0.000521 | Time: 2.96s | LR: 0.00000512\n[Epoch 57/100] Loss: 0.000496 | Time: 3.00s | LR: 0.00000512\n[Epoch 58/100] Loss: 0.000473 | Time: 2.99s | LR: 0.00000512\n[Epoch 59/100] Loss: 0.000452 | Time: 2.94s | LR: 0.00000512\n[Epoch 60/100] Loss: 0.000433 | Time: 3.08s | LR: 0.00000512\n[Epoch 61/100] Loss: 0.000416 | Time: 3.10s | LR: 0.00000410\n[Epoch 62/100] Loss: 0.000365 | Time: 2.97s | LR: 0.00000410\n[Epoch 63/100] Loss: 0.000342 | Time: 3.00s | LR: 0.00000410\n[Epoch 64/100] Loss: 0.000333 | Time: 2.96s | LR: 0.00000410\n[Epoch 65/100] Loss: 0.000324 | Time: 2.96s | LR: 0.00000410\n[Epoch 66/100] Loss: 0.000315 | Time: 2.98s | LR: 0.00000410\n[Epoch 67/100] Loss: 0.000307 | Time: 3.00s | LR: 0.00000410\n[Epoch 68/100] Loss: 0.000298 | Time: 2.93s | LR: 0.00000410\n[Epoch 69/100] Loss: 0.000290 | Time: 2.94s | LR: 0.00000410\n[Epoch 70/100] Loss: 0.000282 | Time: 2.97s | LR: 0.00000410\n[Epoch 71/100] Loss: 0.000274 | Time: 3.15s | LR: 0.00000410\n[Epoch 72/100] Loss: 0.000267 | Time: 2.98s | LR: 0.00000410\n[Epoch 73/100] Loss: 0.000260 | Time: 2.98s | LR: 0.00000410\n[Epoch 74/100] Loss: 0.000254 | Time: 2.97s | LR: 0.00000410\n[Epoch 75/100] Loss: 0.000248 | Time: 2.95s | LR: 0.00000410\n[Epoch 76/100] Loss: 0.000242 | Time: 2.93s | LR: 0.00000328\n[Epoch 77/100] Loss: 0.000212 | Time: 2.99s | LR: 0.00000328\n[Epoch 78/100] Loss: 0.000196 | Time: 2.98s | LR: 0.00000328\n[Epoch 79/100] Loss: 0.000193 | Time: 2.94s | LR: 0.00000328\n[Epoch 80/100] Loss: 0.000191 | Time: 2.97s | LR: 0.00000328\n[Epoch 81/100] Loss: 0.000189 | Time: 2.95s | LR: 0.00000328\n[Epoch 82/100] Loss: 0.000185 | Time: 3.16s | LR: 0.00000328\n[Epoch 83/100] Loss: 0.000182 | Time: 2.97s | LR: 0.00000328\n[Epoch 84/100] Loss: 0.000177 | Time: 2.96s | LR: 0.00000328\n[Epoch 85/100] Loss: 0.000174 | Time: 2.95s | LR: 0.00000328\n[Epoch 86/100] Loss: 0.000170 | Time: 2.95s | LR: 0.00000328\n[Epoch 87/100] Loss: 0.000167 | Time: 2.98s | LR: 0.00000328\n[Epoch 88/100] Loss: 0.000163 | Time: 2.96s | LR: 0.00000328\n[Epoch 89/100] Loss: 0.000160 | Time: 2.91s | LR: 0.00000328\n[Epoch 90/100] Loss: 0.000157 | Time: 3.01s | LR: 0.00000328\n[Epoch 91/100] Loss: 0.000154 | Time: 2.95s | LR: 0.00000262\n[Epoch 92/100] Loss: 0.000134 | Time: 3.03s | LR: 0.00000262\n[Epoch 93/100] Loss: 0.000123 | Time: 3.15s | LR: 0.00000262\n[Epoch 94/100] Loss: 0.000122 | Time: 2.99s | LR: 0.00000262\n[Epoch 95/100] Loss: 0.000122 | Time: 2.93s | LR: 0.00000262\n[Epoch 96/100] Loss: 0.000121 | Time: 2.94s | LR: 0.00000262\n[Epoch 97/100] Loss: 0.000119 | Time: 3.01s | LR: 0.00000262\n[Epoch 98/100] Loss: 0.000117 | Time: 2.95s | LR: 0.00000262\n[Epoch 99/100] Loss: 0.000115 | Time: 3.02s | LR: 0.00000262\n[Epoch 100/100] Loss: 0.000113 | Time: 3.01s | LR: 0.00000262\n\nFinished training.\nBest epoch: 100 with loss: 0.00011264234\nGMM_Mean0  PSNR: 51.07125322287372\nGMM_Mean1  PSNR: 51.050511654888965\nGMM_Mean2  PSNR: 51.05515604155439\nGMM_Std0  PSNR: 44.77076839652206\nGMM_Std1  PSNR: 43.65946033458643\nGMM_Std2  PSNR: 44.08060953820957\nGMM_Weight0  PSNR: 45.46437450874674\nGMM_Weight1  PSNR: 45.400716307011024\nGMM_Weight2  PSNR: 45.419449720167826\n\nAverage psnr :  46.88581108050674\nPSNR: ([51.07125322287372, 51.050511654888965, 51.05515604155439, 44.77076839652206, 43.65946033458643, 44.08060953820957, 45.46437450874674, 45.400716307011024, 45.419449720167826], 46.88581108050674)\nRMSE: 0.009998774\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 4 (+2 = total layers in the net)\nbatch_size = 512\nlearning_rate = 5e-06\n====================================================\n\n[Epoch 1/100] Loss: 0.020206 | Time: 2.97s | LR: 0.00000500\n[Epoch 2/100] Loss: 0.003082 | Time: 2.95s | LR: 0.00000500\n[Epoch 3/100] Loss: 0.002844 | Time: 3.19s | LR: 0.00000500\n[Epoch 4/100] Loss: 0.002746 | Time: 2.96s | LR: 0.00000500\n[Epoch 5/100] Loss: 0.002693 | Time: 2.99s | LR: 0.00000500\n[Epoch 6/100] Loss: 0.002654 | Time: 2.96s | LR: 0.00000500\n[Epoch 7/100] Loss: 0.002625 | Time: 3.02s | LR: 0.00000500\n[Epoch 8/100] Loss: 0.002601 | Time: 2.98s | LR: 0.00000500\n[Epoch 9/100] Loss: 0.002577 | Time: 2.96s | LR: 0.00000500\n[Epoch 10/100] Loss: 0.002558 | Time: 3.02s | LR: 0.00000500\n[Epoch 11/100] Loss: 0.002538 | Time: 2.96s | LR: 0.00000500\n[Epoch 12/100] Loss: 0.002517 | Time: 2.96s | LR: 0.00000500\n[Epoch 13/100] Loss: 0.002500 | Time: 3.01s | LR: 0.00000500\n[Epoch 14/100] Loss: 0.002480 | Time: 3.19s | LR: 0.00000500\n[Epoch 15/100] Loss: 0.002463 | Time: 2.97s | LR: 0.00000500\n[Epoch 16/100] Loss: 0.002441 | Time: 2.96s | LR: 0.00000400\n[Epoch 17/100] Loss: 0.002409 | Time: 3.01s | LR: 0.00000400\n[Epoch 18/100] Loss: 0.002391 | Time: 2.96s | LR: 0.00000400\n[Epoch 19/100] Loss: 0.002376 | Time: 2.93s | LR: 0.00000400\n[Epoch 20/100] Loss: 0.002359 | Time: 2.99s | LR: 0.00000400\n[Epoch 21/100] Loss: 0.002343 | Time: 3.02s | LR: 0.00000400\n[Epoch 22/100] Loss: 0.002327 | Time: 2.97s | LR: 0.00000400\n[Epoch 23/100] Loss: 0.002310 | Time: 2.97s | LR: 0.00000400\n[Epoch 24/100] Loss: 0.002293 | Time: 3.10s | LR: 0.00000400\n[Epoch 25/100] Loss: 0.002277 | Time: 3.00s | LR: 0.00000400\n[Epoch 26/100] Loss: 0.002262 | Time: 2.95s | LR: 0.00000400\n[Epoch 27/100] Loss: 0.002244 | Time: 2.97s | LR: 0.00000400\n[Epoch 28/100] Loss: 0.002227 | Time: 3.00s | LR: 0.00000400\n[Epoch 29/100] Loss: 0.002210 | Time: 2.93s | LR: 0.00000400\n[Epoch 30/100] Loss: 0.002191 | Time: 3.01s | LR: 0.00000400\n[Epoch 31/100] Loss: 0.002175 | Time: 2.95s | LR: 0.00000320\n[Epoch 32/100] Loss: 0.002140 | Time: 2.97s | LR: 0.00000320\n[Epoch 33/100] Loss: 0.002124 | Time: 3.02s | LR: 0.00000320\n[Epoch 34/100] Loss: 0.002109 | Time: 2.97s | LR: 0.00000320\n[Epoch 35/100] Loss: 0.002094 | Time: 3.20s | LR: 0.00000320\n[Epoch 36/100] Loss: 0.002078 | Time: 2.96s | LR: 0.00000320\n[Epoch 37/100] Loss: 0.002063 | Time: 3.01s | LR: 0.00000320\n[Epoch 38/100] Loss: 0.002047 | Time: 3.05s | LR: 0.00000320\n[Epoch 39/100] Loss: 0.002031 | Time: 3.27s | LR: 0.00000320\n[Epoch 40/100] Loss: 0.002016 | Time: 3.13s | LR: 0.00000320\n[Epoch 41/100] Loss: 0.002000 | Time: 3.12s | LR: 0.00000320\n[Epoch 42/100] Loss: 0.001984 | Time: 3.06s | LR: 0.00000320\n[Epoch 43/100] Loss: 0.001968 | Time: 3.06s | LR: 0.00000320\n[Epoch 44/100] Loss: 0.001952 | Time: 2.99s | LR: 0.00000320\n[Epoch 45/100] Loss: 0.001936 | Time: 3.16s | LR: 0.00000320\n[Epoch 46/100] Loss: 0.001920 | Time: 3.14s | LR: 0.00000256\n[Epoch 47/100] Loss: 0.001888 | Time: 3.01s | LR: 0.00000256\n[Epoch 48/100] Loss: 0.001873 | Time: 3.00s | LR: 0.00000256\n[Epoch 49/100] Loss: 0.001860 | Time: 3.03s | LR: 0.00000256\n[Epoch 50/100] Loss: 0.001846 | Time: 3.07s | LR: 0.00000256\n[Epoch 51/100] Loss: 0.001832 | Time: 3.04s | LR: 0.00000256\n[Epoch 52/100] Loss: 0.001818 | Time: 3.01s | LR: 0.00000256\n[Epoch 53/100] Loss: 0.001804 | Time: 3.01s | LR: 0.00000256\n[Epoch 54/100] Loss: 0.001790 | Time: 3.01s | LR: 0.00000256\n[Epoch 55/100] Loss: 0.001776 | Time: 2.97s | LR: 0.00000256\n[Epoch 56/100] Loss: 0.001762 | Time: 3.27s | LR: 0.00000256\n[Epoch 57/100] Loss: 0.001748 | Time: 3.00s | LR: 0.00000256\n[Epoch 58/100] Loss: 0.001733 | Time: 3.00s | LR: 0.00000256\n[Epoch 59/100] Loss: 0.001719 | Time: 3.02s | LR: 0.00000256\n[Epoch 60/100] Loss: 0.001704 | Time: 3.00s | LR: 0.00000256\n[Epoch 61/100] Loss: 0.001689 | Time: 2.97s | LR: 0.00000205\n[Epoch 62/100] Loss: 0.001660 | Time: 2.98s | LR: 0.00000205\n[Epoch 63/100] Loss: 0.001646 | Time: 3.01s | LR: 0.00000205\n[Epoch 64/100] Loss: 0.001634 | Time: 3.01s | LR: 0.00000205\n[Epoch 65/100] Loss: 0.001621 | Time: 2.97s | LR: 0.00000205\n[Epoch 66/100] Loss: 0.001608 | Time: 3.13s | LR: 0.00000205\n[Epoch 67/100] Loss: 0.001595 | Time: 3.10s | LR: 0.00000205\n[Epoch 68/100] Loss: 0.001582 | Time: 2.98s | LR: 0.00000205\n[Epoch 69/100] Loss: 0.001570 | Time: 3.01s | LR: 0.00000205\n[Epoch 70/100] Loss: 0.001556 | Time: 2.98s | LR: 0.00000205\n[Epoch 71/100] Loss: 0.001542 | Time: 2.98s | LR: 0.00000205\n[Epoch 72/100] Loss: 0.001529 | Time: 3.00s | LR: 0.00000205\n[Epoch 73/100] Loss: 0.001515 | Time: 3.02s | LR: 0.00000205\n[Epoch 74/100] Loss: 0.001501 | Time: 3.01s | LR: 0.00000205\n[Epoch 75/100] Loss: 0.001487 | Time: 3.01s | LR: 0.00000205\n[Epoch 76/100] Loss: 0.001473 | Time: 2.99s | LR: 0.00000164\n[Epoch 77/100] Loss: 0.001444 | Time: 3.16s | LR: 0.00000164\n[Epoch 78/100] Loss: 0.001431 | Time: 3.01s | LR: 0.00000164\n[Epoch 79/100] Loss: 0.001419 | Time: 3.05s | LR: 0.00000164\n[Epoch 80/100] Loss: 0.001406 | Time: 3.01s | LR: 0.00000164\n[Epoch 81/100] Loss: 0.001394 | Time: 2.98s | LR: 0.00000164\n[Epoch 82/100] Loss: 0.001381 | Time: 2.98s | LR: 0.00000164\n[Epoch 83/100] Loss: 0.001368 | Time: 3.06s | LR: 0.00000164\n[Epoch 84/100] Loss: 0.001355 | Time: 2.96s | LR: 0.00000164\n[Epoch 85/100] Loss: 0.001342 | Time: 2.97s | LR: 0.00000164\n[Epoch 86/100] Loss: 0.001329 | Time: 3.01s | LR: 0.00000164\n[Epoch 87/100] Loss: 0.001315 | Time: 3.06s | LR: 0.00000164\n[Epoch 88/100] Loss: 0.001302 | Time: 3.11s | LR: 0.00000164\n[Epoch 89/100] Loss: 0.001288 | Time: 2.99s | LR: 0.00000164\n[Epoch 90/100] Loss: 0.001275 | Time: 2.95s | LR: 0.00000164\n[Epoch 91/100] Loss: 0.001261 | Time: 2.95s | LR: 0.00000131\n[Epoch 92/100] Loss: 0.001233 | Time: 2.96s | LR: 0.00000131\n[Epoch 93/100] Loss: 0.001221 | Time: 3.01s | LR: 0.00000131\n[Epoch 94/100] Loss: 0.001209 | Time: 2.97s | LR: 0.00000131\n[Epoch 95/100] Loss: 0.001197 | Time: 2.99s | LR: 0.00000131\n[Epoch 96/100] Loss: 0.001185 | Time: 2.98s | LR: 0.00000131\n[Epoch 97/100] Loss: 0.001173 | Time: 2.97s | LR: 0.00000131\n[Epoch 98/100] Loss: 0.001161 | Time: 3.15s | LR: 0.00000131\n[Epoch 99/100] Loss: 0.001149 | Time: 3.04s | LR: 0.00000131\n[Epoch 100/100] Loss: 0.001137 | Time: 2.97s | LR: 0.00000131\n\nFinished training.\nBest epoch: 100 with loss: 0.0011367202\nGMM_Mean0  PSNR: 49.42320711185861\nGMM_Mean1  PSNR: 49.48909198895444\nGMM_Mean2  PSNR: 49.445633662349245\nGMM_Std0  PSNR: 35.40430534195009\nGMM_Std1  PSNR: 36.31233583199653\nGMM_Std2  PSNR: 35.97536127175531\nGMM_Weight0  PSNR: 32.666155978390115\nGMM_Weight1  PSNR: 32.88309396668188\nGMM_Weight2  PSNR: 32.57202724040289\n\nAverage psnr :  39.352356932704346\nPSNR: ([49.42320711185861, 49.48909198895444, 49.445633662349245, 35.40430534195009, 36.31233583199653, 35.97536127175531, 32.666155978390115, 32.88309396668188, 32.57202724040289], 39.352356932704346)\nRMSE: 0.032780144\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 4 (+2 = total layers in the net)\nbatch_size = 1024\nlearning_rate = 5e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.008252 | Time: 2.18s | LR: 0.00005000\n[Epoch 2/100] Loss: 0.002764 | Time: 2.18s | LR: 0.00005000\n[Epoch 3/100] Loss: 0.002680 | Time: 2.21s | LR: 0.00005000\n[Epoch 4/100] Loss: 0.002625 | Time: 2.16s | LR: 0.00005000\n[Epoch 5/100] Loss: 0.002572 | Time: 2.17s | LR: 0.00005000\n[Epoch 6/100] Loss: 0.002524 | Time: 2.17s | LR: 0.00005000\n[Epoch 7/100] Loss: 0.002483 | Time: 2.16s | LR: 0.00005000\n[Epoch 8/100] Loss: 0.002438 | Time: 2.21s | LR: 0.00005000\n[Epoch 9/100] Loss: 0.002398 | Time: 2.18s | LR: 0.00005000\n[Epoch 10/100] Loss: 0.002368 | Time: 2.16s | LR: 0.00005000\n[Epoch 11/100] Loss: 0.002336 | Time: 2.23s | LR: 0.00005000\n[Epoch 12/100] Loss: 0.002307 | Time: 2.31s | LR: 0.00005000\n[Epoch 13/100] Loss: 0.002275 | Time: 2.16s | LR: 0.00005000\n[Epoch 14/100] Loss: 0.002272 | Time: 2.16s | LR: 0.00005000\n[Epoch 15/100] Loss: 0.002249 | Time: 2.16s | LR: 0.00005000\n[Epoch 16/100] Loss: 0.002227 | Time: 2.15s | LR: 0.00004000\n[Epoch 17/100] Loss: 0.002106 | Time: 2.20s | LR: 0.00004000\n[Epoch 18/100] Loss: 0.002047 | Time: 2.16s | LR: 0.00004000\n[Epoch 19/100] Loss: 0.002034 | Time: 2.16s | LR: 0.00004000\n[Epoch 20/100] Loss: 0.002027 | Time: 2.16s | LR: 0.00004000\n[Epoch 21/100] Loss: 0.002018 | Time: 2.21s | LR: 0.00004000\n[Epoch 22/100] Loss: 0.002007 | Time: 2.19s | LR: 0.00004000\n[Epoch 23/100] Loss: 0.001999 | Time: 2.16s | LR: 0.00004000\n[Epoch 24/100] Loss: 0.001982 | Time: 2.16s | LR: 0.00004000\n[Epoch 25/100] Loss: 0.001967 | Time: 2.16s | LR: 0.00004000\n[Epoch 26/100] Loss: 0.001950 | Time: 2.45s | LR: 0.00004000\n[Epoch 27/100] Loss: 0.001936 | Time: 2.21s | LR: 0.00004000\n[Epoch 28/100] Loss: 0.001923 | Time: 2.15s | LR: 0.00004000\n[Epoch 29/100] Loss: 0.001911 | Time: 2.17s | LR: 0.00004000\n[Epoch 30/100] Loss: 0.001890 | Time: 2.15s | LR: 0.00004000\n[Epoch 31/100] Loss: 0.001884 | Time: 2.19s | LR: 0.00003200\n[Epoch 32/100] Loss: 0.001725 | Time: 2.17s | LR: 0.00003200\n[Epoch 33/100] Loss: 0.001618 | Time: 2.17s | LR: 0.00003200\n[Epoch 34/100] Loss: 0.001592 | Time: 2.17s | LR: 0.00003200\n[Epoch 35/100] Loss: 0.001575 | Time: 2.19s | LR: 0.00003200\n[Epoch 36/100] Loss: 0.001551 | Time: 2.17s | LR: 0.00003200\n[Epoch 37/100] Loss: 0.001533 | Time: 2.19s | LR: 0.00003200\n[Epoch 38/100] Loss: 0.001532 | Time: 2.16s | LR: 0.00003200\n[Epoch 39/100] Loss: 0.001541 | Time: 2.14s | LR: 0.00003200\n[Epoch 40/100] Loss: 0.001548 | Time: 2.25s | LR: 0.00003200\n[Epoch 41/100] Loss: 0.001530 | Time: 2.45s | LR: 0.00003200\n[Epoch 42/100] Loss: 0.001523 | Time: 2.15s | LR: 0.00003200\n[Epoch 43/100] Loss: 0.001529 | Time: 2.16s | LR: 0.00003200\n[Epoch 44/100] Loss: 0.001510 | Time: 2.21s | LR: 0.00003200\n[Epoch 45/100] Loss: 0.001498 | Time: 2.17s | LR: 0.00003200\n[Epoch 46/100] Loss: 0.001486 | Time: 2.16s | LR: 0.00002560\n[Epoch 47/100] Loss: 0.001293 | Time: 2.17s | LR: 0.00002560\n[Epoch 48/100] Loss: 0.001044 | Time: 2.18s | LR: 0.00002560\n[Epoch 49/100] Loss: 0.000901 | Time: 2.17s | LR: 0.00002560\n[Epoch 50/100] Loss: 0.000818 | Time: 2.17s | LR: 0.00002560\n[Epoch 51/100] Loss: 0.000779 | Time: 2.18s | LR: 0.00002560\n[Epoch 52/100] Loss: 0.000766 | Time: 2.17s | LR: 0.00002560\n[Epoch 53/100] Loss: 0.000779 | Time: 2.16s | LR: 0.00002560\n[Epoch 54/100] Loss: 0.000817 | Time: 2.20s | LR: 0.00002560\n[Epoch 55/100] Loss: 0.000848 | Time: 2.29s | LR: 0.00002560\n[Epoch 56/100] Loss: 0.000857 | Time: 2.30s | LR: 0.00002560\n[Epoch 57/100] Loss: 0.000816 | Time: 2.18s | LR: 0.00002560\n[Epoch 58/100] Loss: 0.000771 | Time: 2.18s | LR: 0.00002560\n[Epoch 59/100] Loss: 0.000725 | Time: 2.18s | LR: 0.00002560\n[Epoch 60/100] Loss: 0.000681 | Time: 2.18s | LR: 0.00002560\n[Epoch 61/100] Loss: 0.000671 | Time: 2.16s | LR: 0.00002048\n[Epoch 62/100] Loss: 0.000572 | Time: 2.14s | LR: 0.00002048\n[Epoch 63/100] Loss: 0.000441 | Time: 2.21s | LR: 0.00002048\n[Epoch 64/100] Loss: 0.000387 | Time: 2.16s | LR: 0.00002048\n[Epoch 65/100] Loss: 0.000374 | Time: 2.17s | LR: 0.00002048\n[Epoch 66/100] Loss: 0.000390 | Time: 2.16s | LR: 0.00002048\n[Epoch 67/100] Loss: 0.000421 | Time: 2.19s | LR: 0.00002048\n[Epoch 68/100] Loss: 0.000456 | Time: 2.18s | LR: 0.00002048\n[Epoch 69/100] Loss: 0.000478 | Time: 2.18s | LR: 0.00002048\n[Epoch 70/100] Loss: 0.000468 | Time: 2.41s | LR: 0.00002048\n[Epoch 71/100] Loss: 0.000450 | Time: 2.24s | LR: 0.00002048\n[Epoch 72/100] Loss: 0.000429 | Time: 2.33s | LR: 0.00002048\n[Epoch 73/100] Loss: 0.000412 | Time: 2.17s | LR: 0.00002048\n[Epoch 74/100] Loss: 0.000401 | Time: 2.17s | LR: 0.00002048\n[Epoch 75/100] Loss: 0.000403 | Time: 2.16s | LR: 0.00002048\n[Epoch 76/100] Loss: 0.000399 | Time: 2.20s | LR: 0.00001638\n[Epoch 77/100] Loss: 0.000329 | Time: 2.17s | LR: 0.00001638\n[Epoch 78/100] Loss: 0.000243 | Time: 2.14s | LR: 0.00001638\n[Epoch 79/100] Loss: 0.000212 | Time: 2.16s | LR: 0.00001638\n[Epoch 80/100] Loss: 0.000212 | Time: 2.20s | LR: 0.00001638\n[Epoch 81/100] Loss: 0.000227 | Time: 2.19s | LR: 0.00001638\n[Epoch 82/100] Loss: 0.000256 | Time: 2.16s | LR: 0.00001638\n[Epoch 83/100] Loss: 0.000280 | Time: 2.16s | LR: 0.00001638\n[Epoch 84/100] Loss: 0.000290 | Time: 2.16s | LR: 0.00001638\n[Epoch 85/100] Loss: 0.000283 | Time: 2.44s | LR: 0.00001638\n[Epoch 86/100] Loss: 0.000269 | Time: 2.20s | LR: 0.00001638\n[Epoch 87/100] Loss: 0.000256 | Time: 2.16s | LR: 0.00001638\n[Epoch 88/100] Loss: 0.000248 | Time: 2.17s | LR: 0.00001638\n[Epoch 89/100] Loss: 0.000248 | Time: 2.25s | LR: 0.00001638\n[Epoch 90/100] Loss: 0.000250 | Time: 2.18s | LR: 0.00001638\n[Epoch 91/100] Loss: 0.000253 | Time: 2.15s | LR: 0.00001311\n[Epoch 92/100] Loss: 0.000207 | Time: 2.16s | LR: 0.00001311\n[Epoch 93/100] Loss: 0.000146 | Time: 2.15s | LR: 0.00001311\n[Epoch 94/100] Loss: 0.000126 | Time: 2.16s | LR: 0.00001311\n[Epoch 95/100] Loss: 0.000126 | Time: 2.18s | LR: 0.00001311\n[Epoch 96/100] Loss: 0.000140 | Time: 2.17s | LR: 0.00001311\n[Epoch 97/100] Loss: 0.000160 | Time: 2.15s | LR: 0.00001311\n[Epoch 98/100] Loss: 0.000177 | Time: 2.15s | LR: 0.00001311\n[Epoch 99/100] Loss: 0.000183 | Time: 2.35s | LR: 0.00001311\n[Epoch 100/100] Loss: 0.000180 | Time: 2.26s | LR: 0.00001311\n\nFinished training.\nBest epoch: 94 with loss: 0.00012582724\nGMM_Mean0  PSNR: 50.54641889430347\nGMM_Mean1  PSNR: 50.58352540283786\nGMM_Mean2  PSNR: 50.57598614689431\nGMM_Std0  PSNR: 42.38962769304413\nGMM_Std1  PSNR: 42.68345496821506\nGMM_Std2  PSNR: 42.74652666390888\nGMM_Weight0  PSNR: 42.27651421306249\nGMM_Weight1  PSNR: 42.314953981428076\nGMM_Weight2  PSNR: 42.23255721737249\n\nAverage psnr :  45.149951686785194\nPSNR: ([50.54641889430347, 50.58352540283786, 50.57598614689431, 42.38962769304413, 42.68345496821506, 42.74652666390888, 42.27651421306249, 42.314953981428076, 42.23255721737249], 45.149951686785194)\nRMSE: 0.012801574\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 4 (+2 = total layers in the net)\nbatch_size = 1024\nlearning_rate = 1e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.020977 | Time: 2.19s | LR: 0.00001000\n[Epoch 2/100] Loss: 0.003060 | Time: 2.16s | LR: 0.00001000\n[Epoch 3/100] Loss: 0.002839 | Time: 2.20s | LR: 0.00001000\n[Epoch 4/100] Loss: 0.002748 | Time: 2.15s | LR: 0.00001000\n[Epoch 5/100] Loss: 0.002694 | Time: 2.26s | LR: 0.00001000\n[Epoch 6/100] Loss: 0.002656 | Time: 2.18s | LR: 0.00001000\n[Epoch 7/100] Loss: 0.002627 | Time: 2.16s | LR: 0.00001000\n[Epoch 8/100] Loss: 0.002601 | Time: 2.21s | LR: 0.00001000\n[Epoch 9/100] Loss: 0.002579 | Time: 2.17s | LR: 0.00001000\n[Epoch 10/100] Loss: 0.002559 | Time: 2.26s | LR: 0.00001000\n[Epoch 11/100] Loss: 0.002536 | Time: 2.18s | LR: 0.00001000\n[Epoch 12/100] Loss: 0.002516 | Time: 2.16s | LR: 0.00001000\n[Epoch 13/100] Loss: 0.002498 | Time: 2.18s | LR: 0.00001000\n[Epoch 14/100] Loss: 0.002476 | Time: 2.42s | LR: 0.00001000\n[Epoch 15/100] Loss: 0.002455 | Time: 2.19s | LR: 0.00001000\n[Epoch 16/100] Loss: 0.002437 | Time: 2.19s | LR: 0.00000800\n[Epoch 17/100] Loss: 0.002397 | Time: 2.18s | LR: 0.00000800\n[Epoch 18/100] Loss: 0.002378 | Time: 2.17s | LR: 0.00000800\n[Epoch 19/100] Loss: 0.002361 | Time: 2.19s | LR: 0.00000800\n[Epoch 20/100] Loss: 0.002347 | Time: 2.16s | LR: 0.00000800\n[Epoch 21/100] Loss: 0.002325 | Time: 2.17s | LR: 0.00000800\n[Epoch 22/100] Loss: 0.002308 | Time: 2.17s | LR: 0.00000800\n[Epoch 23/100] Loss: 0.002291 | Time: 2.17s | LR: 0.00000800\n[Epoch 24/100] Loss: 0.002270 | Time: 2.17s | LR: 0.00000800\n[Epoch 25/100] Loss: 0.002250 | Time: 2.16s | LR: 0.00000800\n[Epoch 26/100] Loss: 0.002232 | Time: 2.19s | LR: 0.00000800\n[Epoch 27/100] Loss: 0.002207 | Time: 2.17s | LR: 0.00000800\n[Epoch 28/100] Loss: 0.002187 | Time: 2.33s | LR: 0.00000800\n[Epoch 29/100] Loss: 0.002167 | Time: 2.25s | LR: 0.00000800\n[Epoch 30/100] Loss: 0.002144 | Time: 2.16s | LR: 0.00000800\n[Epoch 31/100] Loss: 0.002123 | Time: 2.30s | LR: 0.00000640\n[Epoch 32/100] Loss: 0.002079 | Time: 2.17s | LR: 0.00000640\n[Epoch 33/100] Loss: 0.002057 | Time: 2.16s | LR: 0.00000640\n[Epoch 34/100] Loss: 0.002037 | Time: 2.17s | LR: 0.00000640\n[Epoch 35/100] Loss: 0.002018 | Time: 2.23s | LR: 0.00000640\n[Epoch 36/100] Loss: 0.001999 | Time: 2.16s | LR: 0.00000640\n[Epoch 37/100] Loss: 0.001979 | Time: 2.14s | LR: 0.00000640\n[Epoch 38/100] Loss: 0.001959 | Time: 2.20s | LR: 0.00000640\n[Epoch 39/100] Loss: 0.001938 | Time: 2.19s | LR: 0.00000640\n[Epoch 40/100] Loss: 0.001917 | Time: 2.19s | LR: 0.00000640\n[Epoch 41/100] Loss: 0.001898 | Time: 2.16s | LR: 0.00000640\n[Epoch 42/100] Loss: 0.001876 | Time: 2.18s | LR: 0.00000640\n[Epoch 43/100] Loss: 0.001855 | Time: 2.47s | LR: 0.00000640\n[Epoch 44/100] Loss: 0.001834 | Time: 2.22s | LR: 0.00000640\n[Epoch 45/100] Loss: 0.001813 | Time: 2.27s | LR: 0.00000640\n[Epoch 46/100] Loss: 0.001792 | Time: 2.18s | LR: 0.00000512\n[Epoch 47/100] Loss: 0.001747 | Time: 2.15s | LR: 0.00000512\n[Epoch 48/100] Loss: 0.001726 | Time: 2.18s | LR: 0.00000512\n[Epoch 49/100] Loss: 0.001707 | Time: 2.20s | LR: 0.00000512\n[Epoch 50/100] Loss: 0.001688 | Time: 2.18s | LR: 0.00000512\n[Epoch 51/100] Loss: 0.001669 | Time: 2.18s | LR: 0.00000512\n[Epoch 52/100] Loss: 0.001647 | Time: 2.16s | LR: 0.00000512\n[Epoch 53/100] Loss: 0.001627 | Time: 2.21s | LR: 0.00000512\n[Epoch 54/100] Loss: 0.001606 | Time: 2.18s | LR: 0.00000512\n[Epoch 55/100] Loss: 0.001585 | Time: 2.17s | LR: 0.00000512\n[Epoch 56/100] Loss: 0.001564 | Time: 2.16s | LR: 0.00000512\n[Epoch 57/100] Loss: 0.001540 | Time: 2.23s | LR: 0.00000512\n[Epoch 58/100] Loss: 0.001518 | Time: 2.32s | LR: 0.00000512\n[Epoch 59/100] Loss: 0.001494 | Time: 2.16s | LR: 0.00000512\n[Epoch 60/100] Loss: 0.001470 | Time: 2.17s | LR: 0.00000512\n[Epoch 61/100] Loss: 0.001445 | Time: 2.18s | LR: 0.00000410\n[Epoch 62/100] Loss: 0.001396 | Time: 2.19s | LR: 0.00000410\n[Epoch 63/100] Loss: 0.001371 | Time: 2.19s | LR: 0.00000410\n[Epoch 64/100] Loss: 0.001349 | Time: 2.17s | LR: 0.00000410\n[Epoch 65/100] Loss: 0.001326 | Time: 2.18s | LR: 0.00000410\n[Epoch 66/100] Loss: 0.001302 | Time: 2.20s | LR: 0.00000410\n[Epoch 67/100] Loss: 0.001278 | Time: 2.18s | LR: 0.00000410\n[Epoch 68/100] Loss: 0.001253 | Time: 2.16s | LR: 0.00000410\n[Epoch 69/100] Loss: 0.001228 | Time: 2.16s | LR: 0.00000410\n[Epoch 70/100] Loss: 0.001203 | Time: 2.14s | LR: 0.00000410\n[Epoch 71/100] Loss: 0.001177 | Time: 2.16s | LR: 0.00000410\n[Epoch 72/100] Loss: 0.001151 | Time: 2.50s | LR: 0.00000410\n[Epoch 73/100] Loss: 0.001124 | Time: 2.25s | LR: 0.00000410\n[Epoch 74/100] Loss: 0.001097 | Time: 2.14s | LR: 0.00000410\n[Epoch 75/100] Loss: 0.001070 | Time: 2.18s | LR: 0.00000410\n[Epoch 76/100] Loss: 0.001042 | Time: 2.22s | LR: 0.00000328\n[Epoch 77/100] Loss: 0.000991 | Time: 2.14s | LR: 0.00000328\n[Epoch 78/100] Loss: 0.000966 | Time: 2.15s | LR: 0.00000328\n[Epoch 79/100] Loss: 0.000943 | Time: 2.17s | LR: 0.00000328\n[Epoch 80/100] Loss: 0.000920 | Time: 2.17s | LR: 0.00000328\n[Epoch 81/100] Loss: 0.000897 | Time: 2.20s | LR: 0.00000328\n[Epoch 82/100] Loss: 0.000874 | Time: 2.17s | LR: 0.00000328\n[Epoch 83/100] Loss: 0.000851 | Time: 2.15s | LR: 0.00000328\n[Epoch 84/100] Loss: 0.000829 | Time: 2.24s | LR: 0.00000328\n[Epoch 85/100] Loss: 0.000806 | Time: 2.22s | LR: 0.00000328\n[Epoch 86/100] Loss: 0.000784 | Time: 2.16s | LR: 0.00000328\n[Epoch 87/100] Loss: 0.000762 | Time: 2.46s | LR: 0.00000328\n[Epoch 88/100] Loss: 0.000741 | Time: 2.16s | LR: 0.00000328\n[Epoch 89/100] Loss: 0.000719 | Time: 2.20s | LR: 0.00000328\n[Epoch 90/100] Loss: 0.000699 | Time: 2.21s | LR: 0.00000328\n[Epoch 91/100] Loss: 0.000678 | Time: 2.18s | LR: 0.00000262\n[Epoch 92/100] Loss: 0.000639 | Time: 2.18s | LR: 0.00000262\n[Epoch 93/100] Loss: 0.000621 | Time: 2.15s | LR: 0.00000262\n[Epoch 94/100] Loss: 0.000606 | Time: 2.28s | LR: 0.00000262\n[Epoch 95/100] Loss: 0.000591 | Time: 2.18s | LR: 0.00000262\n[Epoch 96/100] Loss: 0.000576 | Time: 2.18s | LR: 0.00000262\n[Epoch 97/100] Loss: 0.000562 | Time: 2.18s | LR: 0.00000262\n[Epoch 98/100] Loss: 0.000547 | Time: 2.23s | LR: 0.00000262\n[Epoch 99/100] Loss: 0.000534 | Time: 2.19s | LR: 0.00000262\n[Epoch 100/100] Loss: 0.000520 | Time: 2.16s | LR: 0.00000262\n\nFinished training.\nBest epoch: 100 with loss: 0.0005199454\nGMM_Mean0  PSNR: 47.98430577649077\nGMM_Mean1  PSNR: 47.94462793950193\nGMM_Mean2  PSNR: 48.016754236235364\nGMM_Std0  PSNR: 37.19037018598137\nGMM_Std1  PSNR: 37.927414813524166\nGMM_Std2  PSNR: 37.34795868699847\nGMM_Weight0  PSNR: 38.303986331674004\nGMM_Weight1  PSNR: 38.36802157947203\nGMM_Weight2  PSNR: 38.08482404205709\n\nAverage psnr :  41.2409181768817\nPSNR: ([47.98430577649077, 47.94462793950193, 48.016754236235364, 37.19037018598137, 37.927414813524166, 37.34795868699847, 38.303986331674004, 38.36802157947203, 38.08482404205709], 41.2409181768817)\nRMSE: 0.021425333\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 4 (+2 = total layers in the net)\nbatch_size = 1024\nlearning_rate = 5e-06\n====================================================\n\n[Epoch 1/100] Loss: 0.034173 | Time: 2.47s | LR: 0.00000500\n[Epoch 2/100] Loss: 0.003625 | Time: 2.19s | LR: 0.00000500\n[Epoch 3/100] Loss: 0.003109 | Time: 2.20s | LR: 0.00000500\n[Epoch 4/100] Loss: 0.002924 | Time: 2.20s | LR: 0.00000500\n[Epoch 5/100] Loss: 0.002824 | Time: 2.16s | LR: 0.00000500\n[Epoch 6/100] Loss: 0.002760 | Time: 2.16s | LR: 0.00000500\n[Epoch 7/100] Loss: 0.002710 | Time: 2.15s | LR: 0.00000500\n[Epoch 8/100] Loss: 0.002674 | Time: 2.20s | LR: 0.00000500\n[Epoch 9/100] Loss: 0.002646 | Time: 2.16s | LR: 0.00000500\n[Epoch 10/100] Loss: 0.002619 | Time: 2.16s | LR: 0.00000500\n[Epoch 11/100] Loss: 0.002598 | Time: 2.18s | LR: 0.00000500\n[Epoch 12/100] Loss: 0.002578 | Time: 2.20s | LR: 0.00000500\n[Epoch 13/100] Loss: 0.002562 | Time: 2.16s | LR: 0.00000500\n[Epoch 14/100] Loss: 0.002545 | Time: 2.18s | LR: 0.00000500\n[Epoch 15/100] Loss: 0.002528 | Time: 2.19s | LR: 0.00000500\n[Epoch 16/100] Loss: 0.002514 | Time: 2.38s | LR: 0.00000400\n[Epoch 17/100] Loss: 0.002490 | Time: 2.19s | LR: 0.00000400\n[Epoch 18/100] Loss: 0.002478 | Time: 2.18s | LR: 0.00000400\n[Epoch 19/100] Loss: 0.002464 | Time: 2.18s | LR: 0.00000400\n[Epoch 20/100] Loss: 0.002456 | Time: 2.27s | LR: 0.00000400\n[Epoch 21/100] Loss: 0.002442 | Time: 2.20s | LR: 0.00000400\n[Epoch 22/100] Loss: 0.002432 | Time: 2.19s | LR: 0.00000400\n[Epoch 23/100] Loss: 0.002422 | Time: 2.17s | LR: 0.00000400\n[Epoch 24/100] Loss: 0.002411 | Time: 2.16s | LR: 0.00000400\n[Epoch 25/100] Loss: 0.002401 | Time: 2.18s | LR: 0.00000400\n[Epoch 26/100] Loss: 0.002392 | Time: 2.18s | LR: 0.00000400\n[Epoch 27/100] Loss: 0.002381 | Time: 2.14s | LR: 0.00000400\n[Epoch 28/100] Loss: 0.002372 | Time: 2.20s | LR: 0.00000400\n[Epoch 29/100] Loss: 0.002360 | Time: 2.16s | LR: 0.00000400\n[Epoch 30/100] Loss: 0.002349 | Time: 2.48s | LR: 0.00000400\n[Epoch 31/100] Loss: 0.002341 | Time: 2.21s | LR: 0.00000320\n[Epoch 32/100] Loss: 0.002318 | Time: 2.16s | LR: 0.00000320\n[Epoch 33/100] Loss: 0.002308 | Time: 2.15s | LR: 0.00000320\n[Epoch 34/100] Loss: 0.002303 | Time: 2.17s | LR: 0.00000320\n[Epoch 35/100] Loss: 0.002295 | Time: 2.21s | LR: 0.00000320\n[Epoch 36/100] Loss: 0.002284 | Time: 2.18s | LR: 0.00000320\n[Epoch 37/100] Loss: 0.002276 | Time: 2.18s | LR: 0.00000320\n[Epoch 38/100] Loss: 0.002268 | Time: 2.15s | LR: 0.00000320\n[Epoch 39/100] Loss: 0.002260 | Time: 2.17s | LR: 0.00000320\n[Epoch 40/100] Loss: 0.002254 | Time: 2.19s | LR: 0.00000320\n[Epoch 41/100] Loss: 0.002242 | Time: 2.14s | LR: 0.00000320\n[Epoch 42/100] Loss: 0.002235 | Time: 2.18s | LR: 0.00000320\n[Epoch 43/100] Loss: 0.002229 | Time: 2.16s | LR: 0.00000320\n[Epoch 44/100] Loss: 0.002220 | Time: 2.19s | LR: 0.00000320\n[Epoch 45/100] Loss: 0.002211 | Time: 2.46s | LR: 0.00000320\n[Epoch 46/100] Loss: 0.002202 | Time: 2.17s | LR: 0.00000256\n[Epoch 47/100] Loss: 0.002183 | Time: 2.15s | LR: 0.00000256\n[Epoch 48/100] Loss: 0.002175 | Time: 2.14s | LR: 0.00000256\n[Epoch 49/100] Loss: 0.002168 | Time: 2.21s | LR: 0.00000256\n[Epoch 50/100] Loss: 0.002162 | Time: 2.16s | LR: 0.00000256\n[Epoch 51/100] Loss: 0.002154 | Time: 2.18s | LR: 0.00000256\n[Epoch 52/100] Loss: 0.002148 | Time: 2.17s | LR: 0.00000256\n[Epoch 53/100] Loss: 0.002142 | Time: 2.18s | LR: 0.00000256\n[Epoch 54/100] Loss: 0.002134 | Time: 2.18s | LR: 0.00000256\n[Epoch 55/100] Loss: 0.002126 | Time: 2.17s | LR: 0.00000256\n[Epoch 56/100] Loss: 0.002122 | Time: 2.18s | LR: 0.00000256\n[Epoch 57/100] Loss: 0.002113 | Time: 2.14s | LR: 0.00000256\n[Epoch 58/100] Loss: 0.002108 | Time: 2.18s | LR: 0.00000256\n[Epoch 59/100] Loss: 0.002100 | Time: 2.34s | LR: 0.00000256\n[Epoch 60/100] Loss: 0.002092 | Time: 2.26s | LR: 0.00000256\n[Epoch 61/100] Loss: 0.002085 | Time: 2.16s | LR: 0.00000205\n[Epoch 62/100] Loss: 0.002069 | Time: 2.20s | LR: 0.00000205\n[Epoch 63/100] Loss: 0.002062 | Time: 2.19s | LR: 0.00000205\n[Epoch 64/100] Loss: 0.002057 | Time: 2.16s | LR: 0.00000205\n[Epoch 65/100] Loss: 0.002051 | Time: 2.19s | LR: 0.00000205\n[Epoch 66/100] Loss: 0.002045 | Time: 2.14s | LR: 0.00000205\n[Epoch 67/100] Loss: 0.002040 | Time: 2.17s | LR: 0.00000205\n[Epoch 68/100] Loss: 0.002034 | Time: 2.17s | LR: 0.00000205\n[Epoch 69/100] Loss: 0.002029 | Time: 2.17s | LR: 0.00000205\n[Epoch 70/100] Loss: 0.002023 | Time: 2.16s | LR: 0.00000205\n[Epoch 71/100] Loss: 0.002018 | Time: 2.16s | LR: 0.00000205\n[Epoch 72/100] Loss: 0.002012 | Time: 2.19s | LR: 0.00000205\n[Epoch 73/100] Loss: 0.002007 | Time: 2.15s | LR: 0.00000205\n[Epoch 74/100] Loss: 0.002001 | Time: 2.38s | LR: 0.00000205\n[Epoch 75/100] Loss: 0.001994 | Time: 2.21s | LR: 0.00000205\n[Epoch 76/100] Loss: 0.001989 | Time: 2.19s | LR: 0.00000164\n[Epoch 77/100] Loss: 0.001975 | Time: 2.22s | LR: 0.00000164\n[Epoch 78/100] Loss: 0.001969 | Time: 2.31s | LR: 0.00000164\n[Epoch 79/100] Loss: 0.001964 | Time: 2.16s | LR: 0.00000164\n[Epoch 80/100] Loss: 0.001960 | Time: 2.19s | LR: 0.00000164\n[Epoch 81/100] Loss: 0.001955 | Time: 2.28s | LR: 0.00000164\n[Epoch 82/100] Loss: 0.001951 | Time: 2.19s | LR: 0.00000164\n[Epoch 83/100] Loss: 0.001946 | Time: 2.15s | LR: 0.00000164\n[Epoch 84/100] Loss: 0.001941 | Time: 2.18s | LR: 0.00000164\n[Epoch 85/100] Loss: 0.001937 | Time: 2.23s | LR: 0.00000164\n[Epoch 86/100] Loss: 0.001932 | Time: 2.15s | LR: 0.00000164\n[Epoch 87/100] Loss: 0.001927 | Time: 2.16s | LR: 0.00000164\n[Epoch 88/100] Loss: 0.001923 | Time: 2.19s | LR: 0.00000164\n[Epoch 89/100] Loss: 0.001918 | Time: 2.37s | LR: 0.00000164\n[Epoch 90/100] Loss: 0.001914 | Time: 2.19s | LR: 0.00000164\n[Epoch 91/100] Loss: 0.001910 | Time: 2.18s | LR: 0.00000131\n[Epoch 92/100] Loss: 0.001897 | Time: 2.18s | LR: 0.00000131\n[Epoch 93/100] Loss: 0.001892 | Time: 2.16s | LR: 0.00000131\n[Epoch 94/100] Loss: 0.001889 | Time: 2.20s | LR: 0.00000131\n[Epoch 95/100] Loss: 0.001885 | Time: 2.17s | LR: 0.00000131\n[Epoch 96/100] Loss: 0.001881 | Time: 2.15s | LR: 0.00000131\n[Epoch 97/100] Loss: 0.001877 | Time: 2.14s | LR: 0.00000131\n[Epoch 98/100] Loss: 0.001873 | Time: 2.18s | LR: 0.00000131\n[Epoch 99/100] Loss: 0.001869 | Time: 2.19s | LR: 0.00000131\n[Epoch 100/100] Loss: 0.001866 | Time: 2.16s | LR: 0.00000131\n\nFinished training.\nBest epoch: 100 with loss: 0.0018658409\nGMM_Mean0  PSNR: 51.61970351777191\nGMM_Mean1  PSNR: 51.81057985682208\nGMM_Mean2  PSNR: 51.868212666715515\nGMM_Std0  PSNR: 32.82692067454033\nGMM_Std1  PSNR: 33.633671285855264\nGMM_Std2  PSNR: 33.70047608228761\nGMM_Weight0  PSNR: 30.312278589310534\nGMM_Weight1  PSNR: 30.601527693418046\nGMM_Weight2  PSNR: 30.29368157503225\n\nAverage psnr :  38.5185613268615\nPSNR: ([51.61970351777191, 51.81057985682208, 51.868212666715515, 32.82692067454033, 33.633671285855264, 33.70047608228761, 30.312278589310534, 30.601527693418046, 30.29368157503225], 38.5185613268615)\nRMSE: 0.042880263\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 8 (+2 = total layers in the net)\nbatch_size = 512\nlearning_rate = 5e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.005292 | Time: 4.13s | LR: 0.00005000\n[Epoch 2/100] Loss: 0.002885 | Time: 4.40s | LR: 0.00005000\n[Epoch 3/100] Loss: 0.002806 | Time: 4.22s | LR: 0.00005000\n[Epoch 4/100] Loss: 0.002773 | Time: 4.24s | LR: 0.00005000\n[Epoch 5/100] Loss: 0.002770 | Time: 4.23s | LR: 0.00005000\n[Epoch 6/100] Loss: 0.002756 | Time: 4.23s | LR: 0.00005000\n[Epoch 7/100] Loss: 0.002749 | Time: 4.22s | LR: 0.00005000\n[Epoch 8/100] Loss: 0.002753 | Time: 4.19s | LR: 0.00005000\n[Epoch 9/100] Loss: 0.002811 | Time: 4.44s | LR: 0.00005000\n[Epoch 10/100] Loss: 0.003380 | Time: 4.31s | LR: 0.00005000\n[Epoch 11/100] Loss: 0.019562 | Time: 4.41s | LR: 0.00005000\n[Epoch 12/100] Loss: 0.021212 | Time: 4.20s | LR: 0.00005000\n[Epoch 13/100] Loss: 0.017895 | Time: 4.55s | LR: 0.00005000\n[Epoch 14/100] Loss: 0.016087 | Time: 4.60s | LR: 0.00005000\n[Epoch 15/100] Loss: 0.014918 | Time: 4.41s | LR: 0.00005000\n[Epoch 16/100] Loss: 0.014134 | Time: 4.48s | LR: 0.00004000\n[Epoch 17/100] Loss: 0.013148 | Time: 4.45s | LR: 0.00004000\n[Epoch 18/100] Loss: 0.012544 | Time: 4.32s | LR: 0.00004000\n[Epoch 19/100] Loss: 0.012067 | Time: 4.14s | LR: 0.00004000\n[Epoch 20/100] Loss: 0.011789 | Time: 4.26s | LR: 0.00004000\n[Epoch 21/100] Loss: 0.011520 | Time: 4.37s | LR: 0.00004000\n[Epoch 22/100] Loss: 0.011312 | Time: 4.21s | LR: 0.00004000\n[Epoch 23/100] Loss: 0.011222 | Time: 4.34s | LR: 0.00004000\n[Epoch 24/100] Loss: 0.011131 | Time: 4.39s | LR: 0.00004000\n[Epoch 25/100] Loss: 0.011051 | Time: 4.33s | LR: 0.00004000\n[Epoch 26/100] Loss: 0.010999 | Time: 4.20s | LR: 0.00004000\n[Epoch 27/100] Loss: 0.010879 | Time: 4.41s | LR: 0.00004000\n[Epoch 28/100] Loss: 0.010843 | Time: 4.36s | LR: 0.00004000\n[Epoch 29/100] Loss: 0.010808 | Time: 4.32s | LR: 0.00004000\n[Epoch 30/100] Loss: 0.010757 | Time: 4.23s | LR: 0.00004000\n[Epoch 31/100] Loss: 0.010804 | Time: 4.37s | LR: 0.00003200\n[Epoch 32/100] Loss: 0.010503 | Time: 4.24s | LR: 0.00003200\n[Epoch 33/100] Loss: 0.010138 | Time: 4.26s | LR: 0.00003200\n[Epoch 34/100] Loss: 0.010007 | Time: 4.21s | LR: 0.00003200\n[Epoch 35/100] Loss: 0.009951 | Time: 4.24s | LR: 0.00003200\n[Epoch 36/100] Loss: 0.009832 | Time: 4.25s | LR: 0.00003200\n[Epoch 37/100] Loss: 0.009756 | Time: 4.29s | LR: 0.00003200\n[Epoch 38/100] Loss: 0.009756 | Time: 4.20s | LR: 0.00003200\n[Epoch 39/100] Loss: 0.009671 | Time: 4.42s | LR: 0.00003200\n[Epoch 40/100] Loss: 0.009654 | Time: 4.28s | LR: 0.00003200\n[Epoch 41/100] Loss: 0.009765 | Time: 4.40s | LR: 0.00003200\n[Epoch 42/100] Loss: 0.009555 | Time: 4.27s | LR: 0.00003200\n[Epoch 43/100] Loss: 0.009634 | Time: 4.24s | LR: 0.00003200\n[Epoch 44/100] Loss: 0.009548 | Time: 4.32s | LR: 0.00003200\n[Epoch 45/100] Loss: 0.009535 | Time: 4.10s | LR: 0.00003200\n[Epoch 46/100] Loss: 0.009560 | Time: 4.43s | LR: 0.00002560\n[Epoch 47/100] Loss: 0.009361 | Time: 4.25s | LR: 0.00002560\n[Epoch 48/100] Loss: 0.009168 | Time: 4.24s | LR: 0.00002560\n[Epoch 49/100] Loss: 0.008994 | Time: 4.26s | LR: 0.00002560\n[Epoch 50/100] Loss: 0.008881 | Time: 4.22s | LR: 0.00002560\n[Epoch 51/100] Loss: 0.008904 | Time: 4.25s | LR: 0.00002560\n[Epoch 52/100] Loss: 0.008814 | Time: 4.32s | LR: 0.00002560\n[Epoch 53/100] Loss: 0.008850 | Time: 4.36s | LR: 0.00002560\n[Epoch 54/100] Loss: 0.008794 | Time: 4.37s | LR: 0.00002560\n[Epoch 55/100] Loss: 0.008801 | Time: 4.32s | LR: 0.00002560\n[Epoch 56/100] Loss: 0.008800 | Time: 4.27s | LR: 0.00002560\n[Epoch 57/100] Loss: 0.008729 | Time: 4.14s | LR: 0.00002560\n[Epoch 58/100] Loss: 0.008737 | Time: 4.29s | LR: 0.00002560\n[Epoch 59/100] Loss: 0.008756 | Time: 4.31s | LR: 0.00002560\n[Epoch 60/100] Loss: 0.008772 | Time: 4.34s | LR: 0.00002560\n[Epoch 61/100] Loss: 0.008722 | Time: 4.47s | LR: 0.00002048\n[Epoch 62/100] Loss: 0.008510 | Time: 4.30s | LR: 0.00002048\n[Epoch 63/100] Loss: 0.008326 | Time: 4.29s | LR: 0.00002048\n[Epoch 64/100] Loss: 0.008275 | Time: 4.26s | LR: 0.00002048\n[Epoch 65/100] Loss: 0.008314 | Time: 4.32s | LR: 0.00002048\n[Epoch 66/100] Loss: 0.008329 | Time: 4.10s | LR: 0.00002048\n[Epoch 67/100] Loss: 0.008276 | Time: 4.35s | LR: 0.00002048\n[Epoch 68/100] Loss: 0.008209 | Time: 4.40s | LR: 0.00002048\n[Epoch 69/100] Loss: 0.008240 | Time: 4.47s | LR: 0.00002048\n[Epoch 70/100] Loss: 0.008271 | Time: 4.39s | LR: 0.00002048\n[Epoch 71/100] Loss: 0.008208 | Time: 4.34s | LR: 0.00002048\n[Epoch 72/100] Loss: 0.008177 | Time: 4.26s | LR: 0.00002048\n[Epoch 73/100] Loss: 0.008172 | Time: 4.26s | LR: 0.00002048\n[Epoch 74/100] Loss: 0.008148 | Time: 4.23s | LR: 0.00002048\n[Epoch 75/100] Loss: 0.008150 | Time: 4.21s | LR: 0.00002048\n[Epoch 76/100] Loss: 0.008211 | Time: 4.47s | LR: 0.00001638\n[Epoch 77/100] Loss: 0.008001 | Time: 4.35s | LR: 0.00001638\n[Epoch 78/100] Loss: 0.007884 | Time: 4.38s | LR: 0.00001638\n[Epoch 79/100] Loss: 0.007896 | Time: 4.06s | LR: 0.00001638\n[Epoch 80/100] Loss: 0.007816 | Time: 4.26s | LR: 0.00001638\n[Epoch 81/100] Loss: 0.007803 | Time: 4.31s | LR: 0.00001638\n[Epoch 82/100] Loss: 0.007799 | Time: 4.32s | LR: 0.00001638\n[Epoch 83/100] Loss: 0.007797 | Time: 4.50s | LR: 0.00001638\n[Epoch 84/100] Loss: 0.007826 | Time: 4.36s | LR: 0.00001638\n[Epoch 85/100] Loss: 0.007770 | Time: 4.43s | LR: 0.00001638\n[Epoch 86/100] Loss: 0.007749 | Time: 4.33s | LR: 0.00001638\n[Epoch 87/100] Loss: 0.007744 | Time: 4.40s | LR: 0.00001638\n[Epoch 88/100] Loss: 0.007729 | Time: 4.37s | LR: 0.00001638\n[Epoch 89/100] Loss: 0.007714 | Time: 4.16s | LR: 0.00001638\n[Epoch 90/100] Loss: 0.007730 | Time: 4.30s | LR: 0.00001638\n[Epoch 91/100] Loss: 0.007709 | Time: 4.27s | LR: 0.00001311\n[Epoch 92/100] Loss: 0.007624 | Time: 4.32s | LR: 0.00001311\n[Epoch 93/100] Loss: 0.007597 | Time: 4.26s | LR: 0.00001311\n[Epoch 94/100] Loss: 0.007536 | Time: 4.27s | LR: 0.00001311\n[Epoch 95/100] Loss: 0.007451 | Time: 4.37s | LR: 0.00001311\n[Epoch 96/100] Loss: 0.007463 | Time: 4.33s | LR: 0.00001311\n[Epoch 97/100] Loss: 0.007421 | Time: 4.27s | LR: 0.00001311\n[Epoch 98/100] Loss: 0.007391 | Time: 4.50s | LR: 0.00001311\n[Epoch 99/100] Loss: 0.007392 | Time: 4.39s | LR: 0.00001311\n[Epoch 100/100] Loss: 0.007412 | Time: 4.28s | LR: 0.00001311\n\nFinished training.\nBest epoch: 7 with loss: 0.0027485578\nGMM_Mean0  PSNR: 25.318687785353227\nGMM_Mean1  PSNR: 25.303296011828323\nGMM_Mean2  PSNR: 25.29629813690713\nGMM_Std0  PSNR: 27.77916721727999\nGMM_Std1  PSNR: 27.91868058737424\nGMM_Std2  PSNR: 28.59253409855605\nGMM_Weight0  PSNR: 29.82115873834691\nGMM_Weight1  PSNR: 30.111233409804363\nGMM_Weight2  PSNR: 29.802120981305738\n\nAverage psnr :  27.77146410741733\nPSNR: ([25.318687785353227, 25.303296011828323, 25.29629813690713, 27.77916721727999, 27.91868058737424, 28.59253409855605, 29.82115873834691, 30.111233409804363, 29.802120981305738], 27.77146410741733)\nRMSE: 0.085815765\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 8 (+2 = total layers in the net)\nbatch_size = 512\nlearning_rate = 1e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.008163 | Time: 4.41s | LR: 0.00001000\n[Epoch 2/100] Loss: 0.002797 | Time: 4.23s | LR: 0.00001000\n[Epoch 3/100] Loss: 0.002695 | Time: 4.30s | LR: 0.00001000\n[Epoch 4/100] Loss: 0.002620 | Time: 4.31s | LR: 0.00001000\n[Epoch 5/100] Loss: 0.002565 | Time: 4.45s | LR: 0.00001000\n[Epoch 6/100] Loss: 0.002518 | Time: 4.28s | LR: 0.00001000\n[Epoch 7/100] Loss: 0.002470 | Time: 4.20s | LR: 0.00001000\n[Epoch 8/100] Loss: 0.002424 | Time: 4.19s | LR: 0.00001000\n[Epoch 9/100] Loss: 0.002376 | Time: 4.22s | LR: 0.00001000\n[Epoch 10/100] Loss: 0.002329 | Time: 4.27s | LR: 0.00001000\n[Epoch 11/100] Loss: 0.002279 | Time: 4.33s | LR: 0.00001000\n[Epoch 12/100] Loss: 0.002231 | Time: 4.27s | LR: 0.00001000\n[Epoch 13/100] Loss: 0.002180 | Time: 4.39s | LR: 0.00001000\n[Epoch 14/100] Loss: 0.002131 | Time: 4.33s | LR: 0.00001000\n[Epoch 15/100] Loss: 0.002078 | Time: 4.23s | LR: 0.00001000\n[Epoch 16/100] Loss: 0.002026 | Time: 4.14s | LR: 0.00000800\n[Epoch 17/100] Loss: 0.001903 | Time: 4.31s | LR: 0.00000800\n[Epoch 18/100] Loss: 0.001841 | Time: 4.10s | LR: 0.00000800\n[Epoch 19/100] Loss: 0.001794 | Time: 4.27s | LR: 0.00000800\n[Epoch 20/100] Loss: 0.001740 | Time: 4.48s | LR: 0.00000800\n[Epoch 21/100] Loss: 0.001688 | Time: 4.28s | LR: 0.00000800\n[Epoch 22/100] Loss: 0.001626 | Time: 4.25s | LR: 0.00000800\n[Epoch 23/100] Loss: 0.001560 | Time: 4.35s | LR: 0.00000800\n[Epoch 24/100] Loss: 0.001496 | Time: 4.20s | LR: 0.00000800\n[Epoch 25/100] Loss: 0.001425 | Time: 4.38s | LR: 0.00000800\n[Epoch 26/100] Loss: 0.001354 | Time: 4.24s | LR: 0.00000800\n[Epoch 27/100] Loss: 0.001275 | Time: 4.38s | LR: 0.00000800\n[Epoch 28/100] Loss: 0.001193 | Time: 4.39s | LR: 0.00000800\n[Epoch 29/100] Loss: 0.001112 | Time: 4.26s | LR: 0.00000800\n[Epoch 30/100] Loss: 0.001033 | Time: 4.26s | LR: 0.00000800\n[Epoch 31/100] Loss: 0.000950 | Time: 4.11s | LR: 0.00000640\n[Epoch 32/100] Loss: 0.000763 | Time: 4.23s | LR: 0.00000640\n[Epoch 33/100] Loss: 0.000648 | Time: 4.15s | LR: 0.00000640\n[Epoch 34/100] Loss: 0.000589 | Time: 4.28s | LR: 0.00000640\n[Epoch 35/100] Loss: 0.000548 | Time: 4.45s | LR: 0.00000640\n[Epoch 36/100] Loss: 0.000510 | Time: 4.25s | LR: 0.00000640\n[Epoch 37/100] Loss: 0.000480 | Time: 4.20s | LR: 0.00000640\n[Epoch 38/100] Loss: 0.000458 | Time: 4.33s | LR: 0.00000640\n[Epoch 39/100] Loss: 0.000433 | Time: 4.31s | LR: 0.00000640\n[Epoch 40/100] Loss: 0.000412 | Time: 4.14s | LR: 0.00000640\n[Epoch 41/100] Loss: 0.000394 | Time: 4.28s | LR: 0.00000640\n[Epoch 42/100] Loss: 0.000381 | Time: 4.97s | LR: 0.00000640\n[Epoch 43/100] Loss: 0.000368 | Time: 4.41s | LR: 0.00000640\n[Epoch 44/100] Loss: 0.000355 | Time: 4.24s | LR: 0.00000640\n[Epoch 45/100] Loss: 0.000346 | Time: 4.34s | LR: 0.00000640\n[Epoch 46/100] Loss: 0.000335 | Time: 4.27s | LR: 0.00000512\n[Epoch 47/100] Loss: 0.000256 | Time: 4.17s | LR: 0.00000512\n[Epoch 48/100] Loss: 0.000191 | Time: 4.24s | LR: 0.00000512\n[Epoch 49/100] Loss: 0.000184 | Time: 4.25s | LR: 0.00000512\n[Epoch 50/100] Loss: 0.000200 | Time: 4.68s | LR: 0.00000512\n[Epoch 51/100] Loss: 0.000218 | Time: 4.16s | LR: 0.00000512\n[Epoch 52/100] Loss: 0.000219 | Time: 3.97s | LR: 0.00000512\n[Epoch 53/100] Loss: 0.000209 | Time: 4.56s | LR: 0.00000512\n[Epoch 54/100] Loss: 0.000199 | Time: 4.63s | LR: 0.00000512\n[Epoch 55/100] Loss: 0.000193 | Time: 4.63s | LR: 0.00000512\n[Epoch 56/100] Loss: 0.000191 | Time: 4.08s | LR: 0.00000512\n[Epoch 57/100] Loss: 0.000190 | Time: 4.48s | LR: 0.00000512\n[Epoch 58/100] Loss: 0.000187 | Time: 4.65s | LR: 0.00000512\n[Epoch 59/100] Loss: 0.000184 | Time: 4.25s | LR: 0.00000512\n[Epoch 60/100] Loss: 0.000182 | Time: 4.61s | LR: 0.00000512\n[Epoch 61/100] Loss: 0.000180 | Time: 4.21s | LR: 0.00000410\n[Epoch 62/100] Loss: 0.000135 | Time: 4.48s | LR: 0.00000410\n[Epoch 63/100] Loss: 0.000092 | Time: 4.33s | LR: 0.00000410\n[Epoch 64/100] Loss: 0.000087 | Time: 4.97s | LR: 0.00000410\n[Epoch 65/100] Loss: 0.000102 | Time: 4.35s | LR: 0.00000410\n[Epoch 66/100] Loss: 0.000123 | Time: 4.81s | LR: 0.00000410\n[Epoch 67/100] Loss: 0.000128 | Time: 4.25s | LR: 0.00000410\n[Epoch 68/100] Loss: 0.000121 | Time: 4.67s | LR: 0.00000410\n[Epoch 69/100] Loss: 0.000113 | Time: 4.21s | LR: 0.00000410\n[Epoch 70/100] Loss: 0.000109 | Time: 4.89s | LR: 0.00000410\n[Epoch 71/100] Loss: 0.000109 | Time: 4.47s | LR: 0.00000410\n[Epoch 72/100] Loss: 0.000111 | Time: 4.63s | LR: 0.00000410\n[Epoch 73/100] Loss: 0.000112 | Time: 4.60s | LR: 0.00000410\n[Epoch 74/100] Loss: 0.000111 | Time: 4.64s | LR: 0.00000410\n[Epoch 75/100] Loss: 0.000109 | Time: 4.23s | LR: 0.00000410\n[Epoch 76/100] Loss: 0.000108 | Time: 4.50s | LR: 0.00000328\n[Epoch 77/100] Loss: 0.000082 | Time: 4.22s | LR: 0.00000328\n[Epoch 78/100] Loss: 0.000053 | Time: 4.91s | LR: 0.00000328\n[Epoch 79/100] Loss: 0.000049 | Time: 4.41s | LR: 0.00000328\n[Epoch 80/100] Loss: 0.000059 | Time: 4.68s | LR: 0.00000328\n[Epoch 81/100] Loss: 0.000075 | Time: 4.25s | LR: 0.00000328\n[Epoch 82/100] Loss: 0.000081 | Time: 4.81s | LR: 0.00000328\n[Epoch 83/100] Loss: 0.000075 | Time: 4.30s | LR: 0.00000328\n[Epoch 84/100] Loss: 0.000069 | Time: 4.74s | LR: 0.00000328\n[Epoch 85/100] Loss: 0.000066 | Time: 4.53s | LR: 0.00000328\n[Epoch 86/100] Loss: 0.000067 | Time: 4.77s | LR: 0.00000328\n[Epoch 87/100] Loss: 0.000068 | Time: 4.31s | LR: 0.00000328\n[Epoch 88/100] Loss: 0.000070 | Time: 4.51s | LR: 0.00000328\n[Epoch 89/100] Loss: 0.000069 | Time: 4.28s | LR: 0.00000328\n[Epoch 90/100] Loss: 0.000068 | Time: 4.72s | LR: 0.00000328\n[Epoch 91/100] Loss: 0.000068 | Time: 4.27s | LR: 0.00000262\n[Epoch 92/100] Loss: 0.000051 | Time: 4.92s | LR: 0.00000262\n[Epoch 93/100] Loss: 0.000032 | Time: 4.35s | LR: 0.00000262\n[Epoch 94/100] Loss: 0.000030 | Time: 4.72s | LR: 0.00000262\n[Epoch 95/100] Loss: 0.000036 | Time: 4.27s | LR: 0.00000262\n[Epoch 96/100] Loss: 0.000047 | Time: 4.65s | LR: 0.00000262\n[Epoch 97/100] Loss: 0.000051 | Time: 4.38s | LR: 0.00000262\n[Epoch 98/100] Loss: 0.000048 | Time: 4.65s | LR: 0.00000262\n[Epoch 99/100] Loss: 0.000044 | Time: 4.48s | LR: 0.00000262\n[Epoch 100/100] Loss: 0.000041 | Time: 4.74s | LR: 0.00000262\n\nFinished training.\nBest epoch: 94 with loss: 2.9525338e-05\nGMM_Mean0  PSNR: 56.0682356366993\nGMM_Mean1  PSNR: 56.09375070327519\nGMM_Mean2  PSNR: 56.06962517960451\nGMM_Std0  PSNR: 48.97867139132035\nGMM_Std1  PSNR: 49.472814735369155\nGMM_Std2  PSNR: 49.57287507746011\nGMM_Weight0  PSNR: 47.7944573331138\nGMM_Weight1  PSNR: 48.000542572603706\nGMM_Weight2  PSNR: 47.766108237170215\n\nAverage psnr :  51.09078676295737\nPSNR: ([56.0682356366993, 56.09375070327519, 56.06962517960451, 48.97867139132035, 49.472814735369155, 49.57287507746011, 47.7944573331138, 48.000542572603706, 47.766108237170215], 51.09078676295737)\nRMSE: 0.0063794167\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 8 (+2 = total layers in the net)\nbatch_size = 512\nlearning_rate = 5e-06\n====================================================\n\n[Epoch 1/100] Loss: 0.011973 | Time: 4.74s | LR: 0.00000500\n[Epoch 2/100] Loss: 0.002911 | Time: 4.59s | LR: 0.00000500\n[Epoch 3/100] Loss: 0.002748 | Time: 4.31s | LR: 0.00000500\n[Epoch 4/100] Loss: 0.002664 | Time: 4.82s | LR: 0.00000500\n[Epoch 5/100] Loss: 0.002608 | Time: 4.37s | LR: 0.00000500\n[Epoch 6/100] Loss: 0.002562 | Time: 5.10s | LR: 0.00000500\n[Epoch 7/100] Loss: 0.002517 | Time: 4.29s | LR: 0.00000500\n[Epoch 8/100] Loss: 0.002481 | Time: 4.77s | LR: 0.00000500\n[Epoch 9/100] Loss: 0.002445 | Time: 4.24s | LR: 0.00000500\n[Epoch 10/100] Loss: 0.002408 | Time: 4.72s | LR: 0.00000500\n[Epoch 11/100] Loss: 0.002367 | Time: 4.28s | LR: 0.00000500\n[Epoch 12/100] Loss: 0.002333 | Time: 4.68s | LR: 0.00000500\n[Epoch 13/100] Loss: 0.002295 | Time: 4.45s | LR: 0.00000500\n[Epoch 14/100] Loss: 0.002258 | Time: 4.74s | LR: 0.00000500\n[Epoch 15/100] Loss: 0.002218 | Time: 4.15s | LR: 0.00000500\n[Epoch 16/100] Loss: 0.002179 | Time: 4.74s | LR: 0.00000400\n[Epoch 17/100] Loss: 0.002101 | Time: 4.34s | LR: 0.00000400\n[Epoch 18/100] Loss: 0.002065 | Time: 4.74s | LR: 0.00000400\n[Epoch 19/100] Loss: 0.002030 | Time: 4.69s | LR: 0.00000400\n[Epoch 20/100] Loss: 0.001995 | Time: 5.38s | LR: 0.00000400\n[Epoch 21/100] Loss: 0.001960 | Time: 4.42s | LR: 0.00000400\n[Epoch 22/100] Loss: 0.001926 | Time: 4.74s | LR: 0.00000400\n[Epoch 23/100] Loss: 0.001888 | Time: 4.29s | LR: 0.00000400\n[Epoch 24/100] Loss: 0.001854 | Time: 4.84s | LR: 0.00000400\n[Epoch 25/100] Loss: 0.001819 | Time: 4.31s | LR: 0.00000400\n[Epoch 26/100] Loss: 0.001781 | Time: 4.73s | LR: 0.00000400\n[Epoch 27/100] Loss: 0.001743 | Time: 4.76s | LR: 0.00000400\n[Epoch 28/100] Loss: 0.001706 | Time: 4.68s | LR: 0.00000400\n[Epoch 29/100] Loss: 0.001668 | Time: 4.26s | LR: 0.00000400\n[Epoch 30/100] Loss: 0.001628 | Time: 4.72s | LR: 0.00000400\n[Epoch 31/100] Loss: 0.001588 | Time: 4.24s | LR: 0.00000320\n[Epoch 32/100] Loss: 0.001503 | Time: 4.80s | LR: 0.00000320\n[Epoch 33/100] Loss: 0.001458 | Time: 4.31s | LR: 0.00000320\n[Epoch 34/100] Loss: 0.001419 | Time: 4.85s | LR: 0.00000320\n[Epoch 35/100] Loss: 0.001376 | Time: 4.34s | LR: 0.00000320\n[Epoch 36/100] Loss: 0.001332 | Time: 4.49s | LR: 0.00000320\n[Epoch 37/100] Loss: 0.001287 | Time: 4.17s | LR: 0.00000320\n[Epoch 38/100] Loss: 0.001240 | Time: 4.77s | LR: 0.00000320\n[Epoch 39/100] Loss: 0.001191 | Time: 4.27s | LR: 0.00000320\n[Epoch 40/100] Loss: 0.001140 | Time: 4.78s | LR: 0.00000320\n[Epoch 41/100] Loss: 0.001088 | Time: 4.44s | LR: 0.00000320\n[Epoch 42/100] Loss: 0.001035 | Time: 4.67s | LR: 0.00000320\n[Epoch 43/100] Loss: 0.000984 | Time: 4.59s | LR: 0.00000320\n[Epoch 44/100] Loss: 0.000928 | Time: 4.80s | LR: 0.00000320\n[Epoch 45/100] Loss: 0.000876 | Time: 4.42s | LR: 0.00000320\n[Epoch 46/100] Loss: 0.000822 | Time: 4.66s | LR: 0.00000256\n[Epoch 47/100] Loss: 0.000726 | Time: 4.36s | LR: 0.00000256\n[Epoch 48/100] Loss: 0.000676 | Time: 4.96s | LR: 0.00000256\n[Epoch 49/100] Loss: 0.000637 | Time: 4.29s | LR: 0.00000256\n[Epoch 50/100] Loss: 0.000597 | Time: 4.58s | LR: 0.00000256\n[Epoch 51/100] Loss: 0.000560 | Time: 4.25s | LR: 0.00000256\n[Epoch 52/100] Loss: 0.000524 | Time: 4.87s | LR: 0.00000256\n[Epoch 53/100] Loss: 0.000489 | Time: 4.28s | LR: 0.00000256\n[Epoch 54/100] Loss: 0.000457 | Time: 4.82s | LR: 0.00000256\n[Epoch 55/100] Loss: 0.000427 | Time: 4.43s | LR: 0.00000256\n[Epoch 56/100] Loss: 0.000400 | Time: 4.84s | LR: 0.00000256\n[Epoch 57/100] Loss: 0.000375 | Time: 4.31s | LR: 0.00000256\n[Epoch 58/100] Loss: 0.000352 | Time: 4.76s | LR: 0.00000256\n[Epoch 59/100] Loss: 0.000331 | Time: 4.52s | LR: 0.00000256\n[Epoch 60/100] Loss: 0.000312 | Time: 4.73s | LR: 0.00000256\n[Epoch 61/100] Loss: 0.000294 | Time: 4.29s | LR: 0.00000205\n[Epoch 62/100] Loss: 0.000250 | Time: 4.83s | LR: 0.00000205\n[Epoch 63/100] Loss: 0.000228 | Time: 4.57s | LR: 0.00000205\n[Epoch 64/100] Loss: 0.000219 | Time: 4.65s | LR: 0.00000205\n[Epoch 65/100] Loss: 0.000209 | Time: 4.24s | LR: 0.00000205\n[Epoch 66/100] Loss: 0.000200 | Time: 4.73s | LR: 0.00000205\n[Epoch 67/100] Loss: 0.000191 | Time: 4.13s | LR: 0.00000205\n[Epoch 68/100] Loss: 0.000183 | Time: 4.72s | LR: 0.00000205\n[Epoch 69/100] Loss: 0.000174 | Time: 4.47s | LR: 0.00000205\n[Epoch 70/100] Loss: 0.000167 | Time: 4.63s | LR: 0.00000205\n[Epoch 71/100] Loss: 0.000159 | Time: 4.25s | LR: 0.00000205\n[Epoch 72/100] Loss: 0.000152 | Time: 4.72s | LR: 0.00000205\n[Epoch 73/100] Loss: 0.000147 | Time: 4.25s | LR: 0.00000205\n[Epoch 74/100] Loss: 0.000141 | Time: 4.65s | LR: 0.00000205\n[Epoch 75/100] Loss: 0.000136 | Time: 4.25s | LR: 0.00000205\n[Epoch 76/100] Loss: 0.000131 | Time: 4.90s | LR: 0.00000164\n[Epoch 77/100] Loss: 0.000107 | Time: 4.25s | LR: 0.00000164\n[Epoch 78/100] Loss: 0.000093 | Time: 4.74s | LR: 0.00000164\n[Epoch 79/100] Loss: 0.000090 | Time: 4.15s | LR: 0.00000164\n[Epoch 80/100] Loss: 0.000090 | Time: 4.79s | LR: 0.00000164\n[Epoch 81/100] Loss: 0.000089 | Time: 4.23s | LR: 0.00000164\n[Epoch 82/100] Loss: 0.000087 | Time: 4.74s | LR: 0.00000164\n[Epoch 83/100] Loss: 0.000084 | Time: 4.35s | LR: 0.00000164\n[Epoch 84/100] Loss: 0.000082 | Time: 4.74s | LR: 0.00000164\n[Epoch 85/100] Loss: 0.000079 | Time: 4.28s | LR: 0.00000164\n[Epoch 86/100] Loss: 0.000077 | Time: 4.59s | LR: 0.00000164\n[Epoch 87/100] Loss: 0.000074 | Time: 4.27s | LR: 0.00000164\n[Epoch 88/100] Loss: 0.000073 | Time: 4.67s | LR: 0.00000164\n[Epoch 89/100] Loss: 0.000071 | Time: 4.23s | LR: 0.00000164\n[Epoch 90/100] Loss: 0.000069 | Time: 4.80s | LR: 0.00000164\n[Epoch 91/100] Loss: 0.000067 | Time: 4.20s | LR: 0.00000131\n[Epoch 92/100] Loss: 0.000054 | Time: 4.61s | LR: 0.00000131\n[Epoch 93/100] Loss: 0.000044 | Time: 4.23s | LR: 0.00000131\n[Epoch 94/100] Loss: 0.000043 | Time: 4.70s | LR: 0.00000131\n[Epoch 95/100] Loss: 0.000045 | Time: 4.26s | LR: 0.00000131\n[Epoch 96/100] Loss: 0.000046 | Time: 4.65s | LR: 0.00000131\n[Epoch 97/100] Loss: 0.000047 | Time: 4.88s | LR: 0.00000131\n[Epoch 98/100] Loss: 0.000046 | Time: 4.72s | LR: 0.00000131\n[Epoch 99/100] Loss: 0.000044 | Time: 4.27s | LR: 0.00000131\n[Epoch 100/100] Loss: 0.000043 | Time: 4.78s | LR: 0.00000131\n\nFinished training.\nBest epoch: 100 with loss: 4.3063555e-05\nGMM_Mean0  PSNR: 55.31740165090508\nGMM_Mean1  PSNR: 55.33653128802346\nGMM_Mean2  PSNR: 55.28795057401696\nGMM_Std0  PSNR: 49.5176658693785\nGMM_Std1  PSNR: 49.38767321535722\nGMM_Std2  PSNR: 49.57974462522578\nGMM_Weight0  PSNR: 47.830003628230564\nGMM_Weight1  PSNR: 48.09716422521423\nGMM_Weight2  PSNR: 47.88477685940496\n\nAverage psnr :  50.91543465952852\nPSNR: ([55.31740165090508, 55.33653128802346, 55.28795057401696, 49.5176658693785, 49.38767321535722, 49.57974462522578, 47.830003628230564, 48.09716422521423, 47.88477685940496], 50.91543465952852)\nRMSE: 0.006352081\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 8 (+2 = total layers in the net)\nbatch_size = 1024\nlearning_rate = 5e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.006817 | Time: 3.24s | LR: 0.00005000\n[Epoch 2/100] Loss: 0.002778 | Time: 3.25s | LR: 0.00005000\n[Epoch 3/100] Loss: 0.002683 | Time: 3.26s | LR: 0.00005000\n[Epoch 4/100] Loss: 0.002618 | Time: 3.24s | LR: 0.00005000\n[Epoch 5/100] Loss: 0.002569 | Time: 3.29s | LR: 0.00005000\n[Epoch 6/100] Loss: 0.002525 | Time: 3.44s | LR: 0.00005000\n[Epoch 7/100] Loss: 0.002492 | Time: 3.25s | LR: 0.00005000\n[Epoch 8/100] Loss: 0.002476 | Time: 3.25s | LR: 0.00005000\n[Epoch 9/100] Loss: 0.002465 | Time: 3.27s | LR: 0.00005000\n[Epoch 10/100] Loss: 0.002469 | Time: 3.24s | LR: 0.00005000\n[Epoch 11/100] Loss: 0.002475 | Time: 3.29s | LR: 0.00005000\n[Epoch 12/100] Loss: 0.002478 | Time: 3.25s | LR: 0.00005000\n[Epoch 13/100] Loss: 0.002483 | Time: 3.26s | LR: 0.00005000\n[Epoch 14/100] Loss: 0.002494 | Time: 3.24s | LR: 0.00005000\n[Epoch 15/100] Loss: 0.002491 | Time: 3.37s | LR: 0.00005000\n[Epoch 16/100] Loss: 0.002501 | Time: 3.31s | LR: 0.00004000\n[Epoch 17/100] Loss: 0.002380 | Time: 3.25s | LR: 0.00004000\n[Epoch 18/100] Loss: 0.002277 | Time: 3.28s | LR: 0.00004000\n[Epoch 19/100] Loss: 0.002251 | Time: 3.25s | LR: 0.00004000\n[Epoch 20/100] Loss: 0.002253 | Time: 3.25s | LR: 0.00004000\n[Epoch 21/100] Loss: 0.002260 | Time: 3.29s | LR: 0.00004000\n[Epoch 22/100] Loss: 0.002269 | Time: 3.23s | LR: 0.00004000\n[Epoch 23/100] Loss: 0.002284 | Time: 3.26s | LR: 0.00004000\n[Epoch 24/100] Loss: 0.002295 | Time: 3.35s | LR: 0.00004000\n[Epoch 25/100] Loss: 0.002288 | Time: 3.42s | LR: 0.00004000\n[Epoch 26/100] Loss: 0.002294 | Time: 3.23s | LR: 0.00004000\n[Epoch 27/100] Loss: 0.002308 | Time: 3.27s | LR: 0.00004000\n[Epoch 28/100] Loss: 0.002329 | Time: 3.24s | LR: 0.00004000\n[Epoch 29/100] Loss: 0.002410 | Time: 3.24s | LR: 0.00004000\n[Epoch 30/100] Loss: 0.002489 | Time: 3.27s | LR: 0.00004000\n[Epoch 31/100] Loss: 0.002532 | Time: 3.31s | LR: 0.00003200\n[Epoch 32/100] Loss: 0.002382 | Time: 3.27s | LR: 0.00003200\n[Epoch 33/100] Loss: 0.002304 | Time: 3.27s | LR: 0.00003200\n[Epoch 34/100] Loss: 0.002344 | Time: 3.26s | LR: 0.00003200\n[Epoch 35/100] Loss: 0.002285 | Time: 3.47s | LR: 0.00003200\n[Epoch 36/100] Loss: 0.002535 | Time: 3.28s | LR: 0.00003200\n[Epoch 37/100] Loss: 0.002351 | Time: 3.26s | LR: 0.00003200\n[Epoch 38/100] Loss: 0.002174 | Time: 3.26s | LR: 0.00003200\n[Epoch 39/100] Loss: 0.002149 | Time: 3.26s | LR: 0.00003200\n[Epoch 40/100] Loss: 0.002134 | Time: 3.24s | LR: 0.00003200\n[Epoch 41/100] Loss: 0.002109 | Time: 3.27s | LR: 0.00003200\n[Epoch 42/100] Loss: 0.002113 | Time: 3.29s | LR: 0.00003200\n[Epoch 43/100] Loss: 0.002172 | Time: 3.26s | LR: 0.00003200\n[Epoch 44/100] Loss: 0.002292 | Time: 3.42s | LR: 0.00003200\n[Epoch 45/100] Loss: 0.002759 | Time: 3.32s | LR: 0.00003200\n[Epoch 46/100] Loss: 0.002632 | Time: 3.25s | LR: 0.00002560\n[Epoch 47/100] Loss: 0.002421 | Time: 3.25s | LR: 0.00002560\n[Epoch 48/100] Loss: 0.002270 | Time: 3.27s | LR: 0.00002560\n[Epoch 49/100] Loss: 0.002161 | Time: 3.27s | LR: 0.00002560\n[Epoch 50/100] Loss: 0.002112 | Time: 3.25s | LR: 0.00002560\n[Epoch 51/100] Loss: 0.002110 | Time: 3.26s | LR: 0.00002560\n[Epoch 52/100] Loss: 0.002132 | Time: 3.27s | LR: 0.00002560\n[Epoch 53/100] Loss: 0.002117 | Time: 3.24s | LR: 0.00002560\n[Epoch 54/100] Loss: 0.002139 | Time: 3.34s | LR: 0.00002560\n[Epoch 55/100] Loss: 0.002162 | Time: 3.25s | LR: 0.00002560\n[Epoch 56/100] Loss: 0.002176 | Time: 3.25s | LR: 0.00002560\n[Epoch 57/100] Loss: 0.002147 | Time: 3.26s | LR: 0.00002560\n[Epoch 58/100] Loss: 0.002177 | Time: 3.26s | LR: 0.00002560\n[Epoch 59/100] Loss: 0.002208 | Time: 3.24s | LR: 0.00002560\n[Epoch 60/100] Loss: 0.002169 | Time: 3.26s | LR: 0.00002560\n[Epoch 61/100] Loss: 0.002210 | Time: 3.26s | LR: 0.00002048\n[Epoch 62/100] Loss: 0.002127 | Time: 3.26s | LR: 0.00002048\n[Epoch 63/100] Loss: 0.002138 | Time: 3.25s | LR: 0.00002048\n[Epoch 64/100] Loss: 0.002065 | Time: 3.43s | LR: 0.00002048\n[Epoch 65/100] Loss: 0.002046 | Time: 3.23s | LR: 0.00002048\n[Epoch 66/100] Loss: 0.002047 | Time: 3.25s | LR: 0.00002048\n[Epoch 67/100] Loss: 0.002078 | Time: 3.26s | LR: 0.00002048\n[Epoch 68/100] Loss: 0.002103 | Time: 3.24s | LR: 0.00002048\n[Epoch 69/100] Loss: 0.002108 | Time: 3.24s | LR: 0.00002048\n[Epoch 70/100] Loss: 0.002071 | Time: 3.27s | LR: 0.00002048\n[Epoch 71/100] Loss: 0.002055 | Time: 3.25s | LR: 0.00002048\n[Epoch 72/100] Loss: 0.002087 | Time: 3.26s | LR: 0.00002048\n[Epoch 73/100] Loss: 0.002106 | Time: 3.26s | LR: 0.00002048\n[Epoch 74/100] Loss: 0.002066 | Time: 3.46s | LR: 0.00002048\n[Epoch 75/100] Loss: 0.002089 | Time: 3.27s | LR: 0.00002048\n[Epoch 76/100] Loss: 0.002087 | Time: 3.29s | LR: 0.00001638\n[Epoch 77/100] Loss: 0.002018 | Time: 3.26s | LR: 0.00001638\n[Epoch 78/100] Loss: 0.001965 | Time: 3.25s | LR: 0.00001638\n[Epoch 79/100] Loss: 0.001960 | Time: 3.29s | LR: 0.00001638\n[Epoch 80/100] Loss: 0.001935 | Time: 3.25s | LR: 0.00001638\n[Epoch 81/100] Loss: 0.001945 | Time: 3.25s | LR: 0.00001638\n[Epoch 82/100] Loss: 0.001904 | Time: 3.29s | LR: 0.00001638\n[Epoch 83/100] Loss: 0.001886 | Time: 3.30s | LR: 0.00001638\n[Epoch 84/100] Loss: 0.001854 | Time: 3.30s | LR: 0.00001638\n[Epoch 85/100] Loss: 0.001841 | Time: 3.26s | LR: 0.00001638\n[Epoch 86/100] Loss: 0.001826 | Time: 3.25s | LR: 0.00001638\n[Epoch 87/100] Loss: 0.001845 | Time: 3.25s | LR: 0.00001638\n[Epoch 88/100] Loss: 0.001821 | Time: 3.25s | LR: 0.00001638\n[Epoch 89/100] Loss: 0.001829 | Time: 3.25s | LR: 0.00001638\n[Epoch 90/100] Loss: 0.001796 | Time: 3.26s | LR: 0.00001638\n[Epoch 91/100] Loss: 0.001812 | Time: 3.27s | LR: 0.00001311\n[Epoch 92/100] Loss: 0.001769 | Time: 3.25s | LR: 0.00001311\n[Epoch 93/100] Loss: 0.001610 | Time: 3.35s | LR: 0.00001311\n[Epoch 94/100] Loss: 0.001482 | Time: 3.26s | LR: 0.00001311\n[Epoch 95/100] Loss: 0.001372 | Time: 3.23s | LR: 0.00001311\n[Epoch 96/100] Loss: 0.001303 | Time: 3.26s | LR: 0.00001311\n[Epoch 97/100] Loss: 0.001224 | Time: 3.27s | LR: 0.00001311\n[Epoch 98/100] Loss: 0.001178 | Time: 3.25s | LR: 0.00001311\n[Epoch 99/100] Loss: 0.001140 | Time: 3.25s | LR: 0.00001311\n[Epoch 100/100] Loss: 0.001102 | Time: 3.25s | LR: 0.00001311\n\nFinished training.\nBest epoch: 100 with loss: 0.0011023411\nGMM_Mean0  PSNR: 45.07059599860784\nGMM_Mean1  PSNR: 45.05423211372985\nGMM_Mean2  PSNR: 45.03943589245793\nGMM_Std0  PSNR: 34.16134899295916\nGMM_Std1  PSNR: 34.90813110901273\nGMM_Std2  PSNR: 34.558585230194325\nGMM_Weight0  PSNR: 34.45335302862472\nGMM_Weight1  PSNR: 34.7231492444777\nGMM_Weight2  PSNR: 34.279047439797296\n\nAverage psnr :  38.02754211665129\nPSNR: ([45.07059599860784, 45.05423211372985, 45.03943589245793, 34.16134899295916, 34.90813110901273, 34.558585230194325, 34.45335302862472, 34.7231492444777, 34.279047439797296], 38.02754211665129)\nRMSE: 0.03140655\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 8 (+2 = total layers in the net)\nbatch_size = 1024\nlearning_rate = 1e-05\n====================================================\n\n[Epoch 1/100] Loss: 0.013925 | Time: 3.26s | LR: 0.00001000\n[Epoch 2/100] Loss: 0.002896 | Time: 3.25s | LR: 0.00001000\n[Epoch 3/100] Loss: 0.002739 | Time: 3.52s | LR: 0.00001000\n[Epoch 4/100] Loss: 0.002656 | Time: 3.24s | LR: 0.00001000\n[Epoch 5/100] Loss: 0.002600 | Time: 3.27s | LR: 0.00001000\n[Epoch 6/100] Loss: 0.002556 | Time: 3.28s | LR: 0.00001000\n[Epoch 7/100] Loss: 0.002515 | Time: 3.26s | LR: 0.00001000\n[Epoch 8/100] Loss: 0.002476 | Time: 3.25s | LR: 0.00001000\n[Epoch 9/100] Loss: 0.002439 | Time: 3.27s | LR: 0.00001000\n[Epoch 10/100] Loss: 0.002405 | Time: 3.26s | LR: 0.00001000\n[Epoch 11/100] Loss: 0.002372 | Time: 3.24s | LR: 0.00001000\n[Epoch 12/100] Loss: 0.002337 | Time: 3.35s | LR: 0.00001000\n[Epoch 13/100] Loss: 0.002301 | Time: 3.28s | LR: 0.00001000\n[Epoch 14/100] Loss: 0.002265 | Time: 3.26s | LR: 0.00001000\n[Epoch 15/100] Loss: 0.002232 | Time: 3.26s | LR: 0.00001000\n[Epoch 16/100] Loss: 0.002197 | Time: 3.26s | LR: 0.00000800\n[Epoch 17/100] Loss: 0.002114 | Time: 3.25s | LR: 0.00000800\n[Epoch 18/100] Loss: 0.002078 | Time: 3.28s | LR: 0.00000800\n[Epoch 19/100] Loss: 0.002047 | Time: 3.26s | LR: 0.00000800\n[Epoch 20/100] Loss: 0.002015 | Time: 3.26s | LR: 0.00000800\n[Epoch 21/100] Loss: 0.001981 | Time: 3.27s | LR: 0.00000800\n[Epoch 22/100] Loss: 0.001945 | Time: 3.32s | LR: 0.00000800\n[Epoch 23/100] Loss: 0.001911 | Time: 3.27s | LR: 0.00000800\n[Epoch 24/100] Loss: 0.001873 | Time: 3.27s | LR: 0.00000800\n[Epoch 25/100] Loss: 0.001835 | Time: 3.25s | LR: 0.00000800\n[Epoch 26/100] Loss: 0.001800 | Time: 3.25s | LR: 0.00000800\n[Epoch 27/100] Loss: 0.001761 | Time: 3.28s | LR: 0.00000800\n[Epoch 28/100] Loss: 0.001718 | Time: 3.25s | LR: 0.00000800\n[Epoch 29/100] Loss: 0.001677 | Time: 3.28s | LR: 0.00000800\n[Epoch 30/100] Loss: 0.001633 | Time: 3.25s | LR: 0.00000800\n[Epoch 31/100] Loss: 0.001592 | Time: 3.27s | LR: 0.00000640\n[Epoch 32/100] Loss: 0.001483 | Time: 3.45s | LR: 0.00000640\n[Epoch 33/100] Loss: 0.001423 | Time: 3.28s | LR: 0.00000640\n[Epoch 34/100] Loss: 0.001375 | Time: 3.26s | LR: 0.00000640\n[Epoch 35/100] Loss: 0.001322 | Time: 3.24s | LR: 0.00000640\n[Epoch 36/100] Loss: 0.001270 | Time: 3.24s | LR: 0.00000640\n[Epoch 37/100] Loss: 0.001212 | Time: 3.28s | LR: 0.00000640\n[Epoch 38/100] Loss: 0.001150 | Time: 3.24s | LR: 0.00000640\n[Epoch 39/100] Loss: 0.001089 | Time: 3.26s | LR: 0.00000640\n[Epoch 40/100] Loss: 0.001023 | Time: 3.26s | LR: 0.00000640\n[Epoch 41/100] Loss: 0.000958 | Time: 3.26s | LR: 0.00000640\n[Epoch 42/100] Loss: 0.000894 | Time: 3.48s | LR: 0.00000640\n[Epoch 43/100] Loss: 0.000827 | Time: 3.26s | LR: 0.00000640\n[Epoch 44/100] Loss: 0.000764 | Time: 3.27s | LR: 0.00000640\n[Epoch 45/100] Loss: 0.000706 | Time: 3.25s | LR: 0.00000640\n[Epoch 46/100] Loss: 0.000647 | Time: 3.25s | LR: 0.00000512\n[Epoch 47/100] Loss: 0.000536 | Time: 3.24s | LR: 0.00000512\n[Epoch 48/100] Loss: 0.000474 | Time: 3.23s | LR: 0.00000512\n[Epoch 49/100] Loss: 0.000438 | Time: 3.26s | LR: 0.00000512\n[Epoch 50/100] Loss: 0.000407 | Time: 3.23s | LR: 0.00000512\n[Epoch 51/100] Loss: 0.000379 | Time: 3.31s | LR: 0.00000512\n[Epoch 52/100] Loss: 0.000354 | Time: 3.38s | LR: 0.00000512\n[Epoch 53/100] Loss: 0.000332 | Time: 3.25s | LR: 0.00000512\n[Epoch 54/100] Loss: 0.000313 | Time: 3.23s | LR: 0.00000512\n[Epoch 55/100] Loss: 0.000297 | Time: 3.27s | LR: 0.00000512\n[Epoch 56/100] Loss: 0.000281 | Time: 3.26s | LR: 0.00000512\n[Epoch 57/100] Loss: 0.000269 | Time: 3.26s | LR: 0.00000512\n[Epoch 58/100] Loss: 0.000258 | Time: 3.25s | LR: 0.00000512\n[Epoch 59/100] Loss: 0.000248 | Time: 3.24s | LR: 0.00000512\n[Epoch 60/100] Loss: 0.000237 | Time: 3.23s | LR: 0.00000512\n[Epoch 61/100] Loss: 0.000229 | Time: 3.34s | LR: 0.00000410\n[Epoch 62/100] Loss: 0.000183 | Time: 3.29s | LR: 0.00000410\n[Epoch 63/100] Loss: 0.000149 | Time: 3.28s | LR: 0.00000410\n[Epoch 64/100] Loss: 0.000139 | Time: 3.31s | LR: 0.00000410\n[Epoch 65/100] Loss: 0.000137 | Time: 3.28s | LR: 0.00000410\n[Epoch 66/100] Loss: 0.000138 | Time: 3.27s | LR: 0.00000410\n[Epoch 67/100] Loss: 0.000141 | Time: 3.31s | LR: 0.00000410\n[Epoch 68/100] Loss: 0.000142 | Time: 3.30s | LR: 0.00000410\n[Epoch 69/100] Loss: 0.000140 | Time: 3.24s | LR: 0.00000410\n[Epoch 70/100] Loss: 0.000137 | Time: 3.28s | LR: 0.00000410\n[Epoch 71/100] Loss: 0.000133 | Time: 3.47s | LR: 0.00000410\n[Epoch 72/100] Loss: 0.000129 | Time: 3.25s | LR: 0.00000410\n[Epoch 73/100] Loss: 0.000124 | Time: 3.27s | LR: 0.00000410\n[Epoch 74/100] Loss: 0.000122 | Time: 3.48s | LR: 0.00000410\n[Epoch 75/100] Loss: 0.000119 | Time: 3.39s | LR: 0.00000410\n[Epoch 76/100] Loss: 0.000117 | Time: 3.37s | LR: 0.00000328\n[Epoch 77/100] Loss: 0.000091 | Time: 3.37s | LR: 0.00000328\n[Epoch 78/100] Loss: 0.000066 | Time: 3.42s | LR: 0.00000328\n[Epoch 79/100] Loss: 0.000059 | Time: 3.59s | LR: 0.00000328\n[Epoch 80/100] Loss: 0.000059 | Time: 3.47s | LR: 0.00000328\n[Epoch 81/100] Loss: 0.000064 | Time: 3.46s | LR: 0.00000328\n[Epoch 82/100] Loss: 0.000071 | Time: 3.40s | LR: 0.00000328\n[Epoch 83/100] Loss: 0.000078 | Time: 3.38s | LR: 0.00000328\n[Epoch 84/100] Loss: 0.000081 | Time: 3.44s | LR: 0.00000328\n[Epoch 85/100] Loss: 0.000079 | Time: 3.40s | LR: 0.00000328\n[Epoch 86/100] Loss: 0.000077 | Time: 3.37s | LR: 0.00000328\n[Epoch 87/100] Loss: 0.000073 | Time: 3.37s | LR: 0.00000328\n[Epoch 88/100] Loss: 0.000070 | Time: 3.42s | LR: 0.00000328\n[Epoch 89/100] Loss: 0.000068 | Time: 3.39s | LR: 0.00000328\n[Epoch 90/100] Loss: 0.000068 | Time: 3.55s | LR: 0.00000328\n[Epoch 91/100] Loss: 0.000069 | Time: 3.40s | LR: 0.00000262\n[Epoch 92/100] Loss: 0.000053 | Time: 3.38s | LR: 0.00000262\n[Epoch 93/100] Loss: 0.000036 | Time: 3.36s | LR: 0.00000262\n[Epoch 94/100] Loss: 0.000031 | Time: 3.42s | LR: 0.00000262\n[Epoch 95/100] Loss: 0.000031 | Time: 3.37s | LR: 0.00000262\n[Epoch 96/100] Loss: 0.000035 | Time: 3.24s | LR: 0.00000262\n[Epoch 97/100] Loss: 0.000041 | Time: 3.29s | LR: 0.00000262\n[Epoch 98/100] Loss: 0.000048 | Time: 3.23s | LR: 0.00000262\n[Epoch 99/100] Loss: 0.000052 | Time: 3.32s | LR: 0.00000262\n[Epoch 100/100] Loss: 0.000051 | Time: 3.25s | LR: 0.00000262\n\nFinished training.\nBest epoch: 94 with loss: 3.0831226e-05\nGMM_Mean0  PSNR: 55.42151805451584\nGMM_Mean1  PSNR: 55.44025127126212\nGMM_Mean2  PSNR: 55.41054159519797\nGMM_Std0  PSNR: 48.65370670916798\nGMM_Std1  PSNR: 49.06444045370013\nGMM_Std2  PSNR: 49.13525180040957\nGMM_Weight0  PSNR: 46.89077882275741\nGMM_Weight1  PSNR: 47.08420770964003\nGMM_Weight2  PSNR: 46.88835984929958\n\nAverage psnr :  50.443228473994516\nPSNR: ([55.42151805451584, 55.44025127126212, 55.41054159519797, 48.65370670916798, 49.06444045370013, 49.13525180040957, 46.89077882275741, 47.08420770964003, 46.88835984929958], 50.443228473994516)\nRMSE: 0.0069072563\n\n*** Results appended to 'experiment_results.csv' ***\n\n\n====================================================\nStarting training with hyperparams:\nn_neurons = 480\nhidden_layers = 8 (+2 = total layers in the net)\nbatch_size = 1024\nlearning_rate = 5e-06\n====================================================\n\n[Epoch 1/100] Loss: 0.020247 | Time: 3.26s | LR: 0.00000500\n[Epoch 2/100] Loss: 0.003131 | Time: 3.25s | LR: 0.00000500\n[Epoch 3/100] Loss: 0.002871 | Time: 3.24s | LR: 0.00000500\n[Epoch 4/100] Loss: 0.002751 | Time: 3.26s | LR: 0.00000500\n[Epoch 5/100] Loss: 0.002675 | Time: 3.25s | LR: 0.00000500\n[Epoch 6/100] Loss: 0.002620 | Time: 3.25s | LR: 0.00000500\n[Epoch 7/100] Loss: 0.002577 | Time: 3.23s | LR: 0.00000500\n[Epoch 8/100] Loss: 0.002538 | Time: 3.27s | LR: 0.00000500\n[Epoch 9/100] Loss: 0.002507 | Time: 3.49s | LR: 0.00000500\n[Epoch 10/100] Loss: 0.002477 | Time: 3.27s | LR: 0.00000500\n[Epoch 11/100] Loss: 0.002451 | Time: 3.25s | LR: 0.00000500\n[Epoch 12/100] Loss: 0.002419 | Time: 3.29s | LR: 0.00000500\n[Epoch 13/100] Loss: 0.002393 | Time: 3.26s | LR: 0.00000500\n[Epoch 14/100] Loss: 0.002368 | Time: 3.24s | LR: 0.00000500\n[Epoch 15/100] Loss: 0.002344 | Time: 3.26s | LR: 0.00000500\n[Epoch 16/100] Loss: 0.002317 | Time: 3.24s | LR: 0.00000400\n[Epoch 17/100] Loss: 0.002270 | Time: 3.25s | LR: 0.00000400\n[Epoch 18/100] Loss: 0.002244 | Time: 3.34s | LR: 0.00000400\n[Epoch 19/100] Loss: 0.002225 | Time: 3.31s | LR: 0.00000400\n[Epoch 20/100] Loss: 0.002201 | Time: 3.24s | LR: 0.00000400\n[Epoch 21/100] Loss: 0.002179 | Time: 3.26s | LR: 0.00000400\n[Epoch 22/100] Loss: 0.002156 | Time: 3.23s | LR: 0.00000400\n[Epoch 23/100] Loss: 0.002136 | Time: 3.25s | LR: 0.00000400\n[Epoch 24/100] Loss: 0.002114 | Time: 3.30s | LR: 0.00000400\n[Epoch 25/100] Loss: 0.002090 | Time: 3.23s | LR: 0.00000400\n[Epoch 26/100] Loss: 0.002067 | Time: 3.24s | LR: 0.00000400\n[Epoch 27/100] Loss: 0.002046 | Time: 3.24s | LR: 0.00000400\n[Epoch 28/100] Loss: 0.002018 | Time: 3.31s | LR: 0.00000400\n[Epoch 29/100] Loss: 0.001997 | Time: 3.22s | LR: 0.00000400\n[Epoch 30/100] Loss: 0.001970 | Time: 3.26s | LR: 0.00000400\n[Epoch 31/100] Loss: 0.001948 | Time: 3.23s | LR: 0.00000320\n[Epoch 32/100] Loss: 0.001898 | Time: 3.24s | LR: 0.00000320\n[Epoch 33/100] Loss: 0.001875 | Time: 3.27s | LR: 0.00000320\n[Epoch 34/100] Loss: 0.001854 | Time: 3.24s | LR: 0.00000320\n[Epoch 35/100] Loss: 0.001834 | Time: 3.24s | LR: 0.00000320\n[Epoch 36/100] Loss: 0.001814 | Time: 3.24s | LR: 0.00000320\n[Epoch 37/100] Loss: 0.001792 | Time: 3.25s | LR: 0.00000320\n[Epoch 38/100] Loss: 0.001773 | Time: 3.34s | LR: 0.00000320\n[Epoch 39/100] Loss: 0.001751 | Time: 3.25s | LR: 0.00000320\n[Epoch 40/100] Loss: 0.001732 | Time: 3.25s | LR: 0.00000320\n[Epoch 41/100] Loss: 0.001709 | Time: 3.25s | LR: 0.00000320\n[Epoch 42/100] Loss: 0.001688 | Time: 3.27s | LR: 0.00000320\n[Epoch 43/100] Loss: 0.001666 | Time: 3.23s | LR: 0.00000320\n[Epoch 44/100] Loss: 0.001645 | Time: 3.26s | LR: 0.00000320\n[Epoch 45/100] Loss: 0.001621 | Time: 3.25s | LR: 0.00000320\n[Epoch 46/100] Loss: 0.001599 | Time: 3.24s | LR: 0.00000256\n[Epoch 47/100] Loss: 0.001548 | Time: 3.24s | LR: 0.00000256\n[Epoch 48/100] Loss: 0.001526 | Time: 3.43s | LR: 0.00000256\n[Epoch 49/100] Loss: 0.001506 | Time: 3.24s | LR: 0.00000256\n[Epoch 50/100] Loss: 0.001485 | Time: 3.23s | LR: 0.00000256\n[Epoch 51/100] Loss: 0.001465 | Time: 3.25s | LR: 0.00000256\n[Epoch 52/100] Loss: 0.001442 | Time: 3.25s | LR: 0.00000256\n[Epoch 53/100] Loss: 0.001420 | Time: 3.25s | LR: 0.00000256\n[Epoch 54/100] Loss: 0.001398 | Time: 3.25s | LR: 0.00000256\n[Epoch 55/100] Loss: 0.001375 | Time: 3.26s | LR: 0.00000256\n[Epoch 56/100] Loss: 0.001351 | Time: 3.24s | LR: 0.00000256\n[Epoch 57/100] Loss: 0.001327 | Time: 3.27s | LR: 0.00000256\n[Epoch 58/100] Loss: 0.001302 | Time: 3.46s | LR: 0.00000256\n[Epoch 59/100] Loss: 0.001276 | Time: 3.26s | LR: 0.00000256\n[Epoch 60/100] Loss: 0.001251 | Time: 3.27s | LR: 0.00000256\n[Epoch 61/100] Loss: 0.001224 | Time: 3.27s | LR: 0.00000205\n[Epoch 62/100] Loss: 0.001170 | Time: 3.26s | LR: 0.00000205\n[Epoch 63/100] Loss: 0.001144 | Time: 3.27s | LR: 0.00000205\n[Epoch 64/100] Loss: 0.001121 | Time: 3.28s | LR: 0.00000205\n[Epoch 65/100] Loss: 0.001098 | Time: 3.26s | LR: 0.00000205\n[Epoch 66/100] Loss: 0.001074 | Time: 3.25s | LR: 0.00000205\n[Epoch 67/100] Loss: 0.001049 | Time: 3.36s | LR: 0.00000205\n[Epoch 68/100] Loss: 0.001024 | Time: 3.26s | LR: 0.00000205\n[Epoch 69/100] Loss: 0.001000 | Time: 3.25s | LR: 0.00000205\n[Epoch 70/100] Loss: 0.000975 | Time: 3.29s | LR: 0.00000205\n[Epoch 71/100] Loss: 0.000950 | Time: 3.24s | LR: 0.00000205\n[Epoch 72/100] Loss: 0.000925 | Time: 3.24s | LR: 0.00000205\n[Epoch 73/100] Loss: 0.000899 | Time: 3.28s | LR: 0.00000205\n[Epoch 74/100] Loss: 0.000875 | Time: 3.27s | LR: 0.00000205\n[Epoch 75/100] Loss: 0.000849 | Time: 3.25s | LR: 0.00000205\n[Epoch 76/100] Loss: 0.000824 | Time: 3.27s | LR: 0.00000164\n[Epoch 77/100] Loss: 0.000776 | Time: 3.31s | LR: 0.00000164\n[Epoch 78/100] Loss: 0.000753 | Time: 3.24s | LR: 0.00000164\n[Epoch 79/100] Loss: 0.000733 | Time: 3.28s | LR: 0.00000164\n[Epoch 80/100] Loss: 0.000713 | Time: 3.23s | LR: 0.00000164\n[Epoch 81/100] Loss: 0.000694 | Time: 3.24s | LR: 0.00000164\n[Epoch 82/100] Loss: 0.000674 | Time: 3.26s | LR: 0.00000164\n[Epoch 83/100] Loss: 0.000655 | Time: 3.24s | LR: 0.00000164\n[Epoch 84/100] Loss: 0.000635 | Time: 3.24s | LR: 0.00000164\n[Epoch 85/100] Loss: 0.000617 | Time: 3.25s | LR: 0.00000164\n[Epoch 86/100] Loss: 0.000598 | Time: 3.25s | LR: 0.00000164\n[Epoch 87/100] Loss: 0.000580 | Time: 3.44s | LR: 0.00000164\n[Epoch 88/100] Loss: 0.000563 | Time: 3.27s | LR: 0.00000164\n[Epoch 89/100] Loss: 0.000545 | Time: 3.23s | LR: 0.00000164\n[Epoch 90/100] Loss: 0.000529 | Time: 3.27s | LR: 0.00000164\n[Epoch 91/100] Loss: 0.000512 | Time: 3.26s | LR: 0.00000131\n[Epoch 92/100] Loss: 0.000479 | Time: 3.24s | LR: 0.00000131\n[Epoch 93/100] Loss: 0.000464 | Time: 3.27s | LR: 0.00000131\n[Epoch 94/100] Loss: 0.000452 | Time: 3.26s | LR: 0.00000131\n[Epoch 95/100] Loss: 0.000440 | Time: 3.26s | LR: 0.00000131\n[Epoch 96/100] Loss: 0.000428 | Time: 3.30s | LR: 0.00000131\n[Epoch 97/100] Loss: 0.000416 | Time: 3.45s | LR: 0.00000131\n[Epoch 98/100] Loss: 0.000405 | Time: 3.24s | LR: 0.00000131\n[Epoch 99/100] Loss: 0.000394 | Time: 3.25s | LR: 0.00000131\n[Epoch 100/100] Loss: 0.000383 | Time: 3.25s | LR: 0.00000131\n\nFinished training.\nBest epoch: 100 with loss: 0.00038313892\nGMM_Mean0  PSNR: 49.44409417347535\nGMM_Mean1  PSNR: 49.46856209842089\nGMM_Mean2  PSNR: 49.44167359772466\nGMM_Std0  PSNR: 38.426258242379646\nGMM_Std1  PSNR: 39.08092105575922\nGMM_Std2  PSNR: 38.53378159339193\nGMM_Weight0  PSNR: 39.85403153075901\nGMM_Weight1  PSNR: 39.63098681056615\nGMM_Weight2  PSNR: 39.585816594979086\n\nAverage psnr :  42.607347299717325\nPSNR: ([49.44409417347535, 49.46856209842089, 49.44167359772466, 38.426258242379646, 39.08092105575922, 38.53378159339193, 39.85403153075901, 39.63098681056615, 39.585816594979086], 42.607347299717325)\nRMSE: 0.018423807\n\n*** Results appended to 'experiment_results.csv' ***\n\nAll experiments completed. Check 'experiment_results.csv' for results.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# print(os.path.getsize('/kaggle/working/models/train_3d_data_100ep_4rb_512n_512bs_1e-05lr_Truedecay_0.8dr_decayingAtInterval15.pth') / (1024 ** 2), 'MB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T13:53:45.195293Z","iopub.execute_input":"2025-04-15T13:53:45.195961Z","iopub.status.idle":"2025-04-15T13:53:45.199930Z","shell.execute_reply.started":"2025-04-15T13:53:45.195936Z","shell.execute_reply":"2025-04-15T13:53:45.199230Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # vti saving path\n# vti_path = args.vti_path\n# if not os.path.exists(vti_path):\n#     os.makedirs(vti_path)\n# # vti name\n# vti_name = args.vti_name\n# makeVTI(data, nn_val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name)","metadata":{"_uuid":"8987b9b8-de2f-448c-995c-28bee1a033eb","_cell_guid":"6f9d6b16-aa9c-4808-81e8-a4d1e00459d2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-15T13:53:45.200512Z","iopub.execute_input":"2025-04-15T13:53:45.200713Z","iopub.status.idle":"2025-04-15T13:53:45.219652Z","shell.execute_reply.started":"2025-04-15T13:53:45.200698Z","shell.execute_reply":"2025-04-15T13:53:45.218968Z"}},"outputs":[],"execution_count":20}]}