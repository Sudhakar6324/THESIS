{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9x4zgqRjUuR",
        "outputId": "904fe1b0-5e80-4864-ff64-6a60fc26f31d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vtk\n",
            "  Downloading vtk-9.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from vtk) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->vtk) (1.17.0)\n",
            "Downloading vtk-9.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (112.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vtk\n",
            "Successfully installed vtk-9.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install vtk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-06-28T12:16:58.736373Z",
          "iopub.status.busy": "2025-06-28T12:16:58.736102Z"
        },
        "id": "ILTVZviKjIe1",
        "outputId": "375e276b-5f98-47e2-d035-5d293b0b25e6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device running: cuda\n",
            "Namespace(n_neurons=150, n_layers=6, epochs=200, batchsize=2048, lr=5e-05, no_decay=False, decay_rate=0.8, decay_at_interval=True, decay_interval=15, datapath='/content/new_gmm_isabel_week_6.vti', outpath='./models/', exp_path='../logs/', modified_data_path='./data/', dataset_name='ema', vti_name='new_gmm_isabel_ema_week_6_predict.vti', vti_path='/data/')\n",
            "\n",
            "Learning Rate: 5e-05\n",
            "Batch Size: 2048\n",
            "Decay Rate: 0.8\n",
            "Max Epochs: 200\n",
            "Number of Neurons per Layer: 150\n",
            "Number of Layers (including input/output): 8\n",
            "Data Path: /content/new_gmm_isabel_week_6.vti\n",
            "Output Path: ./models/\n",
            "Dataset Name: ema\n",
            "Vti Name: new_gmm_isabel_ema_week_6_predict.vti\n",
            "train_ema_6rb_150n_2048bs_5e-05lr_Truedecay_0.8dr_decayingAtInterval15\n",
            "Decay: True\n",
            "Extracting variables from path: /content/new_gmm_isabel_week_6.vti\n",
            "\n",
            "n_pts: 3125000 dim: (250, 250, 50) n_dim: 3 total_arr: 9\n",
            "Total Variables: 9\n",
            "Univariate: False\n",
            "Coordinates Shape: (3125000, 3)\n",
            "Values Shape: (3125000, 9)\n",
            "Dataset Name: ema\n",
            "Total Variables: 9\n",
            "Variables Name: ['GMM_Mean0', 'GMM_Mean1', 'GMM_Mean2', 'GMM_Std0', 'GMM_Std1', 'GMM_Std2', 'GMM_Weight0', 'GMM_Weight1', 'GMM_Weight2']\n",
            "\n",
            "Total Points in Data: 3125000\n",
            "Dimension of the Dataset: (250, 250, 50)\n",
            "Number of Dimensions: 3\n",
            "Coordinate Tensor Shape: torch.Size([3125000, 3])\n",
            "Scalar Values Tensor Shape: torch.Size([3125000, 9])\n",
            "\n",
            "###### Data setup is complete, now starting training ######\n",
            "\n",
            "MyResidualSirenNet(\n",
            "  (net_layers): ModuleList(\n",
            "    (0): SineLayer(\n",
            "      (linear): Linear(in_features=3, out_features=150, bias=True)\n",
            "    )\n",
            "    (1-6): 6 x ResidualSineLayer(\n",
            "      (linear_1): Linear(in_features=150, out_features=150, bias=True)\n",
            "      (linear_2): Linear(in_features=150, out_features=150, bias=True)\n",
            "    )\n",
            "    (7): Linear(in_features=150, out_features=9, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 5e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "MSELoss()\n",
            "\n",
            "Learning Rate: 5e-05\n",
            "Max Epochs: 200\n",
            "Batch Size: 2048\n",
            "Number of Hidden Layers: 6\n",
            "Number of Neurons per Layer: 150\n",
            "Decay Rate: 0.8\n",
            "Rate decays every 15 epochs.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  0%|          | 1/200 [00:57<3:09:51, 57.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/200 | Train Loss: 0.036182 | Grad Norm: 0.3411 | Time: 57.24s (cuda) | LR: 5.00e-05\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"parameter_learning_main.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1O6m5CVsAt85AmsY18hWW8Eoj_JgiLqQ8\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import vtk\n",
        "from vtk import *\n",
        "from vtk.util.numpy_support import vtk_to_numpy\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print('Device running:', device)\n",
        "\n",
        "class SineLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        # self.enable_dropout = enable_dropout\n",
        "        # self.dropout_prob = dropout_prob\n",
        "        self.in_features = in_features\n",
        "        # if enable_dropout:\n",
        "        #     if not self.is_first:\n",
        "        #         self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features,\n",
        "                                             1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        # if self.enable_dropout:\n",
        "        #     if not self.is_first:\n",
        "        #         x = self.dropout(x)\n",
        "        return torch.sin(self.omega_0 * x)\n",
        "\n",
        "class ResidualSineLayer(nn.Module):\n",
        "    def __init__(self, features, bias=True, ave_first=False, ave_second=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        # self.enable_dropout = enable_dropout\n",
        "        # self.dropout_prob = dropout_prob\n",
        "        self.features = features\n",
        "        # if enable_dropout:\n",
        "        #     self.dropout_1 = nn.Dropout(dropout_prob)\n",
        "        self.linear_1 = nn.Linear(features, features, bias=bias)\n",
        "        self.linear_2 = nn.Linear(features, features, bias=bias)\n",
        "        self.weight_1 = .5 if ave_first else 1\n",
        "        self.weight_2 = .5 if ave_second else 1\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            self.linear_1.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n",
        "                                           np.sqrt(6 / self.features) / self.omega_0)\n",
        "            self.linear_2.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n",
        "                                           np.sqrt(6 / self.features) / self.omega_0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        linear_1 = self.linear_1(self.weight_1*input)\n",
        "        # if self.enable_dropout:\n",
        "        #     linear_1 = self.dropout_1(linear_1)\n",
        "        sine_1 = torch.sin(self.omega_0 * linear_1)\n",
        "        sine_2 = torch.sin(self.omega_0 * self.linear_2(sine_1))\n",
        "        return self.weight_2*(input+sine_2)\n",
        "\n",
        "class MyResidualSirenNet(nn.Module):\n",
        "    def __init__(self, obj):\n",
        "        super(MyResidualSirenNet, self).__init__()\n",
        "        # self.enable_dropout = obj['enable_dropout']\n",
        "        # self.dropout_prob = obj['dropout_prob']\n",
        "        self.Omega_0=30\n",
        "        self.n_layers = obj['n_layers']\n",
        "        self.input_dim = obj['dim']\n",
        "        self.output_dim = obj['total_vars']\n",
        "        self.neurons_per_layer = obj['n_neurons']\n",
        "        self.layers = [self.input_dim]\n",
        "        for i in range(self.n_layers-1):\n",
        "            self.layers.append(self.neurons_per_layer)\n",
        "        self.layers.append(self.output_dim)\n",
        "        self.net_layers = nn.ModuleList()\n",
        "        for idx in np.arange(self.n_layers):\n",
        "            layer_in = self.layers[idx]\n",
        "            layer_out = self.layers[idx+1]\n",
        "            ## if not the final layer\n",
        "            if idx != self.n_layers-1:\n",
        "                ## if first layer\n",
        "                if idx==0:\n",
        "                    self.net_layers.append(SineLayer(layer_in,layer_out,bias=True,is_first=idx==0))\n",
        "                ## if an intermdeiate layer\n",
        "                else:\n",
        "                    self.net_layers.append(ResidualSineLayer(layer_in,bias=True,ave_first=idx>1,ave_second=idx==(self.n_layers-2)))\n",
        "            ## if final layer\n",
        "            else:\n",
        "                final_linear = nn.Linear(layer_in,layer_out)\n",
        "                ## initialize weights for the final layer\n",
        "                with torch.no_grad():\n",
        "                    final_linear.weight.uniform_(-np.sqrt(6 / (layer_in)) / self.Omega_0, np.sqrt(6 / (layer_in)) / self.Omega_0)\n",
        "                self.net_layers.append(final_linear)\n",
        "\n",
        "    def forward(self,x):\n",
        "        for net_layer in self.net_layers:\n",
        "            x = net_layer(x)\n",
        "        return x\n",
        "\n",
        "def size_of_network(n_layers, n_neurons, d_in, d_out, is_residual = True):\n",
        "    # Adding input layer\n",
        "    layers = [d_in]\n",
        "    # layers = [3]\n",
        "\n",
        "    # Adding hidden layers\n",
        "    layers.extend([n_neurons]*n_layers)\n",
        "    # layers = [3, 5, 5, 5]\n",
        "\n",
        "    # Adding output layer\n",
        "    layers.append(d_out)\n",
        "    # layers = [3, 5, 5, 5, 1]\n",
        "\n",
        "    # Number of steps\n",
        "    n_layers = len(layers)-1\n",
        "    # n_layers = 5 - 1 = 4\n",
        "\n",
        "    n_params = 0\n",
        "\n",
        "    # np.arange(4) = [0, 1, 2, 3]\n",
        "    for ndx in np.arange(n_layers):\n",
        "\n",
        "        # number of neurons in below layer\n",
        "        layer_in = layers[ndx]\n",
        "\n",
        "        # number of neurons in above layer\n",
        "        layer_out = layers[ndx+1]\n",
        "\n",
        "        # max number of neurons in both the layer\n",
        "        og_layer_in = max(layer_in,layer_out)\n",
        "\n",
        "        # if lower layer is the input layer\n",
        "        # or the upper layer is the output layer\n",
        "        if ndx==0 or ndx==(n_layers-1):\n",
        "            # Adding weight corresponding to every neuron for every input neuron\n",
        "            # Adding bias for every neuron in the upper layer\n",
        "            n_params += ((layer_in+1)*layer_out)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # If the layer is residual then proceed as follows as there will be more weights if residual layer is included\n",
        "            if is_residual:\n",
        "                # doubt in the following two lines\n",
        "                n_params += (layer_in*og_layer_in)+og_layer_in\n",
        "                n_params += (og_layer_in*layer_out)+layer_out\n",
        "\n",
        "            # if the layer is non residual then simply add number of weights and biases as follows\n",
        "            else:\n",
        "                n_params += ((layer_in+1)*layer_out)\n",
        "            #\n",
        "        #\n",
        "    #\n",
        "\n",
        "    return n_params\n",
        "\n",
        "def compute_PSNR(arrgt,arr_recon):\n",
        "    diff = arrgt - arr_recon\n",
        "    sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n",
        "    snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n",
        "    return snr\n",
        "\n",
        "def srs(numOfPoints, valid_pts, percentage, isMaskPresent, mask_array):\n",
        "\n",
        "    # getting total number of sampled points\n",
        "    numberOfSampledPoints = int((valid_pts/100) * percentage)\n",
        "\n",
        "    # storing corner indices in indices variable\n",
        "    indices = set()\n",
        "\n",
        "    # As long as we don't get the required amount of sample points keep finding the random numbers\n",
        "    while(len(indices) < numberOfSampledPoints):\n",
        "        rp = random.randint(0, numOfPoints-1)\n",
        "        if isMaskPresent and mask_array[rp] == 0:\n",
        "            continue\n",
        "        indices.add(rp)\n",
        "\n",
        "    # return indices\n",
        "    return indices\n",
        "\n",
        "def findMultiVariatePSNR(var_name, total_vars, actual, pred):\n",
        "    # print('Printing PSNR')\n",
        "    tot = 0\n",
        "    psnr_list = []\n",
        "    for j in range(total_vars):\n",
        "        psnr = compute_PSNR(actual[:,j], pred[:,j])\n",
        "        psnr_list.append(psnr)\n",
        "        tot += psnr\n",
        "        print(var_name[j], ' PSNR:', psnr)\n",
        "    avg_psnr = tot/total_vars\n",
        "    print('\\nAverage psnr : ', avg_psnr)\n",
        "     #this function is calculating the psnr of final epoch (or whenever it is called) of each variable and then averaging it\n",
        "     #Thus individual epochs psnr is not calculated\n",
        "\n",
        "    return psnr_list, avg_psnr\n",
        "\n",
        "def compute_rmse(actual, predicted):\n",
        "    mse = np.mean((actual - predicted) ** 2)\n",
        "    return np.sqrt(mse)\n",
        "\n",
        "def denormalizeValue(total_vars, to, ref):\n",
        "    to_arr = np.array(to)\n",
        "    for i in range(total_vars):\n",
        "        min_data = np.min(ref[:, i])\n",
        "        max_data = np.max(ref[:, i])\n",
        "        to_arr[:, i] = (((to[:, i] * 0.5) + 0.5) * (max_data - min_data)) + min_data\n",
        "    return to_arr\n",
        "\n",
        "def makeVTI(data, val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name, normalizedVersion = False):\n",
        "    nn_predictions = denormalizeValue(total_vars, n_predictions, val) if not normalizedVersion else n_predictions\n",
        "    writer = vtkXMLImageDataWriter()\n",
        "    writer.SetFileName(vti_path + vti_name)\n",
        "    img = vtkImageData()\n",
        "    img.CopyStructure(data)\n",
        "    if not isMaskPresent:\n",
        "        for i in range(total_vars):\n",
        "            f = var_name[i]\n",
        "            temp = nn_predictions[:, i]\n",
        "            arr = vtkFloatArray()\n",
        "            for j in range(n_pts):\n",
        "                arr.InsertNextValue(temp[j])\n",
        "            arr.SetName(f)\n",
        "            img.GetPointData().AddArray(arr)\n",
        "        # print(img)\n",
        "        writer.SetInputData(img)\n",
        "        writer.Write()\n",
        "        print(f'Vti File written successfully at {vti_path}{vti_name}')\n",
        "    else:\n",
        "        for i in range(total_vars):\n",
        "            f = var_name[i]\n",
        "            temp = nn_predictions[:, i]\n",
        "            idx = 0\n",
        "            arr = vtkFloatArray()\n",
        "            for j in range(n_pts):\n",
        "                if(mask_arr[j] == 1):\n",
        "                    arr.InsertNextValue(temp[idx])\n",
        "                    idx += 1\n",
        "                else:\n",
        "                    arr.InsertNextValue(0.0)\n",
        "            arr.SetName('p_' + f)\n",
        "            data.GetPointData().AddArray(arr)\n",
        "        # print(data)\n",
        "        writer.SetInputData(data)\n",
        "        writer.Write()\n",
        "        print(f'Vti File written successfully at {vti_path}{vti_name}')\n",
        "\n",
        "def getImageData(actual_img, val, n_pts, var_name, isMaskPresent, mask_arr):\n",
        "    img = vtkImageData()\n",
        "    img.CopyStructure(actual_img)\n",
        "    # if isMaskPresent:\n",
        "    #     img.DeepCopy(actual_img)\n",
        "    # img.SetDimensions(dim)\n",
        "    # img.SetOrigin(actual_img.GetOrigin())\n",
        "    # img.SetSpacing(actual_img.GetSpacing())\n",
        "    if not isMaskPresent:\n",
        "        f = var_name\n",
        "        data = val\n",
        "        arr = vtkFloatArray()\n",
        "        for j in range(n_pts):\n",
        "            arr.InsertNextValue(data[j])\n",
        "        arr.SetName(f)\n",
        "        img.GetPointData().SetScalars(arr)\n",
        "    else:\n",
        "        f = var_name\n",
        "        data = val\n",
        "        idx = 0\n",
        "        arr = vtkFloatArray()\n",
        "        for j in range(n_pts):\n",
        "            if(mask_arr[j] == 1):\n",
        "                arr.InsertNextValue(data[idx])\n",
        "                idx += 1\n",
        "            else:\n",
        "                arr.InsertNextValue(0.0)\n",
        "        arr.SetName(f)\n",
        "        img.GetPointData().SetScalars(arr)\n",
        "    return img\n",
        "\n",
        "from argparse import Namespace\n",
        "\n",
        "# Parameters (simulating argparse in a Jupyter Notebook)\n",
        "args = Namespace(\n",
        "    n_neurons=150,\n",
        "    n_layers=6,\n",
        "    epochs=200,  # Required argument: Set the number of epochs\n",
        "    batchsize=2048,\n",
        "    lr=0.00005,\n",
        "    no_decay=False,\n",
        "    decay_rate=0.8,\n",
        "    decay_at_interval=True,\n",
        "    decay_interval=15,\n",
        "    datapath='/content/new_gmm_isabel_week_6.vti',  # Required: Set the path to your data\n",
        "    outpath='./models/',\n",
        "    exp_path='../logs/',\n",
        "    modified_data_path='./data/',\n",
        "    dataset_name='ema',  # Required: Set the dataset name\n",
        "    vti_name='new_gmm_isabel_ema_week_6_predict.vti',  # Required: Name of the dataset\n",
        "    vti_path='/data/'\n",
        ")\n",
        "\n",
        "print(args, end='\\n\\n')\n",
        "\n",
        "# Assigning parameters to variables\n",
        "LR = args.lr\n",
        "BATCH_SIZE = args.batchsize\n",
        "decay_rate = args.decay_rate\n",
        "decay_at_equal_interval = args.decay_at_interval\n",
        "\n",
        "decay = not args.no_decay\n",
        "MAX_EPOCH = args.epochs\n",
        "\n",
        "n_neurons = args.n_neurons\n",
        "n_layers = args.n_layers + 2\n",
        "decay_interval = args.decay_interval\n",
        "outpath = args.outpath\n",
        "exp_path = args.exp_path\n",
        "datapath = args.datapath\n",
        "modified_data_path = args.modified_data_path\n",
        "dataset_name = args.dataset_name\n",
        "vti_name = args.vti_name\n",
        "vti_path = args.vti_path\n",
        "\n",
        "# Displaying the final configuration\n",
        "print(f\"Learning Rate: {LR}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Decay Rate: {decay_rate}\")\n",
        "print(f\"Max Epochs: {MAX_EPOCH}\")\n",
        "print(f\"Number of Neurons per Layer: {n_neurons}\")\n",
        "print(f\"Number of Layers (including input/output): {n_layers}\")\n",
        "print(f\"Data Path: {datapath}\")\n",
        "print(f\"Output Path: {outpath}\")\n",
        "print(f\"Dataset Name: {dataset_name}\")\n",
        "print(f\"Vti Name: {vti_name}\")\n",
        "\n",
        "# Variable Initialization\n",
        "var_name = []\n",
        "total_vars = None  # Number of variables\n",
        "univariate = None  # True if dataset has one variable, else False\n",
        "group_size = 5000  # Group size during testing\n",
        "\n",
        "\n",
        "# Constructing the log file name\n",
        "log_file = (\n",
        "    f'train_{dataset_name}_{n_layers-2}rb_{n_neurons}n_{BATCH_SIZE}bs_'\n",
        "    f'{LR}lr_{decay}decay_{decay_rate}dr_'\n",
        "    f'{\"decayingAtInterval\" + str(decay_interval) if decay_at_equal_interval else \"decayingWhenLossIncr\"}'\n",
        ")\n",
        "\n",
        "print(log_file)\n",
        "\n",
        "n_pts = None  # Number of points in the dataset\n",
        "n_dim = None  # Dimensionality of the data\n",
        "dim = None  # Other dimension-specific information\n",
        "\n",
        "print(\"Decay:\", decay)\n",
        "print(f'Extracting variables from path: {datapath}', end=\"\\n\\n\")\n",
        "\n",
        "# Placeholder for data\n",
        "data_array = []\n",
        "scalar_data = None\n",
        "\n",
        "# # Reading values from .vti files\n",
        "# reader = vtk.vtkXMLImageDataReader()\n",
        "# reader.SetFileName(datapath)\n",
        "# reader.Update()\n",
        "\n",
        "# data = reader.GetOutput()\n",
        "# scalar_data = data\n",
        "# pdata = data.GetPointData()\n",
        "# n_pts = data.GetNumberOfPoints()\n",
        "# dim = data.GetDimensions()\n",
        "# n_dim = len(dim)\n",
        "# total_arr = pdata.GetNumberOfArrays()\n",
        "\n",
        "# print(\"n_pts:\", n_pts, \"dim:\", dim, \"n_dim:\", n_dim, \"total_arr:\", total_arr)\n",
        "\n",
        "# mask_arr = []\n",
        "# valid_pts = 0\n",
        "# var_name = []\n",
        "# data_array = []\n",
        "\n",
        "# # Extracting data from the .vti file\n",
        "# for i in range(total_arr):\n",
        "#     a_name = pdata.GetArrayName(i)\n",
        "#     if a_name in ['vtkValidPointMask', 'Swirl']:\n",
        "#         continue\n",
        "\n",
        "#     cur_arr = pdata.GetArray(a_name)\n",
        "#     n_components = cur_arr.GetNumberOfComponents()\n",
        "\n",
        "#     if n_components == 1:\n",
        "#         var_name.append(a_name)\n",
        "#         data_array.append(vtk_to_numpy(cur_arr))\n",
        "#     else:\n",
        "#         component_names = [f\"{a_name}_{c}\" for c in ['x', 'y', 'z'][:n_components]]\n",
        "#         var_name.extend(component_names)\n",
        "#         for c in range(n_components):\n",
        "#             c_data = [cur_arr.GetComponent(j, c) for j in range(n_pts)]\n",
        "#             data_array.append(np.array(c_data))\n",
        "\n",
        "# valid_pts = n_pts  # Assume all points are valid for simplicity\n",
        "# total_vars = len(var_name)\n",
        "# univariate = total_vars == 1\n",
        "\n",
        "# # Prepare numpy arrays for coordinates and variable values\n",
        "# cord = np.zeros((valid_pts, n_dim))\n",
        "# val = np.zeros((valid_pts, total_vars))\n",
        "\n",
        "# # Store data in numpy arrays\n",
        "# for i in range(n_pts):\n",
        "#     pt = scalar_data.GetPoint(i)\n",
        "#     cord[i, :] = pt\n",
        "#     val[i, :] = [arr[i] for arr in data_array]\n",
        "\n",
        "# # Display final information\n",
        "# print(\"Total Variables:\", total_vars)\n",
        "# print(\"Univariate:\", univariate)\n",
        "# print(\"Coordinates Shape:\", cord.shape)\n",
        "# print(\"Values Shape:\", val.shape)\n",
        "\n",
        "# Reading values from .vti files\n",
        "reader = vtk.vtkXMLImageDataReader()\n",
        "reader.SetFileName(datapath)\n",
        "reader.Update()\n",
        "\n",
        "data = reader.GetOutput()\n",
        "scalar_data = data\n",
        "pdata = data.GetPointData()\n",
        "n_pts = data.GetNumberOfPoints()\n",
        "dim = data.GetDimensions()\n",
        "n_dim = len(dim)\n",
        "total_arr = pdata.GetNumberOfArrays()\n",
        "\n",
        "print(\"n_pts:\", n_pts, \"dim:\", dim, \"n_dim:\", n_dim, \"total_arr:\", total_arr)\n",
        "\n",
        "var_name = []\n",
        "data_array = []\n",
        "\n",
        "# Extracting data from the .vti file\n",
        "for i in range(total_arr):\n",
        "    a_name = pdata.GetArrayName(i)\n",
        "\n",
        "    cur_arr = pdata.GetArray(a_name)\n",
        "    n_components = cur_arr.GetNumberOfComponents()\n",
        "\n",
        "    if n_components == 1:\n",
        "        var_name.append(a_name)\n",
        "        data_array.append(vtk_to_numpy(cur_arr))\n",
        "    else:\n",
        "        component_names = [f\"{a_name}_{c}\" for c in ['x', 'y', 'z'][:n_components]]\n",
        "        var_name.extend(component_names)\n",
        "        for c in range(n_components):\n",
        "            c_data = [cur_arr.GetComponent(j, c) for j in range(n_pts)]\n",
        "            data_array.append(np.array(c_data))\n",
        "\n",
        "total_vars = len(var_name)\n",
        "univariate = total_vars == 1\n",
        "\n",
        "# Prepare numpy arrays for coordinates and variable values\n",
        "cord = np.zeros((n_pts, n_dim))\n",
        "val = np.zeros((n_pts, total_vars))\n",
        "\n",
        "# Store data in numpy arrays\n",
        "for i in range(n_pts):\n",
        "    pt = scalar_data.GetPoint(i)\n",
        "    cord[i, :] = pt\n",
        "    val[i, :] = [arr[i] for arr in data_array]\n",
        "\n",
        "# Display final information\n",
        "print(\"Total Variables:\", total_vars)\n",
        "print(\"Univariate:\", univariate)\n",
        "print(\"Coordinates Shape:\", cord.shape)\n",
        "print(\"Values Shape:\", val.shape)\n",
        "\n",
        "# # Ensure modified data path exists\n",
        "# if not os.path.exists(modified_data_path):\n",
        "#     os.mkdir(modified_data_path)\n",
        "\n",
        "# Save raw coordinates and values\n",
        "# np.save(f'{modified_data_path}cord.npy', cord)\n",
        "# np.save(f'{modified_data_path}val.npy', val)\n",
        "\n",
        "# # Create copies of non-normalized data\n",
        "# nn_cord = cord.copy()\n",
        "# nn_val = val.copy()\n",
        "\n",
        "# === Separate Normalization for Values ===\n",
        "# We assume the variable order is:\n",
        "#   - Means: indices 0,1,2\n",
        "#   - Std Devs: indices 3,4,5\n",
        "#   - Weights: indices 6,7,8\n",
        "\n",
        "# # We'll store normalization parameters so that we can invert normalization later.\n",
        "# norm_params = {}\n",
        "# epsilon = 1e-8  # to avoid log(0)\n",
        "\n",
        "# # Normalize Means to [-1,1] using min–max normalization\n",
        "# for i in range(3):\n",
        "#     min_val = np.min(val[:, i])\n",
        "#     max_val = np.max(val[:, i])\n",
        "#     norm_params[var_name[i]] = (min_val, max_val)\n",
        "#     val[:, i] = 2.0 * ((val[:, i] - min_val) / (max_val - min_val) - 0.5)\n",
        "\n",
        "# # Normalize Std Devs: first take log, then min–max to [-1,1]\n",
        "# for i in range(3, 6):\n",
        "#     log_vals = np.log(val[:, i] + epsilon)\n",
        "#     min_val = np.min(log_vals)\n",
        "#     max_val = np.max(log_vals)\n",
        "#     norm_params[var_name[i]] = (min_val, max_val)\n",
        "#     val[:, i] = 2.0 * ((log_vals - min_val) / (max_val - min_val) - 0.5)\n",
        "\n",
        "# # Normalize Weights: take log, then min–max to [-1,1]\n",
        "# for i in range(6, 9):\n",
        "#     log_vals = np.log(val[:, i] + epsilon)\n",
        "#     min_val = np.min(log_vals)\n",
        "#     max_val = np.max(log_vals)\n",
        "#     norm_params[var_name[i]] = (min_val, max_val)\n",
        "#     val[:, i] = 2.0 * ((log_vals - min_val) / (max_val - min_val) - 0.5)\n",
        "\n",
        "# norm_params = {}\n",
        "real_data=val.copy()\n",
        "# Normalize values between -1 and 1\n",
        "for i in range(total_vars):\n",
        "    min_data = np.min(val[:, i])\n",
        "    max_data = np.max(val[:, i])\n",
        "    # norm_params[var_name[i]] = (min_data, max_data)\n",
        "    val[:, i] = 2.0 * ((val[:, i] - min_data) / (max_data - min_data) - 0.5)\n",
        "\n",
        "# Normalize Coordinates to [-1,1]\n",
        "for i in range(n_dim):\n",
        "    # Use (dim[i]-1] so that coordinates go from 0 to dim[i]-1.\n",
        "    cord[:, i] = 2.0 * (cord[:, i] / (dim[i] - 1) - 0.5)\n",
        "\n",
        "# # Normalize coordinates between 0 and 1\n",
        "# for i in range(n_dim):\n",
        "#     cord[:, i] = cord[:, i] / dim[i]\n",
        "\n",
        "\n",
        "# # Save normalized values and coordinates\n",
        "# np.save(f'{modified_data_path}n_cord.npy', cord)\n",
        "# np.save(f'{modified_data_path}n_val.npy', val)\n",
        "n_cord = cord.copy()\n",
        "n_val = val.copy()\n",
        "\n",
        "# # Reload data for verification\n",
        "# n_cord = np.load(f'{modified_data_path}n_cord.npy')\n",
        "# n_val = np.load(f'{modified_data_path}n_val.npy')\n",
        "# cord = np.load(f'{modified_data_path}cord.npy')\n",
        "# val = np.load(f'{modified_data_path}val.npy')\n",
        "\n",
        "# Convert normalized data to PyTorch tensors\n",
        "torch_coords = torch.from_numpy(n_cord)\n",
        "torch_vals = torch.from_numpy(n_val)\n",
        "\n",
        "# Display dataset details\n",
        "print('Dataset Name:', dataset_name)\n",
        "print('Total Variables:', total_vars)\n",
        "print('Variables Name:', var_name, end=\"\\n\\n\")\n",
        "print('Total Points in Data:', n_pts)\n",
        "print('Dimension of the Dataset:', dim)\n",
        "print('Number of Dimensions:', n_dim)\n",
        "print('Coordinate Tensor Shape:', torch_coords.shape)\n",
        "print('Scalar Values Tensor Shape:', torch_vals.shape)\n",
        "\n",
        "print('\\n###### Data setup is complete, now starting training ######\\n')\n",
        "\n",
        "# Prepare the DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "    TensorDataset(torch_coords, torch_vals),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Model configuration\n",
        "obj = {\n",
        "    'total_vars': total_vars,\n",
        "    'dim': n_dim,\n",
        "    'n_neurons': n_neurons,\n",
        "    'n_layers': n_layers\n",
        "}\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = MyResidualSirenNet(obj).to(device)\n",
        "print(model)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999))\n",
        "print(optimizer)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "print(criterion)\n",
        "\n",
        "# Training configuration summary\n",
        "print('\\nLearning Rate:', LR)\n",
        "print('Max Epochs:', MAX_EPOCH)\n",
        "print('Batch Size:', BATCH_SIZE)\n",
        "print('Number of Hidden Layers:', obj['n_layers'] - 2)\n",
        "print('Number of Neurons per Layer:', obj['n_neurons'])\n",
        "\n",
        "if decay:\n",
        "    print('Decay Rate:', decay_rate)\n",
        "    if decay_at_equal_interval:\n",
        "        print(f'Rate decays every {decay_interval} epochs.')\n",
        "    else:\n",
        "        print('Rate decays when the current epoch loss is greater than the previous epoch loss.')\n",
        "else:\n",
        "    print('No decay!')\n",
        "print()\n",
        "\n",
        "train_loss_list = []\n",
        "best_epoch = -1\n",
        "best_loss = 1e8\n",
        "from tqdm import tqdm\n",
        "# Ensure the output path exists\n",
        "if not os.path.exists(outpath):\n",
        "    os.makedirs(outpath)\n",
        "\n",
        "# Training loop\n",
        "\n",
        "# --- Setup for Loss EMA and Helpers ---\n",
        "ema_decay = 0.99\n",
        "grad_ema = {\n",
        "    \"mean\": 1.0,\n",
        "    \"std\": 1.0,\n",
        "    \"weight\": 1.0\n",
        "}\n",
        "\n",
        "def compute_grad_norm(loss):\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward(retain_graph=True)\n",
        "    total = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            total += p.grad.detach().data.norm(2).item() ** 2\n",
        "    return (total ** 0.5)\n",
        "\n",
        "# --- Main Training Loop ---\n",
        "train_loss_list = []\n",
        "grad_norm_list = []\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in tqdm(range(MAX_EPOCH)):\n",
        "    model.train()\n",
        "    temp_loss_list = []\n",
        "    start = time.time()\n",
        "\n",
        "    for X_train, y_train in train_dataloader:\n",
        "        X_train = X_train.float().to(device)\n",
        "        y_train = y_train.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(X_train)  # shape: [batch, 9]\n",
        "        predictions = predictions.squeeze()\n",
        "\n",
        "        # Split losses\n",
        "        # pred_mean, pred_std, pred_weight = predictions[:, :3], predictions[:, 3:6], predictions[:, 6:]\n",
        "        # gt_mean, gt_std, gt_weight = y_train[:, :3], y_train[:, 3:6], y_train[:, 6:]\n",
        "        pred_mean=predictions[:,0]\n",
        "        pred_std=predictions[:,1]\n",
        "        gt_mean=y_train[:,0]\n",
        "        gt_std=y_train[:,1]\n",
        "        # Compute MSE losses for each group\n",
        "        loss_mean = F.mse_loss(pred_mean, gt_mean)\n",
        "        loss_std = F.mse_loss(pred_std, gt_std)\n",
        "        #loss_weight = F.mse_loss(pred_weight, gt_weight)\n",
        "\n",
        "        # --- Gradient Norms for each ---\n",
        "        g_mean = compute_grad_norm(loss_mean)\n",
        "        g_std = compute_grad_norm(loss_std)\n",
        "        #g_weight = compute_grad_norm(loss_weight)\n",
        "\n",
        "        # --- EMA Update ---\n",
        "        grad_ema[\"mean\"] = ema_decay * grad_ema[\"mean\"] + (1 - ema_decay) * g_mean\n",
        "        grad_ema[\"std\"] = ema_decay * grad_ema[\"std\"] + (1 - ema_decay) * g_std\n",
        "        #grad_ema[\"weight\"] = ema_decay * grad_ema[\"weight\"] + (1 - ema_decay) * g_weight\n",
        "\n",
        "        #total_ema = grad_ema[\"mean\"] + grad_ema[\"std\"] + grad_ema[\"weight\"]\n",
        "        total_ema=grad_ema[\"mean\"] + grad_ema[\"std\"]\n",
        "        # --- Adaptive Weights ---\n",
        "        w_mean = total_ema / (grad_ema[\"mean\"])\n",
        "        w_std = total_ema / (grad_ema[\"std\"])\n",
        "        #w_weight = total_ema / (3.0 * grad_ema[\"weight\"])\n",
        "\n",
        "        # --- Final Combined Loss ---\n",
        "        #final_loss = w_mean * loss_mean + w_std * loss_std + w_weight * loss_weight\n",
        "        final_loss = w_mean * loss_mean + w_std * loss_std\n",
        "        final_loss.backward()\n",
        "\n",
        "        # Gradient norm for logging\n",
        "        total_grad_norm = 0.0\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None:\n",
        "                total_grad_norm += param.grad.data.norm(2).item() ** 2\n",
        "        total_grad_norm = total_grad_norm ** 0.5\n",
        "\n",
        "        optimizer.step()\n",
        "        temp_loss_list.append(final_loss.detach().cpu().item())\n",
        "\n",
        "    # Epoch-wise processing\n",
        "    epoch_loss = np.mean(temp_loss_list)\n",
        "\n",
        "    # Learning rate decay\n",
        "    if decay:\n",
        "        if decay_at_equal_interval:\n",
        "            if epoch >= decay_interval and epoch % decay_interval == 0:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= decay_rate\n",
        "        elif epoch > 0 and epoch_loss > train_loss_list[-1]:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_rate\n",
        "\n",
        "    train_loss_list.append(epoch_loss)\n",
        "    grad_norm_list.append(total_grad_norm)\n",
        "\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        best_epoch = epoch + 1\n",
        "\n",
        "    end = time.time()\n",
        "    print(\n",
        "        f\"Epoch: {epoch + 1}/{MAX_EPOCH} | Train Loss: {epoch_loss:.6f} | \"\n",
        "        f\"Grad Norm: {round(total_grad_norm, 4)} | Time: {round(end - start, 2)}s ({device}) | \"\n",
        "        f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
        "    )\n",
        "\n",
        "    # Save model checkpoint\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        model_name = (\n",
        "            f'train_{dataset_name}_{epoch + 1}ep_{n_layers - 2}rb_{n_neurons}n_'\n",
        "            f'{BATCH_SIZE}bs_{LR}lr_{decay}decay_{decay_rate}dr_'\n",
        "            f'{\"interval\" if decay_at_equal_interval else \"lossIncr\"}'\n",
        "        )\n",
        "        torch.save(\n",
        "            {\"epoch\": epoch + 1, \"model_state_dict\": model.state_dict()},\n",
        "            os.path.join(outpath, f'{model_name}.pth')\n",
        "        )\n",
        "\n",
        "# Final model save\n",
        "model_name = f'siren_compressor_ema'\n",
        "torch.save(\n",
        "    {\"epoch\": MAX_EPOCH, \"model_state_dict\": model.state_dict()},\n",
        "    os.path.join(outpath, f'{model_name}.pth')\n",
        ")\n",
        "\n",
        "# --- Plot Loss and Gradient Norm ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss_list, label='Training Loss', color='blue')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training Loss over Epochs')\n",
        "plt.grid(True); plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(grad_norm_list, label='Gradient Norm (L2)', color='orange')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Gradient Norm'); plt.title('Gradient Norm over Epochs')\n",
        "plt.grid(True); plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Initialize prediction lists (optional continuation)\n",
        "prediction_list = [[] for _ in range(total_vars)]\n",
        "\n",
        "\n",
        "# Inference loop\n",
        "with torch.no_grad():\n",
        "    for i in range(0, torch_coords.shape[0], group_size):\n",
        "        coords = torch_coords[i:min(i + group_size, torch_coords.shape[0])].type(torch.float32).to(device)\n",
        "        vals = model(coords)\n",
        "        vals = vals.to('cpu')\n",
        "\n",
        "        for j in range(total_vars):\n",
        "            prediction_list[j].append(vals[:, j])\n",
        "\n",
        "# Extract and concatenate predictions\n",
        "extracted_list = [[] for _ in range(total_vars)]\n",
        "for i in range(len(prediction_list[0])):\n",
        "    for j in range(total_vars):\n",
        "        el = prediction_list[j][i].detach().numpy()\n",
        "        extracted_list[j].append(el)\n",
        "\n",
        "for j in range(total_vars):\n",
        "    extracted_list[j] = np.concatenate(extracted_list[j], dtype='float32')\n",
        "\n",
        "# Final prediction (normalized)\n",
        "n_predictions = np.array(extracted_list).T\n",
        "\n",
        "# Compute PSNR\n",
        "findMultiVariatePSNR(var_name, total_vars, n_val, n_predictions)\n",
        "\n",
        "# Compute RMSE\n",
        "print(\"file name:\",datapath)\n",
        "rmse = compute_rmse(n_val, n_predictions)\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# Initialize prediction lists\n",
        "prediction_list = [[] for _ in range(total_vars)]\n",
        "\n",
        "# Inference loop\n",
        "with torch.no_grad():\n",
        "    for i in range(0, torch_coords.shape[0], group_size):\n",
        "        coords = torch_coords[i:min(i + group_size, torch_coords.shape[0])].type(torch.float32).to(device)\n",
        "        vals = model(coords)\n",
        "        vals = vals.to('cpu')\n",
        "\n",
        "        for j in range(total_vars):\n",
        "            prediction_list[j].append(vals[:, j])\n",
        "\n",
        "# Extract and concatenate predictions\n",
        "extracted_list = [[] for _ in range(total_vars)]\n",
        "for i in range(len(prediction_list[0])):\n",
        "    for j in range(total_vars):\n",
        "        el = prediction_list[j][i].detach().numpy()\n",
        "        extracted_list[j].append(el)\n",
        "\n",
        "for j in range(total_vars):\n",
        "    extracted_list[j] = np.concatenate(extracted_list[j], dtype='float32')\n",
        "\n",
        "# Final prediction (normalized)\n",
        "n_predictions = np.array(extracted_list).T\n",
        "\n",
        "# Compute PSNR\n",
        "findMultiVariatePSNR(var_name, total_vars, n_val, n_predictions)\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = compute_rmse(n_val, n_predictions)\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "# !rm -rf /kaggle/working/*\n",
        "\n",
        "#print(os.path.getsize('/kaggle/working/models/train_3d_data_200ep_6rb_320n_512bs_5e-05lr_Truedecay_0.8dr_decayingAtInterval15.pth') / (1024 ** 2), 'MB')\n",
        "\n",
        "# # vti saving path\n",
        "vti_path = args.vti_path\n",
        "if not os.path.exists(vti_path):\n",
        "       os.makedirs(vti_path)\n",
        "## vti name\n",
        "vti_name = args.vti_name\n",
        "mask_arr=[]\n",
        "isMaskPresent=False\n",
        "makeVTI(data,real_data, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDhCwRahjIe3",
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7759028,
          "sourceId": 12309752,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
