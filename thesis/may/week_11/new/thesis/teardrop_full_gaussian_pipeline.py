# -*- coding: utf-8 -*-
"""full_gaussian_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4EH_HArbFC2NZEcnujixgjWEHW59shT
"""


"""
This script generates a Gaussian distribution for each grid point in a VTI dataset.
The mean is set as the grid point's own value, and the standard deviation is computed
based on the values of its neighboring points.

Usage:
1. Set the correct file path to the input VTI data file.
2. Run the script using the command:
   python3 distribution_generation.py
"""

import numpy as np
import torch
import sys
from torch import nn, optim
import torch.optim as optim
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import TensorDataset, DataLoader
import seaborn as sns
import matplotlib.pyplot as plt
import vtk
from vtk import *
from vtk.util.numpy_support import vtk_to_numpy,numpy_to_vtk
import random
import os
import sys
import time
from tqdm import tqdm
from itertools import product
import multiprocessing
from sklearn.mixture import GaussianMixture
from joblib import Parallel, delayed
from tqdm import tqdm
import glob

"""#gaussian genration"""

from argparse import Namespace

# Parameters (simulating argparse in a Jupyter Notebook)
args = Namespace(
    n_neurons=150,
    n_layers=6,
    epochs=400,  # Required argument: Set the number of epochs
    batchsize=4096,
    lr=0.00005,
    no_decay=False,
    decay_rate=0.8,
    decay_at_interval=True,
    decay_interval=15,
    datapath='GT_teardrop_128x128x128.vti',  # Required: Set the path to your data
    outpath='./models/',
    exp_path='../logs/',
    modified_data_path='./data/',
    dataset_name='3d_data',  # Required: Set the dataset name
    vti_name='predicted.vti',  # Required: Name of the dataset
    vti_path='./data/',
    iso_value=159.9798,
    file_name="teardrop"
)


print(args, end='\n\n',flush=True)

# Assigning parameters to variables
LR = args.lr
BATCH_SIZE = args.batchsize
decay_rate = args.decay_rate
decay_at_equal_interval = args.decay_at_interval
file_name=args.file_name
decay = not args.no_decay
MAX_EPOCH = args.epochs

n_neurons = args.n_neurons
n_layers = args.n_layers + 2
decay_interval = args.decay_interval
outpath = args.outpath
exp_path = args.exp_path
datapath = args.datapath
modified_data_path = args.modified_data_path
dataset_name = args.dataset_name
vti_name = args.vti_name
vti_path = args.vti_path
iso_value= args.iso_value
# Displaying the final configuration
print(f"Learning Rate: {LR}",flush=True)
print(f"Batch Size: {BATCH_SIZE}",flush=True)
print(f"Decay Rate: {decay_rate}",flush=True)
print(f"Max Epochs: {MAX_EPOCH}",flush=True)
print(f"Number of Neurons per Layer: {n_neurons}",flush=True)
print(f"Number of Layers (including input/output): {n_layers}",flush=True)
print(f"Data Path: {datapath}",flush=True)
print(f"Output Path: {outpath}",flush=True)
print(f"Dataset Name: {dataset_name}",flush=True)
print(f"Vti Name: {vti_name}",flush=True)
os.makedirs("data",exist_ok=True)
os.makedirs("models",exist_ok=True)
# load the data
input_file=datapath
reader = vtk.vtkXMLImageDataReader()
reader.SetFileName(input_file)
reader.Update()
data = reader.GetOutput()
dim = data.GetDimensions()
print(dim)

spacing = data.GetSpacing()
origin = data.GetOrigin()
print(spacing)
print(origin)

# getting the data
Number_of_array = data.GetPointData().GetNumberOfArrays()
print("no of arrays", Number_of_array)
total_data = []
array_names = []
for i in range(Number_of_array):
    curr_arr = data.GetPointData().GetArray(i)
    arr = vtk_to_numpy(curr_arr)
    array_names.append(data.GetPointData().GetArrayName(i))

print(array_names)

x = []
for i in range(dim[0]):
    x.append(i)

y = []
for j in range(dim[1]):
    y.append(j)

z = []
for k in range(dim[2]):
    z.append(k)


def compute_mean_std(arr, x, y, z, mode):
    shape = arr.shape
    mean = []
    std = []
    global_std = np.std(arr)
    print(global_std)

    def in_bounds(z_, y_, x_):
        return 0 <= z_ < shape[0] and 0 <= y_ < shape[1] and 0 <= x_ < shape[2]

    def get_neighbours(z, y, x):
        n = []
        max_radius = mode // 6
        remainder = mode % 6

        for i in range(1, max_radius + 1):
            candidates = [
                (z, y, x - i),
                (z, y, x + i),
                (z, y - i, x),
                (z, y + i, x),
                (z - i, y, x),
                (z + i, y, x)
            ]
            for cz, cy, cx in candidates:
                if in_bounds(cz, cy, cx):
                    n.append(arr[cz, cy, cx])

        if remainder > 0:
            candidates = [
                (z, y, x - (max_radius + 1)),
                (z, y, x + (max_radius + 1)),
                (z, y - (max_radius + 1), x),
                (z, y + (max_radius + 1), x),
                (z - (max_radius + 1), y, x),
                (z + (max_radius + 1), y, x)
            ]
            for cz, cy, cx in candidates:
                if remainder == 0:
                    break
                if in_bounds(cz, cy, cx):
                    n.append(arr[cz, cy, cx])
                    remainder -= 1
        return n

    for kz in z:
        for jy in y:
            for ix in x:
                neighbours = get_neighbours(kz, jy, ix)
                mean.append(arr[kz, jy, ix])
                if len(neighbours) > 0:
                   t=np.std(np.array(neighbours))
                   if(t==0):
                     t=1e-5
                   std.append(t)
                else:
                   std.append(1e-5)
    return np.array(mean), np.array(std)

# creating new image_data
new_image_data = vtk.vtkImageData()
new_image_data.DeepCopy(data)
point_data = new_image_data.GetPointData()

if(torch.cuda.is_available()):
    device = "cuda"
else:
    device = "cpu"
print(device)
print("1.currently generating gaussian")
for name in array_names:
    curr_arr = data.GetPointData().GetArray(name)
    arr = vtk_to_numpy(curr_arr).reshape(dim[::-1])  # (z, y, x)

    mean, std = compute_mean_std(arr, np.array(x), np.array(y), np.array(z), mode=12)

    mean_vtk = numpy_to_vtk(num_array=mean, deep=True)
    std_vtk = numpy_to_vtk(num_array=std, deep=True)
    mean_vtk.SetName("Mean")
    std_vtk.SetName("Std")
    point_data.AddArray(mean_vtk)
    point_data.AddArray(std_vtk)

point_data.RemoveArray(array_names[0])
output_file=f"GT_{file_name}_Gaussian.vti"
writer = vtk.vtkXMLImageDataWriter()
writer.SetFileName(output_file)
writer.SetInputData(new_image_data)
writer.Write()
print("1.end of generating gaussian")
"""#pmc generation"""

# === CONFIG ===


def get_cell_vertices(i, j, k):
    return [
        (i,   j,   k),
        (i+1, j,   k),
        (i+1, j+1, k),
        (i,   j+1, k),
        (i,   j,   k+1),
        (i+1, j,   k+1),
        (i+1, j+1, k+1),
        (i,   j+1, k+1),
    ]

def build_covariance(sigma, rho=0.6):
    cov = np.full((8, 8), rho)
    np.fill_diagonal(cov, 1.0)
    return (sigma[:, None] * sigma[None, :]) * cov

def sample_crossing_probability(mu, sigma, isovalue, n_samples, rho):
    cov = build_covariance(sigma, rho)
    try:
        samples = np.random.multivariate_normal(mu, cov, size=n_samples)
    except np.linalg.LinAlgError:
        samples = np.random.normal(loc=mu, scale=sigma, size=(n_samples, 8))
    cross = np.any(samples < isovalue, axis=1) & np.any(samples > isovalue, axis=1)
    return np.mean(cross)

def compute_gaussian_cell(args):
    i, j, k, mean_3d, std_3d, correlation = args
    verts = get_cell_vertices(i, j, k)
    mu = np.array([mean_3d[x, y, z] for (x, y, z) in verts], dtype=np.float64)
    sigma = np.array([std_3d[x, y, z] for (x, y, z) in verts], dtype=np.float64)
    prob = sample_crossing_probability(mu, sigma, ISOLEVEL, N_SAMPLES, correlation)
    return (i, j, k, prob)

def compute_gmm_cell(args):
    i, j, k, gmm_means_3d, gmm_stds_3d, gmm_weights_3d = args
    verts = get_cell_vertices(i, j, k)
    means_list, covs_list, weights_raw = [], [], []
    for c in range(3):
        mu_c = [gmm_means_3d[c][x, y, z] for (x, y, z) in verts]
        std_c = [gmm_stds_3d[c][x, y, z] for (x, y, z) in verts]
        cov_c = np.diag(np.square(std_c))
        means_list.append(np.array(mu_c))
        covs_list.append(cov_c)
        w_c = [gmm_weights_3d[c][x, y, z] for (x, y, z) in verts]
        weights_raw.append(np.prod(w_c))
    weights_array = np.array(weights_raw)
    weights_array /= np.sum(weights_array)
    samples = sample_8d_gmm(weights_array, means_list, covs_list, N_SAMPLES)
    cross = np.any(samples < ISOLEVEL, axis=1) & np.any(samples > ISOLEVEL, axis=1)
    return (i, j, k, np.mean(cross))

def sample_8d_gmm(weights, means, covs, n_samples):
    comps = np.random.choice(3, size=n_samples, p=weights)
    samples = np.empty((n_samples, 8))
    for c in range(3):
        idx = np.where(comps == c)[0]
        if len(idx) > 0:
            samples[idx] = np.random.multivariate_normal(mean=means[c], cov=covs[c], size=len(idx))
    return samples

def write_vti(prob_grid, spacing, origin, output_path):
    prob_image = vtk.vtkImageData()
    prob_image.SetDimensions(prob_grid.shape)
    prob_image.SetSpacing(spacing)
    prob_image.SetOrigin(origin)
    vtk_array = numpy_to_vtk(prob_grid.ravel(order='F'), deep=True)
    vtk_array.SetName("crossing_probability")
    prob_image.GetPointData().AddArray(vtk_array)
    prob_image.GetPointData().SetActiveScalars("crossing_probability")
    writer = vtk.vtkXMLImageDataWriter()
    writer.SetFileName(output_path)
    writer.SetInputData(prob_image)
    writer.Write()
    print(f"Saved output to {output_path}",flush=True)

def process_gaussian(vti_file,data):
    print(f"Processing Gaussian: {vti_file}",flush=True)
    image = data
    dims = image.GetDimensions()
    mean_3d = vtk_to_numpy(image.GetPointData().GetArray(0)).reshape(dims[::-1], order='F')
    std_3d = vtk_to_numpy(image.GetPointData().GetArray(1)).reshape(dims[::-1], order='F')
    output_shape = tuple(d - 1 for d in mean_3d.shape)
    all_indices = [(i, j, k, mean_3d, std_3d, 0.0) for i, j, k in product(*map(range, output_shape))]
    with multiprocessing.Pool() as pool:
        results = pool.map(compute_gaussian_cell, all_indices)
    prob_grid = np.zeros(output_shape, dtype=np.float32)
    for i, j, k, p in results:
        prob_grid[i, j, k] = p
    write_vti(prob_grid, image.GetSpacing(), image.GetOrigin(), os.path.join(vti_dir, f"isosurface_{os.path.basename(vti_file)}"))

def process_gmm(vti_file,data):
    print(f"Processing GMM: {vti_file}",flush=True)
    image=data
    dims = image.GetDimensions()
    gmm_means_3d = [vtk_to_numpy(image.GetPointData().GetArray(f"GMM_Mean{i}")).reshape(dims[::-1], order='F') for i in range(3)]
    gmm_stds_3d = [vtk_to_numpy(image.GetPointData().GetArray(f"GMM_Std{i}")).reshape(dims[::-1], order='F') for i in range(3)]
    gmm_weights_3d = [vtk_to_numpy(image.GetPointData().GetArray(f"GMM_Weight{i}")).reshape(dims[::-1], order='F') for i in range(3)]
    output_shape = tuple(d - 1 for d in dims[::-1])
    all_indices = [(i, j, k, gmm_means_3d, gmm_stds_3d, gmm_weights_3d) for i, j, k in product(*map(range, output_shape))]
    with multiprocessing.Pool() as pool:
        results = pool.map(compute_gmm_cell, all_indices)
    prob_grid = np.zeros(output_shape, dtype=np.float32)
    for i, j, k, p in results:
        prob_grid[i, j, k] = p
    write_vti(prob_grid, image.GetSpacing(), image.GetOrigin(), os.path.join(vti_dir, f"isosurface_{os.path.basename(vti_file)}"))
ISOLEVEL = iso_value
N_SAMPLES = 10000
vti_dir = "data"
if __name__ == '__main__':
    print("2.currently generating original gaussian isosurface")
    curr_file=f"gaussin_{file_name}.vti"
    process_gaussian(curr_file,new_image_data)
    print("2.end generating original gaussian isosurface")

"""#gaussian sirenet"""

# -*- coding: utf-8 -*-
"""gaussian_sirenet.ipynb"""
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print('Device running:', device,flush=True)
print("3.training sirenet for gaussian data")
class SineLayer(nn.Module):
    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):
        super().__init__()
        self.omega_0 = omega_0
        self.is_first = is_first
        # self.enable_dropout = enable_dropout
        # self.dropout_prob = dropout_prob
        self.in_features = in_features
        # if enable_dropout:
        #     if not self.is_first:
        #         self.dropout = nn.Dropout(dropout_prob)
        self.linear = nn.Linear(in_features, out_features, bias=bias)

        self.init_weights()

    def init_weights(self):
        with torch.no_grad():
            if self.is_first:
                self.linear.weight.uniform_(-1 / self.in_features,
                                             1 / self.in_features)
            else:
                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,
                                             np.sqrt(6 / self.in_features) / self.omega_0)


    def forward(self, x):
        x = self.linear(x)
        # if self.enable_dropout:
        #     if not self.is_first:
        #         x = self.dropout(x)
        return torch.sin(self.omega_0 * x)

class ResidualSineLayer(nn.Module):
    def __init__(self, features, bias=True, ave_first=False, ave_second=False, omega_0=30):
        super().__init__()
        self.omega_0 = omega_0
        # self.enable_dropout = enable_dropout
        # self.dropout_prob = dropout_prob
        self.features = features
        # if enable_dropout:
        #     self.dropout_1 = nn.Dropout(dropout_prob)
        self.linear_1 = nn.Linear(features, features, bias=bias)
        self.linear_2 = nn.Linear(features, features, bias=bias)
        self.weight_1 = .5 if ave_first else 1
        self.weight_2 = .5 if ave_second else 1

        self.init_weights()


    def init_weights(self):
        with torch.no_grad():
            self.linear_1.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,
                                           np.sqrt(6 / self.features) / self.omega_0)
            self.linear_2.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,
                                           np.sqrt(6 / self.features) / self.omega_0)

    def forward(self, input):
        linear_1 = self.linear_1(self.weight_1*input)
        # if self.enable_dropout:
        #     linear_1 = self.dropout_1(linear_1)
        sine_1 = torch.sin(self.omega_0 * linear_1)
        sine_2 = torch.sin(self.omega_0 * self.linear_2(sine_1))
        return self.weight_2*(input+sine_2)

class MyResidualSirenNet(nn.Module):
    def __init__(self, obj):
        super(MyResidualSirenNet, self).__init__()
        # self.enable_dropout = obj['enable_dropout']
        # self.dropout_prob = obj['dropout_prob']
        self.Omega_0=30
        self.n_layers = obj['n_layers']
        self.input_dim = obj['dim']
        self.output_dim = obj['total_vars']
        self.neurons_per_layer = obj['n_neurons']
        self.layers = [self.input_dim]
        for i in range(self.n_layers-1):
            self.layers.append(self.neurons_per_layer)
        self.layers.append(self.output_dim)
        self.net_layers = nn.ModuleList()
        for idx in np.arange(self.n_layers):
            layer_in = self.layers[idx]
            layer_out = self.layers[idx+1]
            ## if not the final layer
            if idx != self.n_layers-1:
                ## if first layer
                if idx==0:
                    self.net_layers.append(SineLayer(layer_in,layer_out,bias=True,is_first=idx==0))
                ## if an intermdeiate layer
                else:
                    self.net_layers.append(ResidualSineLayer(layer_in,bias=True,ave_first=idx>1,ave_second=idx==(self.n_layers-2)))
            ## if final layer
            else:
                final_linear = nn.Linear(layer_in,layer_out)
                ## initialize weights for the final layer
                with torch.no_grad():
                    final_linear.weight.uniform_(-np.sqrt(6 / (layer_in)) / self.Omega_0, np.sqrt(6 / (layer_in)) / self.Omega_0)
                self.net_layers.append(final_linear)

    def forward(self,x):
        for net_layer in self.net_layers:
            x = net_layer(x)
        return x

def size_of_network(n_layers, n_neurons, d_in, d_out, is_residual = True):
    # Adding input layer
    layers = [d_in]
    # layers = [3]

    # Adding hidden layers
    layers.extend([n_neurons]*n_layers)
    # layers = [3, 5, 5, 5]

    # Adding output layer
    layers.append(d_out)
    # layers = [3, 5, 5, 5, 1]

    # Number of steps
    n_layers = len(layers)-1
    # n_layers = 5 - 1 = 4

    n_params = 0

    # np.arange(4) = [0, 1, 2, 3]
    for ndx in np.arange(n_layers):

        # number of neurons in below layer
        layer_in = layers[ndx]

        # number of neurons in above layer
        layer_out = layers[ndx+1]

        # max number of neurons in both the layer
        og_layer_in = max(layer_in,layer_out)

        # if lower layer is the input layer
        # or the upper layer is the output layer
        if ndx==0 or ndx==(n_layers-1):
            # Adding weight corresponding to every neuron for every input neuron
            # Adding bias for every neuron in the upper layer
            n_params += ((layer_in+1)*layer_out)

        else:

            # If the layer is residual then proceed as follows as there will be more weights if residual layer is included
            if is_residual:
                # doubt in the following two lines
                n_params += (layer_in*og_layer_in)+og_layer_in
                n_params += (og_layer_in*layer_out)+layer_out

            # if the layer is non residual then simply add number of weights and biases as follows
            else:
                n_params += ((layer_in+1)*layer_out)
            #
        #
    #

    return n_params

def compute_PSNR(arrgt,arr_recon):
    diff = arrgt - arr_recon
    sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2
    snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))
    return snr

def srs(numOfPoints, valid_pts, percentage, isMaskPresent, mask_array):

    # getting total number of sampled points
    numberOfSampledPoints = int((valid_pts/100) * percentage)

    # storing corner indices in indices variable
    indices = set()

    # As long as we don't get the required amount of sample points keep finding the random numbers
    while(len(indices) < numberOfSampledPoints):
        rp = random.randint(0, numOfPoints-1)
        if isMaskPresent and mask_array[rp] == 0:
            continue
        indices.add(rp)

    # return indices
    return indices

def findMultiVariatePSNR(var_name, total_vars, actual, pred):
    # print('Printing PSNR')
    tot = 0
    psnr_list = []
    for j in range(total_vars):
        psnr = compute_PSNR(actual[:,j], pred[:,j])
        psnr_list.append(psnr)
        tot += psnr
        print(var_name[j], ' PSNR:', psnr,flush=True)
    avg_psnr = tot/total_vars
    print('\nAverage psnr : ', avg_psnr,flush=True)
     #this function is calculating the psnr of final epoch (or whenever it is called) of each variable and then averaging it
     #Thus individual epochs psnr is not calculated

    return psnr_list, avg_psnr

def compute_rmse(actual, predicted):
    mse = np.mean((actual - predicted) ** 2)
    return np.sqrt(mse)

def denormalizeValue(total_vars, to, ref):
    to_arr = np.array(to)
    for i in range(total_vars):
        min_data = np.min(ref[:, i])
        max_data = np.max(ref[:, i])
        to_arr[:, i] = (((to[:, i] * 0.5) + 0.5) * (max_data - min_data)) + min_data
    return to_arr

def makeVTI(data, val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name, normalizedVersion = False):
    nn_predictions = denormalizeValue(total_vars, n_predictions, val) if not normalizedVersion else n_predictions
    writer = vtkXMLImageDataWriter()
    writer.SetFileName(vti_path + vti_name)
    img = vtkImageData()
    img.CopyStructure(data)
    if not isMaskPresent:
        for i in range(total_vars):
            f = var_name[i]
            temp = nn_predictions[:, i]
            arr = vtkFloatArray()
            for j in range(n_pts):
                arr.InsertNextValue(temp[j])
            arr.SetName(f)
            img.GetPointData().AddArray(arr)
        # print(img)
        writer.SetInputData(img)
        writer.Write()
        print(f'Vti File written successfully at {vti_path}{vti_name}',flush=True)
        return img
    else:
        for i in range(total_vars):
            f = var_name[i]
            temp = nn_predictions[:, i]
            idx = 0
            arr = vtkFloatArray()
            for j in range(n_pts):
                if(mask_arr[j] == 1):
                    arr.InsertNextValue(temp[idx])
                    idx += 1
                else:
                    arr.InsertNextValue(0.0)
            arr.SetName('p_' + f)
            data.GetPointData().AddArray(arr)
        # print(data)
        writer.SetInputData(data)
        writer.Write()
        print(f'Vti File written successfully at {vti_path}{vti_name}',flush=True)
        return data

def getImageData(actual_img, val, n_pts, var_name, isMaskPresent, mask_arr):
    img = vtkImageData()
    img.CopyStructure(actual_img)
    # if isMaskPresent:
    #     img.DeepCopy(actual_img)
    # img.SetDimensions(dim)
    # img.SetOrigin(actual_img.GetOrigin())
    # img.SetSpacing(actual_img.GetSpacing())
    if not isMaskPresent:
        f = var_name
        data = val
        arr = vtkFloatArray()
        for j in range(n_pts):
            arr.InsertNextValue(data[j])
        arr.SetName(f)
        img.GetPointData().SetScalars(arr)
    else:
        f = var_name
        data = val
        idx = 0
        arr = vtkFloatArray()
        for j in range(n_pts):
            if(mask_arr[j] == 1):
                arr.InsertNextValue(data[idx])
                idx += 1
            else:
                arr.InsertNextValue(0.0)
        arr.SetName(f)
        img.GetPointData().SetScalars(arr)
    return img

# from argparse import Namespace

# # Parameters (simulating argparse in a Jupyter Notebook)
# args = Namespace(
#     n_neurons=150,
#     n_layers=6,
#     epochs=200,  # Required argument: Set the number of epochs
#     batchsize=512,
#     lr=0.00005,
#     no_decay=False,
#     decay_rate=0.8,
#     decay_at_interval=True,
#     decay_interval=15,
#     datapath='data\GT_teardrop_Gaussian.vti',  # Required: Set the path to your data
#     outpath='./models/',
#     exp_path='../logs/',
#     modified_data_path='./data/',
#     dataset_name='3d_data',  # Required: Set the dataset name
#     vti_name='predicted_GT_teardrop_Gaussian.vti',  # Required: Name of the dataset
#     vti_path='./data/'
# )

# print(args, end='\n\n')

# # Assigning parameters to variables
# LR = args.lr
# BATCH_SIZE = args.batchsize
# decay_rate = args.decay_rate
# decay_at_equal_interval = args.decay_at_interval

# decay = not args.no_decay
# MAX_EPOCH = args.epochs

# n_neurons = args.n_neurons
# n_layers = args.n_layers + 2
# decay_interval = args.decay_interval
# outpath = args.outpath
# exp_path = args.exp_path
# datapath = args.datapath
# modified_data_path = args.modified_data_path
# dataset_name = args.dataset_name
# vti_name = args.vti_name
# vti_path = args.vti_path

# # Displaying the final configuration
# print(f"Learning Rate: {LR}")
# print(f"Batch Size: {BATCH_SIZE}")
# print(f"Decay Rate: {decay_rate}")
# print(f"Max Epochs: {MAX_EPOCH}")
# print(f"Number of Neurons per Layer: {n_neurons}")
# print(f"Number of Layers (including input/output): {n_layers}")
# print(f"Data Path: {datapath}")
# print(f"Output Path: {outpath}")
# print(f"Dataset Name: {dataset_name}")
# print(f"Vti Name: {vti_name}")

# Variable Initialization
var_name = []
total_vars = None  # Number of variables
univariate = None  # True if dataset has one variable, else False
group_size = 5000  # Group size during testing


# Constructing the log file name
log_file = (
    f'train_{dataset_name}_{n_layers-2}rb_{n_neurons}n_{BATCH_SIZE}bs_'
    f'{LR}lr_{decay}decay_{decay_rate}dr_'
    f'{"decayingAtInterval" + str(decay_interval) if decay_at_equal_interval else "decayingWhenLossIncr"}'
)

print(log_file)

n_pts = None  # Number of points in the dataset
n_dim = None  # Dimensionality of the data
dim = None  # Other dimension-specific information

print("Decay:", decay,flush=True)
print(f'Extracting variables from path: {datapath}', end="\n\n",flush=True)

# Placeholder for data
data_array = []
scalar_data = None

# Reading values from .vti files
reader = vtk.vtkXMLImageDataReader()
reader.SetFileName(datapath)
reader.Update()

data = new_image_data
scalar_data = data
pdata = data.GetPointData()
n_pts = data.GetNumberOfPoints()
dim = data.GetDimensions()
n_dim = len(dim)
total_arr = pdata.GetNumberOfArrays()

print("n_pts:", n_pts, "dim:", dim, "n_dim:", n_dim, "total_arr:", total_arr,flush=True)

var_name = []
data_array = []

# Extracting data from the .vti file
for i in range(total_arr):
    a_name = pdata.GetArrayName(i)

    cur_arr = pdata.GetArray(a_name)
    n_components = cur_arr.GetNumberOfComponents()

    if n_components == 1:
        var_name.append(a_name)
        data_array.append(vtk_to_numpy(cur_arr))
    else:
        component_names = [f"{a_name}_{c}" for c in ['x', 'y', 'z'][:n_components]]
        var_name.extend(component_names)
        for c in range(n_components):
            c_data = [cur_arr.GetComponent(j, c) for j in range(n_pts)]
            data_array.append(np.array(c_data))

total_vars = len(var_name)
univariate = total_vars == 1

# Prepare numpy arrays for coordinates and variable values
cord = np.zeros((n_pts, n_dim))
val = np.zeros((n_pts, total_vars))

# Store data in numpy arrays
for i in range(n_pts):
    pt = scalar_data.GetPoint(i)
    cord[i, :] = pt
    val[i, :] = [arr[i] for arr in data_array]

# Display final information
print("Total Variables:", total_vars,flush=True)
print("Univariate:", univariate,flush=True)
print("Coordinates Shape:", cord.shape,flush=True)
print("Values Shape:", val.shape,flush=True)

real_data=val.copy()
print(np.max(real_data))
# Normalize values between -1 and 1
for i in range(total_vars):
    min_data = np.min(val[:, i])
    max_data = np.max(val[:, i])
    # norm_params[var_name[i]] = (min_data, max_data)
    val[:, i] = 2.0 * ((val[:, i] - min_data) / (max_data - min_data) - 0.5)

# Normalize Coordinates to [-1,1]
for i in range(n_dim):
    # Use (dim[i]-1] so that coordinates go from 0 to dim[i]-1.
    cord[:, i] = 2.0 * (cord[:, i] / (dim[i] - 1) - 0.5)
n_cord = cord.copy()
n_val = val.copy()
means=n_val[:,0]
stds=n_val[:,1]
# Convert normalized data to PyTorch tensors
torch_coords = torch.from_numpy(n_cord)
torch_means = torch.from_numpy(means)
torch_stds =torch.from_numpy(stds)
# Display dataset details
print('Dataset Name:', dataset_name,flush=True)
print('Total Variables:', total_vars,flush=True)
print('Variables Name:', var_name, end="\n\n",flush=True)
print('Total Points in Data:', n_pts,flush=True)
print('Dimension of the Dataset:', dim,flush=True)
print('Number of Dimensions:', n_dim,flush=True)
print('Coordinate Tensor Shape:', torch_coords.shape,flush=True)
print('Scalar means Values Tensor Shape:', torch_means.shape,flush=True)
print('Scalar stds Values Tensor Shape:', torch_stds.shape,flush=True)

print('\n###### Data setup is complete, now starting training ######\n',flush=True)

# Prepare the DataLoader
train_dataloader_mean = DataLoader(
    TensorDataset(torch_coords, torch_means),
    batch_size=BATCH_SIZE,
    pin_memory=True,
    shuffle=True,
    num_workers=4
)
# Model configuration
obj = {
    'total_vars': 1,
    'dim': n_dim,
    'n_neurons': n_neurons,
    'n_layers': n_layers
}

# Initialize the model, optimizer, and loss function
model_mean = MyResidualSirenNet(obj).to(device)
print(model_mean)

optimizer = optim.Adam(model_mean.parameters(), lr=LR, betas=(0.9, 0.999))
print(optimizer)

criterion = nn.MSELoss()
print(criterion)

# Training configuration summary
print('\nLearning Rate:', LR)
print('Max Epochs:', MAX_EPOCH)
print('Batch Size:', BATCH_SIZE)
print('Number of Hidden Layers:', obj['n_layers'] - 2)
print('Number of Neurons per Layer:', obj['n_neurons'])

if decay:
    print('Decay Rate:', decay_rate)
    if decay_at_equal_interval:
        print(f'Rate decays every {decay_interval} epochs.',flush=True)
    else:
        print('Rate decays when the current epoch loss is greater than the previous epoch loss.',flush=True)
else:
    print('No decay!',flush=True)
print()

train_loss_list = []
best_epoch = -1
best_loss = 1e8
best_model=""
from tqdm import tqdm
# Ensure the output path exists
if not os.path.exists(outpath):
    os.makedirs(outpath)

# Training loop
for epoch in tqdm(range(MAX_EPOCH)):
    model_mean.train()
    temp_loss_list = []
    start = time.time()

    # Batch-by-batch training
    for X_train, y_train in train_dataloader_mean:
        X_train = X_train.type(torch.float32).to(device)
        y_train = y_train.type(torch.float32).to(device)

        if univariate:
            y_train = y_train.squeeze()

        optimizer.zero_grad()
        predictions = model_mean(X_train)
        predictions = predictions.squeeze()
        loss = criterion(predictions, y_train)
        loss.backward()
        optimizer.step()

        # Track batch loss
        temp_loss_list.append(loss.detach().cpu().numpy())

    # Calculate epoch loss
    epoch_loss = np.average(temp_loss_list)

    # Learning rate decay
    if decay:
        if decay_at_equal_interval:
            if epoch >= decay_interval and epoch % decay_interval == 0:
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= decay_rate
        else:
        #     if epoch > 0 and epoch_loss > train_loss_list[-1]:
        #         for param_group in optimizer.param_groups:
        #             param_group['lr'] *= decay_rate
            if epoch > 0 and epoch_loss > train_loss_list[-1]:
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= decay_rate

    # Track losses and best model
    train_loss_list.append(epoch_loss)
    if epoch_loss < best_loss:
        best_loss = epoch_loss
        best_epoch = epoch+1
        if(best_model==0):
            best_model=model_mean.state_dict()
        else:
            best_model=model_mean.state_dict()

    end = time.time()
    print(
        f"Epoch: {epoch + 1}/{MAX_EPOCH} | Train Loss: {train_loss_list[-1]} | "
        f"Time: {round(end - start, 2)}s ({device}) | LR: {optimizer.param_groups[0]['lr']}",flush=True
    )

    # Save model at intervals
    if (epoch + 1) % 50 == 0:
        model_name = (
            f'train_{dataset_name}_{epoch + 1}ep_{n_layers - 2}rb_{n_neurons}n_'
            f'{BATCH_SIZE}bs_{LR}lr_{decay}decay_{decay_rate}dr_'
            f'{"decayingAtInterval" + str(decay_interval)+"mean" if decay_at_equal_interval else "decayingWhenLossIncr"}'
        )
        torch.save(
            {"epoch": epoch + 1, "model_state_dict": model_mean.state_dict()},
            os.path.join(outpath, f'{model_name}_mean.pth')
        )

# Final summary
print('\nEpoch with Least Loss:', best_epoch, '| Loss:', best_loss, '\n',flush=True)

# Save the final model
model_name = f'siren_compressor'
torch.save(
    {"epoch": MAX_EPOCH, "model_state_dict": model_mean.state_dict()},
    os.path.join(outpath, f'{model_name}_mean.pth')
)
torch.save(
    {"epoch": best_epoch, "model_state_dict": best_model},
    os.path.join(outpath, f'{best_epoch}_mean.pth')
)

# Initialize prediction lists
prediction_list = [[] for _ in range(1)]
total_vars=1
# Inference loop
model_mean = MyResidualSirenNet(obj).to(device)
state_dict = torch.load(os.path.join(outpath, f'{best_epoch}_mean.pth'))['model_state_dict']
model_mean.load_state_dict(state_dict)
with torch.no_grad():
    for i in range(0, torch_coords.shape[0], group_size):
        coords = torch_coords[i:min(i + group_size, torch_coords.shape[0])].type(torch.float32).to(device)
        vals = model_mean(coords)
        vals = vals.to('cpu')

        for j in range(total_vars):
            prediction_list[j].append(vals[:, j])

# Extract and concatenate predictions
extracted_list = [[] for _ in range(1)]
for i in range(len(prediction_list[0])):
    for j in range(1):
        el = prediction_list[j][i].detach().numpy()
        extracted_list[j].append(el)

for j in range(1):
    extracted_list[j] = np.concatenate(extracted_list[j], dtype='float32')

# Final prediction (normalized)
n_predictions_means = np.array(extracted_list).T

# Compute PSNR
#findMultiVariatePSNR(var_name[0], total_vars, n_val[:,0], n_predictions_means[:,0])
print("mean",compute_PSNR(n_val[:,0],n_predictions_means[:,0]),flush=True)
# Compute RMSE
rmse = compute_rmse(n_val[:,0], n_predictions_means[:,0])
print("RMSE:", rmse)

# Prepare the DataLoader
train_dataloader_std= DataLoader(
    TensorDataset(torch_coords, torch_stds),
    batch_size=BATCH_SIZE,
    pin_memory=True,
    shuffle=True,
    num_workers=4
)
# Model configuration
obj = {
    'total_vars': 1,
    'dim': n_dim,
    'n_neurons': n_neurons,
    'n_layers': n_layers
}

# Initialize the model, optimizer, and loss function
model_std = MyResidualSirenNet(obj).to(device)
print(model_std)

optimizer_std = optim.Adam(model_std.parameters(), lr=LR, betas=(0.9, 0.999))
print(optimizer_std)

criterion_std = nn.MSELoss()
print(criterion_std)

# Training configuration summary
print('\nLearning Rate:', LR,flush=True)
print('Max Epochs:', MAX_EPOCH,flush=True)
print('Batch Size:', BATCH_SIZE,flush=True)
print('Number of Hidden Layers:', obj['n_layers'] - 2,flush=True)
print('Number of Neurons per Layer:', obj['n_neurons'],flush=True)

if decay:
    print('Decay Rate:', decay_rate,flush=True)
    if decay_at_equal_interval:
        print(f'Rate decays every {decay_interval} epochs.',flush=True)
    else:
        print('Rate decays when the current epoch loss is greater than the previous epoch loss.',flush=True)
else:
    print('No decay!',flush=True)
print()

train_loss_list = []
best_epoch = -1
best_loss = 1e8
best_model=""
from tqdm import tqdm
# Ensure the output path exists
if not os.path.exists(outpath):
    os.makedirs(outpath)

# Training loop
for epoch in tqdm(range(MAX_EPOCH)):
    model_std.train()
    temp_loss_list = []
    start = time.time()

    # Batch-by-batch training
    for X_train, y_train in train_dataloader_std:
        X_train = X_train.type(torch.float32).to(device)
        y_train = y_train.type(torch.float32).to(device)

        if univariate:
            y_train = y_train.squeeze()

        optimizer_std.zero_grad()
        predictions = model_std(X_train)
        predictions = predictions.squeeze()
        loss = criterion_std(predictions, y_train)
        loss.backward()
        optimizer_std.step()

        # Track batch loss
        temp_loss_list.append(loss.detach().cpu().numpy())

    # Calculate epoch loss
    epoch_loss = np.average(temp_loss_list)

    # Learning rate decay
    if decay:
        if decay_at_equal_interval:
            if epoch >= decay_interval and epoch % decay_interval == 0:
                for param_group in optimizer_std.param_groups:
                    param_group['lr'] *= decay_rate
        else:
        #     if epoch > 0 and epoch_loss > train_loss_list[-1]:
        #         for param_group in optimizer.param_groups:
        #             param_group['lr'] *= decay_rate
            if epoch > 0 and epoch_loss > train_loss_list[-1]:
                for param_group in optimizer_std.param_groups:
                    param_group['lr'] *= decay_rate

    # Track losses and best model
    train_loss_list.append(epoch_loss)
    if epoch_loss < best_loss:
        best_loss = epoch_loss
        best_epoch = epoch+1
        if(best_model==0):
            best_model=model_std.state_dict()
        else:
            best_model=model_std.state_dict()

    end = time.time()
    print(
        f"Epoch: {epoch + 1}/{MAX_EPOCH} | Train Loss: {train_loss_list[-1]} | "
        f"Time: {round(end - start, 2)}s ({device}) | LR: {optimizer_std.param_groups[0]['lr']}",flush=True
    )

    # Save model at intervals
    if (epoch + 1) % 50 == 0:
        model_name = (
            f'train_{dataset_name}_{epoch + 1}ep_{n_layers - 2}rb_{n_neurons}n_'
            f'{BATCH_SIZE}bs_{LR}lr_{decay}decay_{decay_rate}dr_'
            f'{"decayingAtInterval" + str(decay_interval)+"std" if decay_at_equal_interval else "decayingWhenLossIncr"}'
        )
        torch.save(
            {"epoch": epoch + 1, "model_state_dict": model_std.state_dict()},
            os.path.join(outpath, f'{model_name}_std.pth')
        )

# Final summary
print('\nEpoch with Least Loss:', best_epoch, '| Loss:', best_loss, '\n',flush=True)

# Save the final model
model_name = f'siren_compressor'
torch.save(
    {"epoch": MAX_EPOCH, "model_state_dict": model_std.state_dict()},
    os.path.join(outpath, f'{model_name}_std.pth')
)
torch.save(
    {"epoch": best_epoch, "model_state_dict": best_model},
    os.path.join(outpath, f'{best_epoch}_std.pth')
)

# Initialize prediction lists
# Initialize prediction lists
prediction_list = [[] for _ in range(1)]
total_vars=1
# Inference loop
model_std = MyResidualSirenNet(obj).to(device)
state_dict = torch.load(os.path.join(outpath, f'{best_epoch}_std.pth'))['model_state_dict']
model_std.load_state_dict(state_dict)
with torch.no_grad():
    for i in range(0, torch_coords.shape[0], group_size):
        coords = torch_coords[i:min(i + group_size, torch_coords.shape[0])].type(torch.float32).to(device)
        vals = model_std(coords)
        vals = vals.to('cpu')

        for j in range(total_vars):
            prediction_list[j].append(vals[:, j])

# Extract and concatenate predictions
extracted_list = [[] for _ in range(1)]
for i in range(len(prediction_list[0])):
    for j in range(1):
        el = prediction_list[j][i].detach().numpy()
        extracted_list[j].append(el)

for j in range(1):
    extracted_list[j] = np.concatenate(extracted_list[j], dtype='float32')

# Final prediction (normalized)
n_predictions_stds = np.array(extracted_list).T
print(n_predictions_stds.shape)
# Compute PSNR
#findMultiVariatePSNR(var_name[0], total_vars, n_val[:,0], n_predictions_means[:,0])
print("std",compute_PSNR(n_val[:,1],n_predictions_stds[:,0]),flush=True)
# Compute RMSE
rmse = compute_rmse(n_val[:,1], n_predictions_stds[:,0])
print("RMSE:", rmse,flush=True)

n_predictions = np.concatenate([n_predictions_means, n_predictions_stds], axis=1)
# !rm -rf /kaggle/working/*

# #print(os.path.getsize('/kaggle/working/models/train_3d_data_200ep_6rb_320n_512bs_5e-05lr_Truedecay_0.8dr_decayingAtInterval15.pth') / (1024 ** 2), 'MB')

# # vti saving path
vti_path = args.vti_path
if not os.path.exists(vti_path):
    os.makedirs(vti_path)
# vti name
vti_name = "predicted_gaussian_"+f"{file_name}"+".vti"
isMaskPresent = False
mask_arr = []
total_vars=2
data=makeVTI(data,real_data, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name)
#print(data)
print("3.end of predciting and training sirenet for gaussian data")
if __name__ == '__main__':
    print("4.genrating isosurface on predicted gaussian")
    curr_file=f"gausssian_predicted_{file_name}.vti"
    process_gaussian(curr_file,data)
    print("4. end of genrating isosurface on predicted gaussian")

"""#gmm generation

"""

no_of_samples = 10000
scale_factor = 2
output_file = "GT_teardrop_GMM.vti"   # Output filename
print("5.genration of gmm dataset")
# === Load VTI Data ===
# reader = vtk.vtkXMLImageDataReader()
# reader.SetFileName(input_file)
# reader.Update()
image_data = new_image_data
point_data = image_data.GetPointData()
print(point_data.GetNumberOfArrays())
dims = image_data.GetDimensions()
num_points = dims[0] * dims[1] * dims[2]
print(f"Loaded grid: {dims}, total points: {num_points}")

# === Get Mean and Std Arrays ===
mean = vtk_to_numpy(point_data.GetArray(0))
std = vtk_to_numpy(point_data.GetArray(1))

# === Fit GMM Per Point (Parallel) ===
def fit_gmm_for_point(mu, sigma, scale_factor, n_samples=500):
    # if sigma<=1e-3 or np.isnan(sigma) or np.isinf(sigma):
    #     sigma = 1e-3
    sigma = sigma * scale_factor

    samples = np.random.normal(mu, sigma, size=n_samples).reshape(-1, 1)
    gmm = GaussianMixture(n_components=3, covariance_type='diag', max_iter=50, random_state=42)
    gmm.fit(samples)

    means = gmm.means_.flatten()
    stds = np.sqrt(gmm.covariances_.flatten())
    stds = np.clip(stds, a_min=1e-8, a_max=None)
    weights = gmm.weights_.flatten()

    order = np.argsort(means)
    return means[order], stds[order], weights[order]

print("Fitting GMMs in parallel...",flush=True)
results = Parallel(n_jobs=-1, backend='loky')(delayed(fit_gmm_for_point)(
    mean[i], std[i], scale_factor, no_of_samples) for i in tqdm(range(num_points)))

# === Create Output VTK Arrays ===
def create_vtk_array(name):
    arr = vtk.vtkFloatArray()
    arr.SetName(name)
    arr.SetNumberOfComponents(1)
    arr.SetNumberOfTuples(num_points)
    return arr

gmm_mean0 = create_vtk_array("GMM_Mean0")
gmm_mean1 = create_vtk_array("GMM_Mean1")
gmm_mean2 = create_vtk_array("GMM_Mean2")
gmm_std0  = create_vtk_array("GMM_Std0")
gmm_std1  = create_vtk_array("GMM_Std1")
gmm_std2  = create_vtk_array("GMM_Std2")
gmm_w0    = create_vtk_array("GMM_Weight0")
gmm_w1    = create_vtk_array("GMM_Weight1")
gmm_w2    = create_vtk_array("GMM_Weight2")

# === Fill VTK Arrays with Fitted GMM Data ===
for pt_id, (means, stds, weights) in enumerate(results):
    gmm_mean0.SetValue(pt_id, means[0])
    gmm_mean1.SetValue(pt_id, means[1])
    gmm_mean2.SetValue(pt_id, means[2])

    gmm_std0.SetValue(pt_id, stds[0])
    gmm_std1.SetValue(pt_id, stds[1])
    gmm_std2.SetValue(pt_id, stds[2])

    gmm_w0.SetValue(pt_id, weights[0])
    gmm_w1.SetValue(pt_id, weights[1])
    gmm_w2.SetValue(pt_id, weights[2])

# === Attach Arrays to VTK Data ===
point_data.AddArray(gmm_mean0)
point_data.AddArray(gmm_mean1)
point_data.AddArray(gmm_mean2)
point_data.AddArray(gmm_std0)
point_data.AddArray(gmm_std1)
point_data.AddArray(gmm_std2)
point_data.AddArray(gmm_w0)
point_data.AddArray(gmm_w1)
point_data.AddArray(gmm_w2)
point_data.RemoveArray("Mean")
point_data.RemoveArray("Std")
# === Write Output ===
gmm_image_data=image_data
output_file=f"GT_{file_name}_GMM.vti"
writer = vtk.vtkXMLImageDataWriter()
writer.SetFileName(output_file)
writer.SetInputData(image_data)
writer.Write()
print(f"Done. GMM output saved to '{output_file}'",flush=True)
print("5. end of genration of gmm dataset")
if __name__ == '__main__':
    print("6.genration of isosurface on gmm dataset")
    curr_file=f"gmm_{file_name}.vti"
    process_gmm(curr_file,gmm_image_data)
    print("6.end of genration of isosurface on gmm dataset")

"""#pmc for gmm"""

'''sirenet for the gmm with three models one model for three means and one model for three stds and one model for three weights'''


'''loading the data'''
#datasetup
print("7.trainning sirenet on gmm dataset and then precdiction")
reader=vtk.vtkXMLImageDataReader()
reader.SetFileName(datapath)
reader.update()
image_data=gmm_image_data
point_data=image_data.GetPointData()
no_of_arrays=point_data.GetNumberOfArrays()
variables=[]
for i in range(no_of_arrays):
  array=point_data.GetArrayName(i)
  variables.append(array)
variables

origin=image_data.GetOrigin()
dim=image_data.GetDimensions()
spacing=image_data.GetSpacing()
num_points=dim[0]*dim[1]*dim[2]
#print(num_points)

data=[]
for i in variables:
  vtk_array=point_data.GetArray(i)
  arr=vtk_to_numpy(vtk_array)
  data.append(arr)
#print(data)

def scale(arr):
   min = np.min(arr)
   max = np.max(arr)
   arr = (arr - min) / (max - min)   # Scale to [0, 1]
   arr = 2 * (arr - 0.5)             # Scale to [-1, 1]
   return arr

x = scale(np.array(range(dim[0])))
y = scale(np.array(range(dim[1])))
z = scale(np.array(range(dim[2])))

print(np.min(x), np.max(x))

# Scaled coordinates already in x, y, z (each 1D, scaled to [-1, 1])
loc = []
for i in z:
    for j in y:
        for k in x:
            loc.append([k, j, i])  # [x, y, z] order
loc = np.array(loc)
print(loc.shape)  #



'''normalizing the values'''
n_pts = image_data.GetNumberOfPoints()
scaled_data = data.copy()
real_data=data.copy()
for i in range(9):  # 0-5: means and stds
    min_val = np.min(scaled_data[i])
    max_val = np.max(scaled_data[i])
    scaled_data[i] = (scaled_data[i] - min_val) / (max_val - min_val)  # [0,1]
    scaled_data[i] = 2 * (scaled_data[i] - 0.5)  # [-1,1]
scaled_data = np.array(scaled_data).T
scaled_data.shape

t_data=torch.from_numpy(scaled_data)
t_loc=torch.from_numpy(loc)

print('Dataset Name:', dataset_name,flush=True)
print('Total Variables:', no_of_arrays,flush=True)
print('Variables Name:', variables, end="\n\n",flush=True)
print('Total Points in Data:', num_points,flush=True)
print('Dimension of the Dataset:', dim,flush=True)
print('Number of Dimensions:',len(dim),flush=True)
print('Coordinate Tensor Shape:', t_loc.shape,flush=True)
print('Scalar Values Tensor Shape:', t_data.shape,flush=True)

print('\n###### Data setup is complete, now starting training ######\n',flush=True)
import random
random.seed(42)
torch.manual_seed(42)
np.random.seed(42)



train_dataloader = DataLoader(
    TensorDataset(t_loc, t_data),
    batch_size=BATCH_SIZE,
    pin_memory=True,
    shuffle=True,
    num_workers=0
)

obj = {
    'total_vars': no_of_arrays,
    'dim': len(dim),
    'n_neurons': n_neurons,
    'n_layers': n_layers
}
print(obj)
model=MyResidualSirenNet(obj).to(device)
print(model)

optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999))
print(optimizer)

criterion = nn.MSELoss()
print(criterion)

group_size = 5000
univariate=None

output_groups = {
    'gmm_means': [0, 1, 2],
    'gmm_stds': [3, 4, 5],
    'gmm_weights': [6, 7, 8]
}

best_losses = {k: 1e8 for k in output_groups}
best_epochs = {k: -1 for k in output_groups}
models = {}
optimizers = {}
train_loss_logs = {k: [] for k in output_groups}

# Model constructor
def create_model():
    obj = {
        'total_vars': 3,
        'dim': 3,  # x, y, z
        'n_neurons': n_neurons,
        'n_layers': n_layers
    }
    return MyResidualSirenNet(obj).to(device)

# Initialize output path
if not os.path.exists(outpath):
    os.makedirs(outpath)

# Train separate models for each component group
for name, indices in output_groups.items():
    print(f"\nTraining model for: {name.upper()}",flush=True)
    model = create_model()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    train_loss_list = train_loss_logs[name]

    for epoch in tqdm(range(MAX_EPOCH)):
        model.train()
        temp_loss_list = []
        start = time.time()

        for X_train, y_train in train_dataloader:
            X_train = X_train.float().to(device)
            y_train = y_train.float().to(device)

            y_target = y_train[:, indices]

            optimizer.zero_grad()
            predictions = model(X_train).squeeze()
            loss = criterion(predictions, y_target)
            loss.backward()
            optimizer.step()

            temp_loss_list.append(loss.detach().cpu().numpy())

        epoch_loss = np.mean(temp_loss_list)
        train_loss_list.append(epoch_loss)

           # Learning rate decay
        if decay:
            if decay_at_equal_interval:
                if epoch >= decay_interval and epoch % decay_interval == 0:
                    for param_group in optimizer.param_groups:
                        param_group['lr'] *= decay_rate
            else:
            #     if epoch > 0 and epoch_loss > train_loss_list[-1]:
            #         for param_group in optimizer.param_groups:
            #             param_group['lr'] *= decay_rate
                if epoch > 0 and epoch_loss > train_loss_list[-1]:
                    for param_group in optimizer.param_groups:
                        param_group['lr'] *= decay_rate

        # Save best model
        if epoch_loss < best_losses[name]:
            best_losses[name] = epoch_loss
            best_epochs[name] = epoch + 1
            torch.save(
                {"epoch": epoch + 1, "model_state_dict": model.state_dict()},
                os.path.join(outpath, f'{name}_best_model.pth')
            )

        # Log progress
        end = time.time()
        print(f"Epoch {epoch+1}/{MAX_EPOCH} | {name.upper()} Loss: {epoch_loss:.6f} | "
              f"Time: {round(end - start, 2)}s | LR: {optimizer.param_groups[0]['lr']:.6e}",flush=True)

# Final summary
for name in output_groups:
    print(f"\nBest {name.upper()} model at epoch {best_epochs[name]} with loss {best_losses[name]:.6f}",flush=True)

# Load and prepare models
output_groups = {
    'gmm_means': [0, 1, 2],
    'gmm_stds': [3, 4, 5],
    'gmm_weights': [6, 7, 8]
}

prediction_dict = {}

# Define model creation for loading
def create_model():
    obj = {
        'total_vars': 3,
        'dim': 3,
        'n_neurons': n_neurons,
        'n_layers': n_layers
    }
    return MyResidualSirenNet(obj).to(device)





# Inference loop
with torch.no_grad():
    for name in output_groups:
        print(f"\nRunning inference for: {name.upper()}",flush=True)

        # Load best model
        model = create_model()
        model_path = os.path.join(outpath, f"{name}_best_model.pth")
        model.load_state_dict(torch.load(model_path)['model_state_dict'])
        model.eval()

        predictions = []

        # Batch-wise prediction on coordinates
        for i in range(0, t_loc.shape[0], group_size):
            coords = t_loc[i:min(i + group_size, t_loc.shape[0])].to(device).float()
            output = model(coords).to('cpu')
            predictions.append(output)

        # Concatenate predictions
        prediction_dict[name] = torch.cat(predictions, dim=0).numpy()

# Combine all into final output
means_pred = prediction_dict['gmm_means']    # shape (N, 3)
stds_pred = prediction_dict['gmm_stds']      # shape (N, 3)
weights_pred = prediction_dict['gmm_weights']# shape (N, 3)

# Final combined prediction: (N, 9)
n_predictions = np.concatenate([means_pred, stds_pred, weights_pred], axis=1)

# Evaluation
total_vars = 9
#def findMultiVariatePSNR(var_name, total_vars, actual, pred):
findMultiVariatePSNR(variables, total_vars,scaled_data, n_predictions)
rmse = compute_rmse(scaled_data, n_predictions)
print("Final RMSE:", rmse,flush=True)
def getImageData(actual_img, val, n_pts, var_name, isMaskPresent, mask_arr):
    img = vtkImageData()
    img.CopyStructure(actual_img)
    # if isMaskPresent:
    #     img.DeepCopy(actual_img)
    # img.SetDimensions(dim)
    # img.SetOrigin(actual_img.GetOrigin())
    # img.SetSpacing(actual_img.GetSpacing())
    if not isMaskPresent:
        f = var_name
        data = val
        arr = vtkFloatArray()
        for j in range(n_pts):
            arr.InsertNextValue(data[j])
        arr.SetName(f)
        img.GetPointData().SetScalars(arr)
    else:
        f = var_name
        data = val
        idx = 0
        arr = vtkFloatArray()
        for j in range(n_pts):
            if(mask_arr[j] == 1):
                arr.InsertNextValue(data[idx])
                idx += 1
            else:
                arr.InsertNextValue(0.0)
        arr.SetName(f)
        img.GetPointData().SetScalars(arr)
    return img



# vti saving path
vti_path = args.vti_path
if not os.path.exists(vti_path):
    os.makedirs(vti_path)
# vti name
isMaskPresent=False
mask_arr=[]
vti_name = "gmm_"+f"{file_name}"+args.vti_name
real_data=np.array(real_data)
predicted_gmm_image_data=makeVTI(image_data, real_data, n_predictions, n_pts, total_vars, variables, dim, isMaskPresent, mask_arr, vti_path, vti_name)
print("7. end of trainning sirenet on gmm dataset and then precdiction")
if __name__ == '__main__':
    print("8.genration of isosuface on predicted gmm dataset")
    curr_file=f"gmm_predicted_{file_name}.vti"
    process_gmm(curr_file,predicted_gmm_image_data)
    print("8. end of genration of isosuface on predicted gmm dataset")

