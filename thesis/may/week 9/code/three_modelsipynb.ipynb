{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install vtk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1lQKiz_3paj",
        "outputId": "84a78523-5e48-4f5f-c9a0-83a809c16b1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vtk in /usr/local/lib/python3.11/dist-packages (9.5.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from vtk) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->vtk) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->vtk) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ek-wSRJ42New",
        "outputId": "5d6344e9-714d-4d8c-9f39-b8c32073c405"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2362672935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2149\u001b[0m )\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m from torch import (\n\u001b[0m\u001b[1;32m   2152\u001b[0m     \u001b[0m__config__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__config__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0m__future__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__future__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \"\"\"\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m from torch.distributions.utils import (\n\u001b[1;32m     13\u001b[0m     \u001b[0m_sum_rightmost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import vtk\n",
        "from vtk import *\n",
        "from vtk.util.numpy_support import vtk_to_numpy\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print('Device running:', device)\n",
        "\n",
        "class SineLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        # self.enable_dropout = enable_dropout\n",
        "        # self.dropout_prob = dropout_prob\n",
        "        self.in_features = in_features\n",
        "        # if enable_dropout:\n",
        "        #     if not self.is_first:\n",
        "        #         self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features,\n",
        "                                             1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        # if self.enable_dropout:\n",
        "        #     if not self.is_first:\n",
        "        #         x = self.dropout(x)\n",
        "        return torch.sin(self.omega_0 * x)\n",
        "\n",
        "class ResidualSineLayer(nn.Module):\n",
        "    def __init__(self, features, bias=True, ave_first=False, ave_second=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        # self.enable_dropout = enable_dropout\n",
        "        # self.dropout_prob = dropout_prob\n",
        "        self.features = features\n",
        "        # if enable_dropout:\n",
        "        #     self.dropout_1 = nn.Dropout(dropout_prob)\n",
        "        self.linear_1 = nn.Linear(features, features, bias=bias)\n",
        "        self.linear_2 = nn.Linear(features, features, bias=bias)\n",
        "        self.weight_1 = .5 if ave_first else 1\n",
        "        self.weight_2 = .5 if ave_second else 1\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            self.linear_1.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n",
        "                                           np.sqrt(6 / self.features) / self.omega_0)\n",
        "            self.linear_2.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n",
        "                                           np.sqrt(6 / self.features) / self.omega_0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        linear_1 = self.linear_1(self.weight_1*input)\n",
        "        # if self.enable_dropout:\n",
        "        #     linear_1 = self.dropout_1(linear_1)\n",
        "        sine_1 = torch.sin(self.omega_0 * linear_1)\n",
        "        sine_2 = torch.sin(self.omega_0 * self.linear_2(sine_1))\n",
        "        return self.weight_2*(input+sine_2)\n",
        "\n",
        "class MyResidualSirenNet(nn.Module):\n",
        "    def __init__(self, obj):\n",
        "        super(MyResidualSirenNet, self).__init__()\n",
        "        # self.enable_dropout = obj['enable_dropout']\n",
        "        # self.dropout_prob = obj['dropout_prob']\n",
        "        self.Omega_0=30\n",
        "        self.n_layers = obj['n_layers']\n",
        "        self.input_dim = obj['dim']\n",
        "        self.output_dim = obj['total_vars']\n",
        "        self.neurons_per_layer = obj['n_neurons']\n",
        "        self.layers = [self.input_dim]\n",
        "        for i in range(self.n_layers-1):\n",
        "            self.layers.append(self.neurons_per_layer)\n",
        "        self.layers.append(self.output_dim)\n",
        "        self.net_layers = nn.ModuleList()\n",
        "        for idx in np.arange(self.n_layers):\n",
        "            layer_in = self.layers[idx]\n",
        "            layer_out = self.layers[idx+1]\n",
        "            ## if not the final layer\n",
        "            if idx != self.n_layers-1:\n",
        "                ## if first layer\n",
        "                if idx==0:\n",
        "                    self.net_layers.append(SineLayer(layer_in,layer_out,bias=True,is_first=idx==0))\n",
        "                ## if an intermdeiate layer\n",
        "                else:\n",
        "                    self.net_layers.append(ResidualSineLayer(layer_in,bias=True,ave_first=idx>1,ave_second=idx==(self.n_layers-2)))\n",
        "            ## if final layer\n",
        "            else:\n",
        "                final_linear = nn.Linear(layer_in,layer_out)\n",
        "                ## initialize weights for the final layer\n",
        "                with torch.no_grad():\n",
        "                    final_linear.weight.uniform_(-np.sqrt(6 / (layer_in)) / self.Omega_0, np.sqrt(6 / (layer_in)) / self.Omega_0)\n",
        "                self.net_layers.append(final_linear)\n",
        "\n",
        "    def forward(self,x):\n",
        "        for net_layer in self.net_layers:\n",
        "            x = net_layer(x)\n",
        "        return x\n",
        "\n",
        "def size_of_network(n_layers, n_neurons, d_in, d_out, is_residual = True):\n",
        "    # Adding input layer\n",
        "    layers = [d_in]\n",
        "    # layers = [3]\n",
        "\n",
        "    # Adding hidden layers\n",
        "    layers.extend([n_neurons]*n_layers)\n",
        "    # layers = [3, 5, 5, 5]\n",
        "\n",
        "    # Adding output layer\n",
        "    layers.append(d_out)\n",
        "    # layers = [3, 5, 5, 5, 1]\n",
        "\n",
        "    # Number of steps\n",
        "    n_layers = len(layers)-1\n",
        "    # n_layers = 5 - 1 = 4\n",
        "\n",
        "    n_params = 0\n",
        "\n",
        "    # np.arange(4) = [0, 1, 2, 3]\n",
        "    for ndx in np.arange(n_layers):\n",
        "\n",
        "        # number of neurons in below layer\n",
        "        layer_in = layers[ndx]\n",
        "\n",
        "        # number of neurons in above layer\n",
        "        layer_out = layers[ndx+1]\n",
        "\n",
        "        # max number of neurons in both the layer\n",
        "        og_layer_in = max(layer_in,layer_out)\n",
        "\n",
        "        # if lower layer is the input layer\n",
        "        # or the upper layer is the output layer\n",
        "        if ndx==0 or ndx==(n_layers-1):\n",
        "            # Adding weight corresponding to every neuron for every input neuron\n",
        "            # Adding bias for every neuron in the upper layer\n",
        "            n_params += ((layer_in+1)*layer_out)\n",
        "\n",
        "        else:\n",
        "\n",
        "            # If the layer is residual then proceed as follows as there will be more weights if residual layer is included\n",
        "            if is_residual:\n",
        "                # doubt in the following two lines\n",
        "                n_params += (layer_in*og_layer_in)+og_layer_in\n",
        "                n_params += (og_layer_in*layer_out)+layer_out\n",
        "\n",
        "            # if the layer is non residual then simply add number of weights and biases as follows\n",
        "            else:\n",
        "                n_params += ((layer_in+1)*layer_out)\n",
        "            #\n",
        "        #\n",
        "    #\n",
        "\n",
        "    return n_params\n",
        "\n",
        "def compute_PSNR(arrgt,arr_recon):\n",
        "    diff = arrgt - arr_recon\n",
        "    sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n",
        "    snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n",
        "    return snr\n",
        "\n",
        "def findMultiVariatePSNR(var_name, total_vars, actual, pred):\n",
        "    # print('Printing PSNR')\n",
        "    tot = 0\n",
        "    psnr_list = []\n",
        "    for j in range(total_vars):\n",
        "        psnr = compute_PSNR(actual[:,j], pred[:,j])\n",
        "        psnr_list.append(psnr)\n",
        "        tot += psnr\n",
        "        print(var_name[j], ' PSNR:', psnr)\n",
        "    avg_psnr = tot/total_vars\n",
        "    print('\\nAverage psnr : ', avg_psnr)\n",
        "     #this function is calculating the psnr of final epoch (or whenever it is called) of each variable and then averaging it\n",
        "     #Thus individual epochs psnr is not calculated\n",
        "\n",
        "    return psnr_list, avg_psnr\n",
        "\n",
        "def compute_rmse(actual, predicted):\n",
        "    mse = np.mean((actual - predicted) ** 2)\n",
        "    return np.sqrt(mse)\n",
        "\n",
        "def denormalizeValue(total_vars, to, ref):\n",
        "    to_arr = np.array(to)\n",
        "    for i in range(total_vars):\n",
        "        min_data = np.min(ref[:, i])\n",
        "        max_data = np.max(ref[:, i])\n",
        "        to_arr[:, i] = (((to[:, i] * 0.5) + 0.5) * (max_data - min_data)) + min_data\n",
        "    return to_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTRez2_aiXku",
        "outputId": "cafbcddf-a7ac-4366-aa59-e0be021b1f99"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device running: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# Parameters (simulating argparse in a Jupyter Notebook)\n",
        "args = Namespace(\n",
        "    n_neurons=150,\n",
        "    n_layers=6,\n",
        "    epochs=200,  # Required argument: Set the number of epochs\n",
        "    batchsize=512,\n",
        "    lr=0.00005,\n",
        "    no_decay=False,\n",
        "    decay_rate=0.8,\n",
        "    decay_at_interval=True,\n",
        "    decay_interval=15,\n",
        "    datapath='/content/new_gmm_teardrop_week_9.vti',  # Required: Set the path to your data\n",
        "    outpath='./models/',\n",
        "    exp_path='../logs/',\n",
        "    modified_data_path='./data/',\n",
        "    dataset_name='3d_data',  # Required: Set the dataset name\n",
        "    vti_name='new_Mine_teardrop_gmm_predicted_week9_150_three_models.vti',  # Required: Name of the dataset\n",
        "    vti_path='./data/'\n",
        ")\n",
        "\n",
        "print(args, end='\\n\\n')\n",
        "\n",
        "# Assigning parameters to variables\n",
        "LR = args.lr\n",
        "BATCH_SIZE = args.batchsize\n",
        "decay_rate = args.decay_rate\n",
        "decay_at_equal_interval = args.decay_at_interval\n",
        "\n",
        "decay = not args.no_decay\n",
        "MAX_EPOCH = args.epochs\n",
        "\n",
        "n_neurons = args.n_neurons\n",
        "n_layers = args.n_layers + 2\n",
        "decay_interval = args.decay_interval\n",
        "outpath = args.outpath\n",
        "exp_path = args.exp_path\n",
        "datapath = args.datapath\n",
        "modified_data_path = args.modified_data_path\n",
        "dataset_name = args.dataset_name\n",
        "vti_name = args.vti_name\n",
        "vti_path = args.vti_path\n",
        "\n",
        "# Displaying the final configuration\n",
        "print(f\"Learning Rate: {LR}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Decay Rate: {decay_rate}\")\n",
        "print(f\"Max Epochs: {MAX_EPOCH}\")\n",
        "print(f\"Number of Neurons per Layer: {n_neurons}\")\n",
        "print(f\"Number of Layers (including input/output): {n_layers}\")\n",
        "print(f\"Data Path: {datapath}\")\n",
        "print(f\"Output Path: {outpath}\")\n",
        "print(f\"Dataset Name: {dataset_name}\")\n",
        "print(f\"Vti Name: {vti_name}\")\n",
        "\n",
        "#datasetup\n",
        "reader=vtk.vtkXMLImageDataReader()\n",
        "reader.SetFileName(datapath)\n",
        "reader.update()\n",
        "image_data=reader.GetOutput()\n",
        "point_data=image_data.GetPointData()\n",
        "no_of_arrays=point_data.GetNumberOfArrays()\n",
        "variables=[]\n",
        "for i in range(no_of_arrays):\n",
        "  array=point_data.GetArrayName(i)\n",
        "  variables.append(array)\n",
        "variables\n",
        "\n",
        "origin=image_data.GetOrigin()\n",
        "dim=image_data.GetDimensions()\n",
        "spacing=image_data.GetSpacing()\n",
        "num_points=dim[0]*dim[1]*dim[2]\n",
        "print(num_points)\n",
        "\n",
        "data=[]\n",
        "for i in variables:\n",
        "  vtk_array=point_data.GetArray(i)\n",
        "  arr=vtk_to_numpy(vtk_array)\n",
        "  data.append(arr)\n",
        "print(data)\n",
        "\n",
        "def scale(arr):\n",
        "   min = np.min(arr)\n",
        "   max = np.max(arr)\n",
        "   arr = (arr - min) / (max - min)   # Scale to [0, 1]\n",
        "   arr = 2 * (arr - 0.5)             # Scale to [-1, 1]\n",
        "   return arr\n",
        "\n",
        "x = scale(np.array(range(dim[0])))\n",
        "y = scale(np.array(range(dim[1])))\n",
        "z = scale(np.array(range(dim[2])))\n",
        "\n",
        "print(np.min(x), np.max(x))\n",
        "\n",
        "# Scaled coordinates already in x, y, z (each 1D, scaled to [-1, 1])\n",
        "loc = []\n",
        "for i in z:\n",
        "    for j in y:\n",
        "        for k in x:\n",
        "            loc.append([k, j, i])  # [x, y, z] order\n",
        "loc = np.array(loc)\n",
        "print(loc.shape)  #\n",
        "\n",
        "n_pts = image_data.GetNumberOfPoints()\n",
        "scaled_data = data.copy()\n",
        "real_data=data.copy()\n",
        "for i in range(9):  # 0-5: means and stds\n",
        "    min_val = np.min(scaled_data[i])\n",
        "    max_val = np.max(scaled_data[i])\n",
        "    scaled_data[i] = (scaled_data[i] - min_val) / (max_val - min_val)  # [0,1]\n",
        "    scaled_data[i] = 2 * (scaled_data[i] - 0.5)  # [-1,1]\n",
        "scaled_data = np.array(scaled_data).T\n",
        "scaled_data.shape\n",
        "\n",
        "t_data=torch.from_numpy(scaled_data)\n",
        "t_loc=torch.from_numpy(loc)\n",
        "\n",
        "print('Dataset Name:', dataset_name)\n",
        "print('Total Variables:', no_of_arrays)\n",
        "print('Variables Name:', variables, end=\"\\n\\n\")\n",
        "print('Total Points in Data:', num_points)\n",
        "print('Dimension of the Dataset:', dim)\n",
        "print('Number of Dimensions:',len(dim))\n",
        "print('Coordinate Tensor Shape:', t_loc.shape)\n",
        "print('Scalar Values Tensor Shape:', t_data.shape)\n",
        "\n",
        "print('\\n###### Data setup is complete, now starting training ######\\n')\n",
        "import random\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "LISqpLooieHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "    TensorDataset(t_loc, t_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    pin_memory=True,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "obj = {\n",
        "    'total_vars': no_of_arrays,\n",
        "    'dim': len(dim),\n",
        "    'n_neurons': n_neurons,\n",
        "    'n_layers': n_layers\n",
        "}\n",
        "print(obj)\n",
        "model=MyResidualSirenNet(obj).to(device)\n",
        "print(model)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999))\n",
        "print(optimizer)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "print(criterion)\n",
        "\n",
        "group_size = 5000\n",
        "univariate=None\n",
        "\n",
        "output_groups = {\n",
        "    'means': [0, 1, 2],\n",
        "    'stds': [3, 4, 5],\n",
        "    'weights': [6, 7, 8]\n",
        "}\n",
        "\n",
        "best_losses = {k: 1e8 for k in output_groups}\n",
        "best_epochs = {k: -1 for k in output_groups}\n",
        "models = {}\n",
        "optimizers = {}\n",
        "train_loss_logs = {k: [] for k in output_groups}\n",
        "\n",
        "# Model constructor\n",
        "def create_model():\n",
        "    obj = {\n",
        "        'total_vars': 3,\n",
        "        'dim': 3,  # x, y, z\n",
        "        'n_neurons': n_neurons,\n",
        "        'n_layers': n_layers\n",
        "    }\n",
        "    return MyResidualSirenNet(obj).to(device)\n",
        "\n",
        "# Initialize output path\n",
        "if not os.path.exists(outpath):\n",
        "    os.makedirs(outpath)\n"
      ],
      "metadata": {
        "id": "fgVF4mHbinWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train separate models for each component group\n",
        "for name, indices in output_groups.items():\n",
        "    print(f\"\\nTraining model for: {name.upper()}\")\n",
        "    model = create_model()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    train_loss_list = train_loss_logs[name]\n",
        "    if(name=='means'):\n",
        "        MAX_EPOCH=200\n",
        "    else:\n",
        "        MAX_EPOCH=200\n",
        "\n",
        "    for epoch in tqdm(range(MAX_EPOCH)):\n",
        "        model.train()\n",
        "        temp_loss_list = []\n",
        "        start = time.time()\n",
        "\n",
        "        for X_train, y_train in train_dataloader:\n",
        "            X_train = X_train.float().to(device)\n",
        "            y_train = y_train.float().to(device)\n",
        "\n",
        "            y_target = y_train[:, indices]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(X_train).squeeze()\n",
        "            loss = criterion(predictions, y_target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            temp_loss_list.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        epoch_loss = np.mean(temp_loss_list)\n",
        "        train_loss_list.append(epoch_loss)\n",
        "\n",
        "        # Learning rate decay\n",
        "        if decay:\n",
        "            if decay_at_equal_interval and epoch >= decay_interval and epoch % decay_interval == 0:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= decay_rate\n",
        "            elif epoch > 0 and epoch_loss > train_loss_list[-2]:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] *= decay_rate\n",
        "\n",
        "        # Save best model\n",
        "        if epoch_loss < best_losses[name]:\n",
        "            best_losses[name] = epoch_loss\n",
        "            best_epochs[name] = epoch + 1\n",
        "            torch.save(\n",
        "                {\"epoch\": epoch + 1, \"model_state_dict\": model.state_dict()},\n",
        "                os.path.join(outpath, f'{name}_best_model.pth')\n",
        "            )\n",
        "\n",
        "        # Log progress\n",
        "        end = time.time()\n",
        "        print(f\"Epoch {epoch+1}/{MAX_EPOCH} | {name.upper()} Loss: {epoch_loss:.6f} | \"\n",
        "              f\"Time: {round(end - start, 2)}s | LR: {optimizer.param_groups[0]['lr']:.6e}\")"
      ],
      "metadata": {
        "id": "7mMj7ytairRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final summary\n",
        "for name in output_groups:\n",
        "    print(f\"\\nBest {name.upper()} model at epoch {best_epochs[name]} with loss {best_losses[name]:.6f}\")\n",
        "\n",
        "# Load and prepare models\n",
        "output_groups = {\n",
        "    'means': [0, 1, 2],\n",
        "    'stds': [3, 4, 5],\n",
        "    'weights': [6, 7, 8]\n",
        "}\n",
        "\n",
        "prediction_dict = {}\n",
        "\n",
        "# Define model creation for loading\n",
        "def create_model():\n",
        "    obj = {\n",
        "        'total_vars': 3,\n",
        "        'dim': 3,\n",
        "        'n_neurons': n_neurons,\n",
        "        'n_layers': n_layers\n",
        "    }\n",
        "    return MyResidualSirenNet(obj).to(device)\n",
        "\n",
        "# Inference loop\n",
        "with torch.no_grad():\n",
        "    for name in output_groups:\n",
        "        print(f\"\\nRunning inference for: {name.upper()}\")\n",
        "\n",
        "        # Load best model\n",
        "        model = create_model()\n",
        "        model_path = os.path.join(outpath, f\"{name}_best_model.pth\")\n",
        "        model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        # Batch-wise prediction on coordinates\n",
        "        for i in range(0, t_loc.shape[0], group_size):\n",
        "            coords = t_loc[i:min(i + group_size, t_loc.shape[0])].to(device).float()\n",
        "            output = model(coords).to('cpu')\n",
        "            predictions.append(output)\n",
        "\n",
        "        # Concatenate predictions\n",
        "        prediction_dict[name] = torch.cat(predictions, dim=0).numpy()\n",
        "\n",
        "# Combine all into final output\n",
        "means_pred = prediction_dict['means']    # shape (N, 3)\n",
        "stds_pred = prediction_dict['stds']      # shape (N, 3)\n",
        "weights_pred = prediction_dict['weights']# shape (N, 3)\n",
        "\n",
        "# Final combined prediction: (N, 9)\n",
        "n_predictions = np.concatenate([means_pred, stds_pred, weights_pred], axis=1)\n",
        "\n",
        "# Evaluation\n",
        "total_vars = 9\n",
        "findMultiVariatePSNR(variables, total_vars,scaled_data, n_predictions)\n",
        "rmse = compute_rmse(scaled_data, n_predictions)\n",
        "print(\"Final RMSE:\", rmse)\n",
        "\n"
      ],
      "metadata": {
        "id": "bi0NOPL_iYWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeVTI(data, val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name, normalizedVersion = False):\n",
        "    nn_predictions = denormalizeValue(total_vars, n_predictions, val) if not normalizedVersion else n_predictions\n",
        "    writer = vtkXMLImageDataWriter()\n",
        "    writer.SetFileName(vti_path + vti_name)\n",
        "    img = vtkImageData()\n",
        "    img.CopyStructure(data)\n",
        "    if not isMaskPresent:\n",
        "        for i in range(total_vars):\n",
        "            f = var_name[i]\n",
        "            temp = nn_predictions[:, i]\n",
        "            arr = vtkFloatArray()\n",
        "            for j in range(n_pts):\n",
        "                arr.InsertNextValue(temp[j])\n",
        "            arr.SetName(f)\n",
        "            img.GetPointData().AddArray(arr)\n",
        "        # print(img)\n",
        "        writer.SetInputData(img)\n",
        "        writer.Write()\n",
        "        print(f'Vti File written successfully at {vti_path}{vti_name}')\n",
        "    else:\n",
        "        for i in range(total_vars):\n",
        "            f = var_name[i]\n",
        "            temp = nn_predictions[:, i]\n",
        "            idx = 0\n",
        "            arr = vtkFloatArray()\n",
        "            for j in range(n_pts):\n",
        "                if(mask_arr[j] == 1):\n",
        "                    arr.InsertNextValue(temp[idx])\n",
        "                    idx += 1\n",
        "                else:\n",
        "                    arr.InsertNextValue(0.0)\n",
        "            arr.SetName('p_' + f)\n",
        "            data.GetPointData().AddArray(arr)\n",
        "        # print(data)\n",
        "        writer.SetInputData(data)\n",
        "        writer.Write()\n",
        "        print(f'Vti File written successfully at {vti_path}{vti_name}')"
      ],
      "metadata": {
        "id": "WHSjcOyI2hsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalizeValue(total_vars, to, ref):\n",
        "    to_arr = np.array(to)\n",
        "    for i in range(total_vars):\n",
        "        min_data = np.min(ref[:, i])\n",
        "        max_data = np.max(ref[:, i])\n",
        "        to_arr[:, i] = (((to[:, i] * 0.5) + 0.5) * (max_data - min_data)) + min_data\n",
        "    return to_arr"
      ],
      "metadata": {
        "id": "KwFKupQp271Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def makeVTI(data, val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name, normalizedVersion = False):\n",
        "    nn_predictions = denormalizeValue(total_vars, n_predictions, val) if not normalizedVersion else n_predictions\n",
        "    writer = vtkXMLImageDataWriter()\n",
        "    writer.SetFileName(vti_path + vti_name)\n",
        "    img = vtkImageData()\n",
        "    img.CopyStructure(data)\n",
        "    if not isMaskPresent:\n",
        "        for i in range(total_vars):\n",
        "            f = var_name[i]\n",
        "            temp = nn_predictions[:, i]\n",
        "            arr = vtkFloatArray()\n",
        "            for j in range(n_pts):\n",
        "                arr.InsertNextValue(temp[j])\n",
        "            arr.SetName(f)\n",
        "            img.GetPointData().AddArray(arr)\n",
        "        # print(img)\n",
        "        writer.SetInputData(img)\n",
        "        writer.Write()\n",
        "        print(f'Vti File written successfully at {vti_path}{vti_name}')\n",
        "    else:\n",
        "        for i in range(total_vars):\n",
        "            f = var_name[i]\n",
        "            temp = nn_predictions[:, i]\n",
        "            idx = 0\n",
        "            arr = vtkFloatArray()\n",
        "            for j in range(n_pts):\n",
        "                if(mask_arr[j] == 1):\n",
        "                    arr.InsertNextValue(temp[idx])\n",
        "                    idx += 1\n",
        "                else:\n",
        "                    arr.InsertNextValue(0.0)\n",
        "            arr.SetName('p_' + f)\n",
        "            data.GetPointData().AddArray(arr)\n",
        "        # print(data)\n",
        "        writer.SetInputData(data)\n",
        "        writer.Write()\n",
        "        print(f'Vti File written successfully at {vti_path}{vti_name}')"
      ],
      "metadata": {
        "id": "vvRDmrEY2xnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImageData(actual_img, val, n_pts, var_name, isMaskPresent, mask_arr):\n",
        "    img = vtkImageData()\n",
        "    img.CopyStructure(actual_img)\n",
        "    # if isMaskPresent:\n",
        "    #     img.DeepCopy(actual_img)\n",
        "    # img.SetDimensions(dim)\n",
        "    # img.SetOrigin(actual_img.GetOrigin())\n",
        "    # img.SetSpacing(actual_img.GetSpacing())\n",
        "    if not isMaskPresent:\n",
        "        f = var_name\n",
        "        data = val\n",
        "        arr = vtkFloatArray()\n",
        "        for j in range(n_pts):\n",
        "            arr.InsertNextValue(data[j])\n",
        "        arr.SetName(f)\n",
        "        img.GetPointData().SetScalars(arr)\n",
        "    else:\n",
        "        f = var_name\n",
        "        data = val\n",
        "        idx = 0\n",
        "        arr = vtkFloatArray()\n",
        "        for j in range(n_pts):\n",
        "            if(mask_arr[j] == 1):\n",
        "                arr.InsertNextValue(data[idx])\n",
        "                idx += 1\n",
        "            else:\n",
        "                arr.InsertNextValue(0.0)\n",
        "        arr.SetName(f)\n",
        "        img.GetPointData().SetScalars(arr)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "IfuUbTg82jx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vti saving path\n",
        "vti_path = args.vti_path\n",
        "if not os.path.exists(vti_path):\n",
        "    os.makedirs(vti_path)\n",
        "# vti name\n",
        "isMaskPresent=False\n",
        "mask_arr=[]\n",
        "vti_name = args.vti_name\n",
        "real_data=np.array(real_data)\n",
        "makeVTI(image_data, real_data, n_predictions, n_pts, total_vars, variables, dim, isMaskPresent, mask_arr, vti_path, vti_name)"
      ],
      "metadata": {
        "id": "aOLIo3CD3O1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b852c99-d36b-412f-a34f-31ec01a81558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vti File written successfully at ./data/new_Mine_teardrop_gmm_predicted_week9_150_three_models.vti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8ndZBIfVU8t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}