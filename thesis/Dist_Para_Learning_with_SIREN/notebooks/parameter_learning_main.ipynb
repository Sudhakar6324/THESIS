{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10578389,"sourceType":"datasetVersion","datasetId":6546447},{"sourceId":11041818,"sourceType":"datasetVersion","datasetId":6878023},{"sourceId":11251078,"sourceType":"datasetVersion","datasetId":7030699},{"sourceId":11389139,"sourceType":"datasetVersion","datasetId":7132144},{"sourceId":11421576,"sourceType":"datasetVersion","datasetId":7153051}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn, optim\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport vtk\nfrom vtk import *\nfrom vtk.util.numpy_support import vtk_to_numpy\nimport random\nimport os\nimport sys\nimport time","metadata":{"_uuid":"d33f75d4-5445-4e48-8179-f0c5dd333f79","_cell_guid":"240889c7-74eb-4fd4-a5b1-ec2aead81dac","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:13.755511Z","iopub.execute_input":"2025-05-26T20:25:13.755738Z","iopub.status.idle":"2025-05-26T20:25:19.575599Z","shell.execute_reply.started":"2025-05-26T20:25:13.755716Z","shell.execute_reply":"2025-05-26T20:25:19.574610Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint('Device running:', device)","metadata":{"_uuid":"43d958cf-562d-4fb8-9670-43075fd82174","_cell_guid":"bda30f3b-5c85-404f-8cdc-861e65b7ae69","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.576562Z","iopub.execute_input":"2025-05-26T20:25:19.576994Z","iopub.status.idle":"2025-05-26T20:25:19.654026Z","shell.execute_reply.started":"2025-05-26T20:25:19.576972Z","shell.execute_reply":"2025-05-26T20:25:19.653204Z"}},"outputs":[{"name":"stdout","text":"Device running: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class SineLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n        super().__init__()\n        self.omega_0 = omega_0\n        self.is_first = is_first\n        # self.enable_dropout = enable_dropout\n        # self.dropout_prob = dropout_prob\n        self.in_features = in_features\n        # if enable_dropout:\n        #     if not self.is_first:\n        #         self.dropout = nn.Dropout(dropout_prob)\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n        self.init_weights()\n\n    def init_weights(self):\n        with torch.no_grad():\n            if self.is_first:\n                self.linear.weight.uniform_(-1 / self.in_features,\n                                             1 / self.in_features)\n            else:\n                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n                                             np.sqrt(6 / self.in_features) / self.omega_0)\n\n\n    def forward(self, x):\n        x = self.linear(x)\n        # if self.enable_dropout:\n        #     if not self.is_first:\n        #         x = self.dropout(x)\n        return torch.sin(self.omega_0 * x)","metadata":{"_uuid":"c6d00f8a-3e62-40d1-81e8-b30fd44e994f","_cell_guid":"47639dbe-7c29-4716-ad12-c245edbd4925","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.654942Z","iopub.execute_input":"2025-05-26T20:25:19.655223Z","iopub.status.idle":"2025-05-26T20:25:19.667030Z","shell.execute_reply.started":"2025-05-26T20:25:19.655199Z","shell.execute_reply":"2025-05-26T20:25:19.666295Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class ResidualSineLayer(nn.Module):\n    def __init__(self, features, bias=True, ave_first=False, ave_second=False, omega_0=30):\n        super().__init__()\n        self.omega_0 = omega_0\n        # self.enable_dropout = enable_dropout\n        # self.dropout_prob = dropout_prob\n        self.features = features\n        # if enable_dropout:\n        #     self.dropout_1 = nn.Dropout(dropout_prob)\n        self.linear_1 = nn.Linear(features, features, bias=bias)\n        self.linear_2 = nn.Linear(features, features, bias=bias)\n        self.weight_1 = .5 if ave_first else 1\n        self.weight_2 = .5 if ave_second else 1\n\n        self.init_weights()\n\n\n    def init_weights(self):\n        with torch.no_grad():\n            self.linear_1.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n                                           np.sqrt(6 / self.features) / self.omega_0)\n            self.linear_2.weight.uniform_(-np.sqrt(6 / self.features) / self.omega_0,\n                                           np.sqrt(6 / self.features) / self.omega_0)\n\n    def forward(self, input):\n        linear_1 = self.linear_1(self.weight_1*input)\n        # if self.enable_dropout:\n        #     linear_1 = self.dropout_1(linear_1)\n        sine_1 = torch.sin(self.omega_0 * linear_1)\n        sine_2 = torch.sin(self.omega_0 * self.linear_2(sine_1))\n        return self.weight_2*(input+sine_2)","metadata":{"_uuid":"286fbfd2-ba2c-4fbe-8ac6-cfebd4a21e55","_cell_guid":"ef7cdfa6-ccb1-405d-a306-c83170e479b4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.667767Z","iopub.execute_input":"2025-05-26T20:25:19.668045Z","iopub.status.idle":"2025-05-26T20:25:19.679335Z","shell.execute_reply.started":"2025-05-26T20:25:19.668025Z","shell.execute_reply":"2025-05-26T20:25:19.678679Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MyResidualSirenNet(nn.Module):\n    def __init__(self, obj):\n        super(MyResidualSirenNet, self).__init__()\n        # self.enable_dropout = obj['enable_dropout']\n        # self.dropout_prob = obj['dropout_prob']\n        self.Omega_0=30\n        self.n_layers = obj['n_layers']\n        self.input_dim = obj['dim']\n        self.output_dim = obj['total_vars']\n        self.neurons_per_layer = obj['n_neurons']\n        self.layers = [self.input_dim]\n        for i in range(self.n_layers-1):\n            self.layers.append(self.neurons_per_layer)\n        self.layers.append(self.output_dim)\n        self.net_layers = nn.ModuleList()\n        for idx in np.arange(self.n_layers):\n            layer_in = self.layers[idx]\n            layer_out = self.layers[idx+1]\n            ## if not the final layer\n            if idx != self.n_layers-1:\n                ## if first layer\n                if idx==0:\n                    self.net_layers.append(SineLayer(layer_in,layer_out,bias=True,is_first=idx==0))\n                ## if an intermdeiate layer\n                else:\n                    self.net_layers.append(ResidualSineLayer(layer_in,bias=True,ave_first=idx>1,ave_second=idx==(self.n_layers-2)))\n            ## if final layer   \n            else:\n                final_linear = nn.Linear(layer_in,layer_out)\n                ## initialize weights for the final layer\n                with torch.no_grad():\n                    final_linear.weight.uniform_(-np.sqrt(6 / (layer_in)) / self.Omega_0, np.sqrt(6 / (layer_in)) / self.Omega_0)\n                self.net_layers.append(final_linear)\n\n    def forward(self,x):\n        for net_layer in self.net_layers:\n            x = net_layer(x)\n        return x","metadata":{"_uuid":"e1400ad9-b3ed-49fc-81e0-929816a0aaf2","_cell_guid":"807361ca-2c71-4799-83d0-2da9ccf9fac0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.680067Z","iopub.execute_input":"2025-05-26T20:25:19.680289Z","iopub.status.idle":"2025-05-26T20:25:19.693684Z","shell.execute_reply.started":"2025-05-26T20:25:19.680270Z","shell.execute_reply":"2025-05-26T20:25:19.693049Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def size_of_network(n_layers, n_neurons, d_in, d_out, is_residual = True):\n    # Adding input layer\n    layers = [d_in]\n    # layers = [3]\n\n    # Adding hidden layers\n    layers.extend([n_neurons]*n_layers)\n    # layers = [3, 5, 5, 5]\n\n    # Adding output layer\n    layers.append(d_out)\n    # layers = [3, 5, 5, 5, 1]\n\n    # Number of steps \n    n_layers = len(layers)-1\n    # n_layers = 5 - 1 = 4\n\n    n_params = 0\n\n    # np.arange(4) = [0, 1, 2, 3]\n    for ndx in np.arange(n_layers):\n\n        # number of neurons in below layer\n        layer_in = layers[ndx]\n\n        # number of neurons in above layer\n        layer_out = layers[ndx+1]\n\n        # max number of neurons in both the layer\n        og_layer_in = max(layer_in,layer_out)\n\n        # if lower layer is the input layer \n        # or the upper layer is the output layer\n        if ndx==0 or ndx==(n_layers-1):\n            # Adding weight corresponding to every neuron for every input neuron\n            # Adding bias for every neuron in the upper layer\n            n_params += ((layer_in+1)*layer_out)\n        \n        else:\n\n            # If the layer is residual then proceed as follows as there will be more weights if residual layer is included\n            if is_residual:\n                # doubt in the following two lines\n                n_params += (layer_in*og_layer_in)+og_layer_in\n                n_params += (og_layer_in*layer_out)+layer_out\n\n            # if the layer is non residual then simply add number of weights and biases as follows\n            else:\n                n_params += ((layer_in+1)*layer_out)\n            #\n        #\n    #\n\n    return n_params","metadata":{"_uuid":"49e23bf9-3de6-417b-8de1-53aaf02fd323","_cell_guid":"7342b7cd-612f-4c56-ad94-ecd81203519a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.695847Z","iopub.execute_input":"2025-05-26T20:25:19.696059Z","iopub.status.idle":"2025-05-26T20:25:19.707241Z","shell.execute_reply.started":"2025-05-26T20:25:19.696041Z","shell.execute_reply":"2025-05-26T20:25:19.706562Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def compute_PSNR(arrgt,arr_recon):\n    diff = arrgt - arr_recon\n    sqd_max_diff = (np.max(arrgt)-np.min(arrgt))**2\n    snr = 10*np.log10(sqd_max_diff/np.mean(diff**2))\n    return snr","metadata":{"_uuid":"4bee2b5f-d501-4d1c-b226-410b2624c2d5","_cell_guid":"c8d04a55-6de3-4520-9cf7-45a568420b31","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.708470Z","iopub.execute_input":"2025-05-26T20:25:19.708697Z","iopub.status.idle":"2025-05-26T20:25:19.720894Z","shell.execute_reply.started":"2025-05-26T20:25:19.708678Z","shell.execute_reply":"2025-05-26T20:25:19.720185Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def srs(numOfPoints, valid_pts, percentage, isMaskPresent, mask_array):\n    \n    # getting total number of sampled points\n    numberOfSampledPoints = int((valid_pts/100) * percentage)\n\n    # storing corner indices in indices variable\n    indices = set()\n    \n    # As long as we don't get the required amount of sample points keep finding the random numbers\n    while(len(indices) < numberOfSampledPoints):\n        rp = random.randint(0, numOfPoints-1)\n        if isMaskPresent and mask_array[rp] == 0:\n            continue\n        indices.add(rp)\n\n    # return indices\n    return indices","metadata":{"_uuid":"9b219fbe-1596-4e3b-a0a7-6eb88db2f52d","_cell_guid":"415f4118-71de-4504-b39f-a7bd40cdcc02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.721825Z","iopub.execute_input":"2025-05-26T20:25:19.722096Z","iopub.status.idle":"2025-05-26T20:25:19.732038Z","shell.execute_reply.started":"2025-05-26T20:25:19.722064Z","shell.execute_reply":"2025-05-26T20:25:19.731249Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def findMultiVariatePSNR(var_name, total_vars, actual, pred):\n    # print('Printing PSNR')\n    tot = 0\n    psnr_list = []\n    for j in range(total_vars):\n        psnr = compute_PSNR(actual[:,j], pred[:,j])\n        psnr_list.append(psnr)\n        tot += psnr \n        print(var_name[j], ' PSNR:', psnr)\n    avg_psnr = tot/total_vars\n    print('\\nAverage psnr : ', avg_psnr)\n     #this function is calculating the psnr of final epoch (or whenever it is called) of each variable and then averaging it\n     #Thus individual epochs psnr is not calculated\n                                         \n    return psnr_list, avg_psnr","metadata":{"_uuid":"964befc2-020d-46d4-bd1b-ef061a65b9e8","_cell_guid":"c20e48d5-ebad-408b-b564-627ba81d02f9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.732800Z","iopub.execute_input":"2025-05-26T20:25:19.733052Z","iopub.status.idle":"2025-05-26T20:25:19.745670Z","shell.execute_reply.started":"2025-05-26T20:25:19.733032Z","shell.execute_reply":"2025-05-26T20:25:19.744868Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def compute_rmse(actual, predicted):\n    mse = np.mean((actual - predicted) ** 2)\n    return np.sqrt(mse)\n\ndef denormalizeValue(total_vars, to, ref):\n    to_arr = np.array(to)\n    for i in range(total_vars):\n        min_data = np.min(ref[:, i])\n        max_data = np.max(ref[:, i])\n        to_arr[:, i] = (((to[:, i] * 0.5) + 0.5) * (max_data - min_data)) + min_data\n    return to_arr","metadata":{"_uuid":"2face64a-1559-415e-9527-5a581fab1db5","_cell_guid":"716ccaed-b6c8-4cc4-83a6-643495093c02","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.746505Z","iopub.execute_input":"2025-05-26T20:25:19.746788Z","iopub.status.idle":"2025-05-26T20:25:19.755298Z","shell.execute_reply.started":"2025-05-26T20:25:19.746760Z","shell.execute_reply":"2025-05-26T20:25:19.754436Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def makeVTI(data, val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name, normalizedVersion = False):\n    nn_predictions = denormalizeValue(total_vars, n_predictions, val) if not normalizedVersion else n_predictions\n    writer = vtkXMLImageDataWriter()\n    writer.SetFileName(vti_path + vti_name)\n    img = vtkImageData()\n    img.CopyStructure(data)\n    if not isMaskPresent:\n        for i in range(total_vars):\n            f = var_name[i]\n            temp = nn_predictions[:, i]\n            arr = vtkFloatArray()\n            for j in range(n_pts):\n                arr.InsertNextValue(temp[j])\n            arr.SetName(f)\n            img.GetPointData().AddArray(arr)\n        # print(img)\n        writer.SetInputData(img)\n        writer.Write()\n        print(f'Vti File written successfully at {vti_path}{vti_name}')\n    else:\n        for i in range(total_vars):\n            f = var_name[i]\n            temp = nn_predictions[:, i]\n            idx = 0\n            arr = vtkFloatArray()\n            for j in range(n_pts):\n                if(mask_arr[j] == 1):\n                    arr.InsertNextValue(temp[idx])\n                    idx += 1\n                else:\n                    arr.InsertNextValue(0.0)\n            arr.SetName('p_' + f)\n            data.GetPointData().AddArray(arr)\n        # print(data)\n        writer.SetInputData(data)\n        writer.Write()\n        print(f'Vti File written successfully at {vti_path}{vti_name}')","metadata":{"_uuid":"88d4128a-9484-48bb-b8cd-21abe4801eb1","_cell_guid":"4d173df2-a1ab-442d-8eeb-bd0f1e71b9ee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.756183Z","iopub.execute_input":"2025-05-26T20:25:19.756430Z","iopub.status.idle":"2025-05-26T20:25:19.768524Z","shell.execute_reply.started":"2025-05-26T20:25:19.756410Z","shell.execute_reply":"2025-05-26T20:25:19.767693Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def getImageData(actual_img, val, n_pts, var_name, isMaskPresent, mask_arr):\n    img = vtkImageData()\n    img.CopyStructure(actual_img)\n    # if isMaskPresent:\n    #     img.DeepCopy(actual_img)\n    # img.SetDimensions(dim)\n    # img.SetOrigin(actual_img.GetOrigin())\n    # img.SetSpacing(actual_img.GetSpacing())\n    if not isMaskPresent:\n        f = var_name\n        data = val\n        arr = vtkFloatArray()\n        for j in range(n_pts):\n            arr.InsertNextValue(data[j])\n        arr.SetName(f)\n        img.GetPointData().SetScalars(arr)\n    else:\n        f = var_name\n        data = val\n        idx = 0\n        arr = vtkFloatArray()\n        for j in range(n_pts):\n            if(mask_arr[j] == 1):\n                arr.InsertNextValue(data[idx])\n                idx += 1\n            else:\n                arr.InsertNextValue(0.0)\n        arr.SetName(f)\n        img.GetPointData().SetScalars(arr)\n    return img","metadata":{"_uuid":"99a2cbb5-4737-4a6d-b398-c37e117de5a7","_cell_guid":"7042e60f-eafc-446d-87ab-ec4298dd9276","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.769381Z","iopub.execute_input":"2025-05-26T20:25:19.769639Z","iopub.status.idle":"2025-05-26T20:25:19.780625Z","shell.execute_reply.started":"2025-05-26T20:25:19.769609Z","shell.execute_reply":"2025-05-26T20:25:19.779903Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from argparse import Namespace\n\n# Parameters (simulating argparse in a Jupyter Notebook)\nargs = Namespace(\n    n_neurons=320,\n    n_layers=6,\n    epochs=200,  # Required argument: Set the number of epochs\n    batchsize=512,\n    lr=0.00005,\n    no_decay=False,\n    decay_rate=0.8,\n    decay_at_interval=True,\n    decay_interval=15,\n    datapath='/kaggle/input/fit-gmm3/gmm3.vti',  # Required: Set the path to your data\n    outpath='./models/',\n    exp_path='../logs/',\n    modified_data_path='./data/',\n    dataset_name='3d_data',  # Required: Set the dataset name\n    vti_name='predicted_vti',  # Required: Name of the dataset\n    vti_path='./data/pred.vti'\n)\n\nprint(args, end='\\n\\n')\n\n# Assigning parameters to variables\nLR = args.lr\nBATCH_SIZE = args.batchsize\ndecay_rate = args.decay_rate\ndecay_at_equal_interval = args.decay_at_interval\n\ndecay = not args.no_decay\nMAX_EPOCH = args.epochs\n\nn_neurons = args.n_neurons\nn_layers = args.n_layers + 2\ndecay_interval = args.decay_interval\noutpath = args.outpath\nexp_path = args.exp_path\ndatapath = args.datapath\nmodified_data_path = args.modified_data_path\ndataset_name = args.dataset_name\nvti_name = args.vti_name\nvti_path = args.vti_path\n\n# Displaying the final configuration\nprint(f\"Learning Rate: {LR}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Decay Rate: {decay_rate}\")\nprint(f\"Max Epochs: {MAX_EPOCH}\")\nprint(f\"Number of Neurons per Layer: {n_neurons}\")\nprint(f\"Number of Layers (including input/output): {n_layers}\")\nprint(f\"Data Path: {datapath}\")\nprint(f\"Output Path: {outpath}\")\nprint(f\"Dataset Name: {dataset_name}\")\nprint(f\"Vti Name: {vti_name}\")","metadata":{"_uuid":"07692dd4-4d61-4352-8b23-bff56509856c","_cell_guid":"4204ef20-76e1-416f-b261-20a979ca06e6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.781711Z","iopub.execute_input":"2025-05-26T20:25:19.782004Z","iopub.status.idle":"2025-05-26T20:25:19.797158Z","shell.execute_reply.started":"2025-05-26T20:25:19.781976Z","shell.execute_reply":"2025-05-26T20:25:19.796521Z"}},"outputs":[{"name":"stdout","text":"Namespace(n_neurons=320, n_layers=6, epochs=200, batchsize=512, lr=5e-05, no_decay=False, decay_rate=0.8, decay_at_interval=True, decay_interval=15, datapath='/kaggle/input/fit-gmm3/gmm3.vti', outpath='./models/', exp_path='../logs/', modified_data_path='./data/', dataset_name='3d_data', vti_name='predicted_vti', vti_path='./data/pred.vti')\n\nLearning Rate: 5e-05\nBatch Size: 512\nDecay Rate: 0.8\nMax Epochs: 200\nNumber of Neurons per Layer: 320\nNumber of Layers (including input/output): 8\nData Path: /kaggle/input/fit-gmm3/gmm3.vti\nOutput Path: ./models/\nDataset Name: 3d_data\nVti Name: predicted_vti\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Variable Initialization\nvar_name = []\ntotal_vars = None  # Number of variables\nunivariate = None  # True if dataset has one variable, else False\ngroup_size = 5000  # Group size during testing\n\n\n# Constructing the log file name\nlog_file = (\n    f'train_{dataset_name}_{n_layers-2}rb_{n_neurons}n_{BATCH_SIZE}bs_'\n    f'{LR}lr_{decay}decay_{decay_rate}dr_'\n    f'{\"decayingAtInterval\" + str(decay_interval) if decay_at_equal_interval else \"decayingWhenLossIncr\"}'\n)\n\nprint(log_file)","metadata":{"_uuid":"844184e9-6479-45d0-8d31-b1d3a9a4a116","_cell_guid":"cd516584-ec76-4358-888f-79e1116c82d2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.797890Z","iopub.execute_input":"2025-05-26T20:25:19.798155Z","iopub.status.idle":"2025-05-26T20:25:19.810981Z","shell.execute_reply.started":"2025-05-26T20:25:19.798129Z","shell.execute_reply":"2025-05-26T20:25:19.810268Z"}},"outputs":[{"name":"stdout","text":"train_3d_data_6rb_320n_512bs_5e-05lr_Truedecay_0.8dr_decayingAtInterval15\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"n_pts = None  # Number of points in the dataset\nn_dim = None  # Dimensionality of the data\ndim = None  # Other dimension-specific information\n\nprint(\"Decay:\", decay)\nprint(f'Extracting variables from path: {datapath}', end=\"\\n\\n\")\n\n# Placeholder for data\ndata_array = []\nscalar_data = None","metadata":{"_uuid":"bc8c0631-eaaf-4af9-acb9-35bd7467277a","_cell_guid":"c45f69dd-0524-4855-a64e-801e6bfb5765","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.811760Z","iopub.execute_input":"2025-05-26T20:25:19.812043Z","iopub.status.idle":"2025-05-26T20:25:19.825096Z","shell.execute_reply.started":"2025-05-26T20:25:19.812017Z","shell.execute_reply":"2025-05-26T20:25:19.824332Z"}},"outputs":[{"name":"stdout","text":"Decay: True\nExtracting variables from path: /kaggle/input/fit-gmm3/gmm3.vti\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# # Reading values from .vti files\n# reader = vtk.vtkXMLImageDataReader()\n# reader.SetFileName(datapath)\n# reader.Update()\n\n# data = reader.GetOutput()\n# scalar_data = data\n# pdata = data.GetPointData()\n# n_pts = data.GetNumberOfPoints()\n# dim = data.GetDimensions()\n# n_dim = len(dim)\n# total_arr = pdata.GetNumberOfArrays()\n\n# print(\"n_pts:\", n_pts, \"dim:\", dim, \"n_dim:\", n_dim, \"total_arr:\", total_arr)\n\n# mask_arr = []\n# valid_pts = 0\n# var_name = []\n# data_array = []\n\n# # Extracting data from the .vti file\n# for i in range(total_arr):\n#     a_name = pdata.GetArrayName(i)\n#     if a_name in ['vtkValidPointMask', 'Swirl']:\n#         continue\n    \n#     cur_arr = pdata.GetArray(a_name)\n#     n_components = cur_arr.GetNumberOfComponents()\n    \n#     if n_components == 1:\n#         var_name.append(a_name)\n#         data_array.append(vtk_to_numpy(cur_arr))\n#     else:\n#         component_names = [f\"{a_name}_{c}\" for c in ['x', 'y', 'z'][:n_components]]\n#         var_name.extend(component_names)\n#         for c in range(n_components):\n#             c_data = [cur_arr.GetComponent(j, c) for j in range(n_pts)]\n#             data_array.append(np.array(c_data))\n\n# valid_pts = n_pts  # Assume all points are valid for simplicity\n# total_vars = len(var_name)\n# univariate = total_vars == 1\n\n# # Prepare numpy arrays for coordinates and variable values\n# cord = np.zeros((valid_pts, n_dim))\n# val = np.zeros((valid_pts, total_vars))\n\n# # Store data in numpy arrays\n# for i in range(n_pts):\n#     pt = scalar_data.GetPoint(i)\n#     cord[i, :] = pt\n#     val[i, :] = [arr[i] for arr in data_array]\n\n# # Display final information\n# print(\"Total Variables:\", total_vars)\n# print(\"Univariate:\", univariate)\n# print(\"Coordinates Shape:\", cord.shape)\n# print(\"Values Shape:\", val.shape)\n\n# Reading values from .vti files\nreader = vtk.vtkXMLImageDataReader()\nreader.SetFileName(datapath)\nreader.Update()\n\ndata = reader.GetOutput()\nscalar_data = data\npdata = data.GetPointData()\nn_pts = data.GetNumberOfPoints()\ndim = data.GetDimensions()\nn_dim = len(dim)\ntotal_arr = pdata.GetNumberOfArrays()\n\nprint(\"n_pts:\", n_pts, \"dim:\", dim, \"n_dim:\", n_dim, \"total_arr:\", total_arr)\n\nvar_name = []\ndata_array = []\n\n# Extracting data from the .vti file\nfor i in range(total_arr):\n    a_name = pdata.GetArrayName(i)\n    \n    cur_arr = pdata.GetArray(a_name)\n    n_components = cur_arr.GetNumberOfComponents()\n    \n    if n_components == 1:\n        var_name.append(a_name)\n        data_array.append(vtk_to_numpy(cur_arr))\n    else:\n        component_names = [f\"{a_name}_{c}\" for c in ['x', 'y', 'z'][:n_components]]\n        var_name.extend(component_names)\n        for c in range(n_components):\n            c_data = [cur_arr.GetComponent(j, c) for j in range(n_pts)]\n            data_array.append(np.array(c_data))\n\ntotal_vars = len(var_name)\nunivariate = total_vars == 1\n\n# Prepare numpy arrays for coordinates and variable values\ncord = np.zeros((n_pts, n_dim))\nval = np.zeros((n_pts, total_vars))\n\n# Store data in numpy arrays\nfor i in range(n_pts):\n    pt = scalar_data.GetPoint(i)\n    cord[i, :] = pt\n    val[i, :] = [arr[i] for arr in data_array]\n\n# Display final information\nprint(\"Total Variables:\", total_vars)\nprint(\"Univariate:\", univariate)\nprint(\"Coordinates Shape:\", cord.shape)\nprint(\"Values Shape:\", val.shape)","metadata":{"_uuid":"01428998-749b-433d-8d00-0ad9ac906055","_cell_guid":"2f13ab5d-71b7-4829-8d28-4a08f4f9a1cf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:19.825827Z","iopub.execute_input":"2025-05-26T20:25:19.826013Z","iopub.status.idle":"2025-05-26T20:25:21.280626Z","shell.execute_reply.started":"2025-05-26T20:25:19.825996Z","shell.execute_reply":"2025-05-26T20:25:21.279633Z"}},"outputs":[{"name":"stdout","text":"n_pts: 262144 dim: (64, 64, 64) n_dim: 3 total_arr: 9\nTotal Variables: 9\nUnivariate: False\nCoordinates Shape: (262144, 3)\nValues Shape: (262144, 9)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# # Ensure modified data path exists\n# if not os.path.exists(modified_data_path):\n#     os.mkdir(modified_data_path)\n\n# Save raw coordinates and values\n# np.save(f'{modified_data_path}cord.npy', cord)\n# np.save(f'{modified_data_path}val.npy', val)\n\n# # Create copies of non-normalized data\n# nn_cord = cord.copy()\n# nn_val = val.copy()\n\n# === Separate Normalization for Values ===\n# We assume the variable order is:\n#   - Means: indices 0,1,2\n#   - Std Devs: indices 3,4,5\n#   - Weights: indices 6,7,8\n\n# # We'll store normalization parameters so that we can invert normalization later.\n# norm_params = {}\n# epsilon = 1e-8  # to avoid log(0)\n\n# # Normalize Means to [-1,1] using min–max normalization\n# for i in range(3):\n#     min_val = np.min(val[:, i])\n#     max_val = np.max(val[:, i])\n#     norm_params[var_name[i]] = (min_val, max_val)\n#     val[:, i] = 2.0 * ((val[:, i] - min_val) / (max_val - min_val) - 0.5)\n\n# # Normalize Std Devs: first take log, then min–max to [-1,1]\n# for i in range(3, 6):\n#     log_vals = np.log(val[:, i] + epsilon)\n#     min_val = np.min(log_vals)\n#     max_val = np.max(log_vals)\n#     norm_params[var_name[i]] = (min_val, max_val)\n#     val[:, i] = 2.0 * ((log_vals - min_val) / (max_val - min_val) - 0.5)\n\n# # Normalize Weights: take log, then min–max to [-1,1]\n# for i in range(6, 9):\n#     log_vals = np.log(val[:, i] + epsilon)\n#     min_val = np.min(log_vals)\n#     max_val = np.max(log_vals)\n#     norm_params[var_name[i]] = (min_val, max_val)\n#     val[:, i] = 2.0 * ((log_vals - min_val) / (max_val - min_val) - 0.5)\n\n# norm_params = {}\n\n# Normalize values between -1 and 1\nfor i in range(total_vars):\n    min_data = np.min(val[:, i])\n    max_data = np.max(val[:, i])\n    # norm_params[var_name[i]] = (min_data, max_data)\n    val[:, i] = 2.0 * ((val[:, i] - min_data) / (max_data - min_data) - 0.5)\n\n# Normalize Coordinates to [-1,1] \nfor i in range(n_dim):\n    # Use (dim[i]-1] so that coordinates go from 0 to dim[i]-1.\n    cord[:, i] = 2.0 * (cord[:, i] / (dim[i] - 1) - 0.5)\n\n# # Normalize coordinates between 0 and 1\n# for i in range(n_dim):\n#     cord[:, i] = cord[:, i] / dim[i]\n\n\n# # Save normalized values and coordinates\n# np.save(f'{modified_data_path}n_cord.npy', cord)\n# np.save(f'{modified_data_path}n_val.npy', val)\nn_cord = cord.copy()\nn_val = val.copy()\n\n# # Reload data for verification\n# n_cord = np.load(f'{modified_data_path}n_cord.npy')\n# n_val = np.load(f'{modified_data_path}n_val.npy')\n# cord = np.load(f'{modified_data_path}cord.npy')\n# val = np.load(f'{modified_data_path}val.npy')\n\n# Convert normalized data to PyTorch tensors\ntorch_coords = torch.from_numpy(n_cord)\ntorch_vals = torch.from_numpy(n_val)\n\n# Display dataset details\nprint('Dataset Name:', dataset_name)\nprint('Total Variables:', total_vars)\nprint('Variables Name:', var_name, end=\"\\n\\n\")\nprint('Total Points in Data:', n_pts)\nprint('Dimension of the Dataset:', dim)\nprint('Number of Dimensions:', n_dim)\nprint('Coordinate Tensor Shape:', torch_coords.shape)\nprint('Scalar Values Tensor Shape:', torch_vals.shape)\n\nprint('\\n###### Data setup is complete, now starting training ######\\n')","metadata":{"_uuid":"521f7f9c-9a01-41f9-a149-1d15ff3fd1b8","_cell_guid":"b0cbd8a2-2b80-4d5e-a135-f3c4b8b3c6c2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:21.281683Z","iopub.execute_input":"2025-05-26T20:25:21.282051Z","iopub.status.idle":"2025-05-26T20:25:21.356747Z","shell.execute_reply.started":"2025-05-26T20:25:21.282027Z","shell.execute_reply":"2025-05-26T20:25:21.356094Z"}},"outputs":[{"name":"stdout","text":"Dataset Name: 3d_data\nTotal Variables: 9\nVariables Name: ['GMM_Mean0', 'GMM_Mean1', 'GMM_Mean2', 'GMM_Std0', 'GMM_Std1', 'GMM_Std2', 'GMM_Weight0', 'GMM_Weight1', 'GMM_Weight2']\n\nTotal Points in Data: 262144\nDimension of the Dataset: (64, 64, 64)\nNumber of Dimensions: 3\nCoordinate Tensor Shape: torch.Size([262144, 3])\nScalar Values Tensor Shape: torch.Size([262144, 9])\n\n###### Data setup is complete, now starting training ######\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Prepare the DataLoader\ntrain_dataloader = DataLoader(\n    TensorDataset(torch_coords, torch_vals),\n    batch_size=BATCH_SIZE,\n    pin_memory=True,\n    shuffle=True,\n    num_workers=4\n)\n\n# Model configuration\nobj = {\n    'total_vars': total_vars,\n    'dim': n_dim,\n    'n_neurons': n_neurons,\n    'n_layers': n_layers\n}\n\n# Initialize the model, optimizer, and loss function\nmodel = MyResidualSirenNet(obj).to(device)\nprint(model)\n\noptimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999))\nprint(optimizer)\n\ncriterion = nn.MSELoss()\nprint(criterion)\n\n# Training configuration summary\nprint('\\nLearning Rate:', LR)\nprint('Max Epochs:', MAX_EPOCH)\nprint('Batch Size:', BATCH_SIZE)\nprint('Number of Hidden Layers:', obj['n_layers'] - 2)\nprint('Number of Neurons per Layer:', obj['n_neurons'])\n\nif decay:\n    print('Decay Rate:', decay_rate)\n    if decay_at_equal_interval:\n        print(f'Rate decays every {decay_interval} epochs.')\n    else:\n        print('Rate decays when the current epoch loss is greater than the previous epoch loss.')\nelse:\n    print('No decay!')\nprint()","metadata":{"_uuid":"ec0c651a-5dad-486b-9fb7-1ccbaaf98cda","_cell_guid":"adf3a3b0-29d8-4b33-ad38-babd78b228d1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:21.357490Z","iopub.execute_input":"2025-05-26T20:25:21.357712Z","iopub.status.idle":"2025-05-26T20:25:23.403911Z","shell.execute_reply.started":"2025-05-26T20:25:21.357694Z","shell.execute_reply":"2025-05-26T20:25:23.403094Z"}},"outputs":[{"name":"stdout","text":"MyResidualSirenNet(\n  (net_layers): ModuleList(\n    (0): SineLayer(\n      (linear): Linear(in_features=3, out_features=320, bias=True)\n    )\n    (1-6): 6 x ResidualSineLayer(\n      (linear_1): Linear(in_features=320, out_features=320, bias=True)\n      (linear_2): Linear(in_features=320, out_features=320, bias=True)\n    )\n    (7): Linear(in_features=320, out_features=9, bias=True)\n  )\n)\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 5e-05\n    maximize: False\n    weight_decay: 0\n)\nMSELoss()\n\nLearning Rate: 5e-05\nMax Epochs: 200\nBatch Size: 512\nNumber of Hidden Layers: 6\nNumber of Neurons per Layer: 320\nDecay Rate: 0.8\nRate decays every 15 epochs.\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"train_loss_list = []\nbest_epoch = -1\nbest_loss = 1e8\n\n# Ensure the output path exists\nif not os.path.exists(outpath):\n    os.makedirs(outpath)\n\n# Training loop\nfor epoch in range(MAX_EPOCH):\n    model.train()\n    temp_loss_list = []\n    start = time.time()\n\n    # Batch-by-batch training\n    for X_train, y_train in train_dataloader:\n        X_train = X_train.type(torch.float32).to(device)\n        y_train = y_train.type(torch.float32).to(device)\n\n        if univariate:\n            y_train = y_train.squeeze()\n\n        optimizer.zero_grad()\n        predictions = model(X_train)\n        predictions = predictions.squeeze()\n        loss = criterion(predictions, y_train)\n        loss.backward()\n        optimizer.step()\n\n        # Track batch loss\n        temp_loss_list.append(loss.detach().cpu().numpy())\n\n    # Calculate epoch loss\n    epoch_loss = np.average(temp_loss_list)\n\n    # Learning rate decay\n    if decay:\n        if decay_at_equal_interval:\n            if epoch >= decay_interval and epoch % decay_interval == 0:\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] *= decay_rate\n        # else:\n        #     if epoch > 0 and epoch_loss > train_loss_list[-1]:\n        #         for param_group in optimizer.param_groups:\n        #             param_group['lr'] *= decay_rate\n        if epoch > 0 and epoch_loss > train_loss_list[-1]:\n            for param_group in optimizer.param_groups:\n                param_group['lr'] *= decay_rate\n\n    # Track losses and best model\n    train_loss_list.append(epoch_loss)\n    if epoch_loss < best_loss:\n        best_loss = epoch_loss\n        best_epoch = epoch+1\n\n    end = time.time()\n    print(\n        f\"Epoch: {epoch + 1}/{MAX_EPOCH} | Train Loss: {train_loss_list[-1]} | \"\n        f\"Time: {round(end - start, 2)}s ({device}) | LR: {optimizer.param_groups[0]['lr']}\"\n    )\n\n    # Save model at intervals\n    if (epoch + 1) % 50 == 0:\n        model_name = (\n            f'train_{dataset_name}_{epoch + 1}ep_{n_layers - 2}rb_{n_neurons}n_'\n            f'{BATCH_SIZE}bs_{LR}lr_{decay}decay_{decay_rate}dr_'\n            f'{\"decayingAtInterval\" + str(decay_interval) if decay_at_equal_interval else \"decayingWhenLossIncr\"}'\n        )\n        torch.save(\n            {\"epoch\": epoch + 1, \"model_state_dict\": model.state_dict()},\n            os.path.join(outpath, f'{model_name}.pth')\n        )\n\n# Final summary\nprint('\\nEpoch with Least Loss:', best_epoch, '| Loss:', best_loss, '\\n')\n\n# Save the final model\nmodel_name = f'siren_compressor'\ntorch.save(\n    {\"epoch\": MAX_EPOCH, \"model_state_dict\": model.state_dict()},\n    os.path.join(outpath, f'{model_name}.pth')\n)","metadata":{"_uuid":"16645d61-b423-4e5a-a7d2-5f692a4f6102","_cell_guid":"1c6f398e-b851-4fad-b62e-4080114fa832","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:25:23.404654Z","iopub.execute_input":"2025-05-26T20:25:23.405027Z","iopub.status.idle":"2025-05-26T20:39:04.911778Z","shell.execute_reply.started":"2025-05-26T20:25:23.405000Z","shell.execute_reply":"2025-05-26T20:39:04.910700Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1/200 | Train Loss: 0.006002567708492279 | Time: 4.95s (cuda) | LR: 5e-05\nEpoch: 2/200 | Train Loss: 0.0028159478679299355 | Time: 4.36s (cuda) | LR: 5e-05\nEpoch: 3/200 | Train Loss: 0.0027356112841516733 | Time: 4.15s (cuda) | LR: 5e-05\nEpoch: 4/200 | Train Loss: 0.0026741097681224346 | Time: 4.24s (cuda) | LR: 5e-05\nEpoch: 5/200 | Train Loss: 0.0026362696662545204 | Time: 4.66s (cuda) | LR: 5e-05\nEpoch: 6/200 | Train Loss: 0.0026015834882855415 | Time: 4.6s (cuda) | LR: 5e-05\nEpoch: 7/200 | Train Loss: 0.002568003488704562 | Time: 4.81s (cuda) | LR: 5e-05\nEpoch: 8/200 | Train Loss: 0.00255525391548872 | Time: 5.02s (cuda) | LR: 5e-05\nEpoch: 9/200 | Train Loss: 0.002536132000386715 | Time: 4.83s (cuda) | LR: 5e-05\nEpoch: 10/200 | Train Loss: 0.002521958900615573 | Time: 4.57s (cuda) | LR: 5e-05\nEpoch: 11/200 | Train Loss: 0.0025006753858178854 | Time: 4.76s (cuda) | LR: 5e-05\nEpoch: 12/200 | Train Loss: 0.0024847574532032013 | Time: 4.08s (cuda) | LR: 5e-05\nEpoch: 13/200 | Train Loss: 0.0024678725749254227 | Time: 4.34s (cuda) | LR: 5e-05\nEpoch: 14/200 | Train Loss: 0.0024575842544436455 | Time: 4.24s (cuda) | LR: 5e-05\nEpoch: 15/200 | Train Loss: 0.0024560815654695034 | Time: 4.41s (cuda) | LR: 5e-05\nEpoch: 16/200 | Train Loss: 0.0024472197983413935 | Time: 4.44s (cuda) | LR: 4e-05\nEpoch: 17/200 | Train Loss: 0.002324853092432022 | Time: 4.38s (cuda) | LR: 4e-05\nEpoch: 18/200 | Train Loss: 0.002245389623567462 | Time: 4.23s (cuda) | LR: 4e-05\nEpoch: 19/200 | Train Loss: 0.0022312202490866184 | Time: 4.24s (cuda) | LR: 4e-05\nEpoch: 20/200 | Train Loss: 0.002231935039162636 | Time: 4.32s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 21/200 | Train Loss: 0.002124250400811434 | Time: 4.22s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 22/200 | Train Loss: 0.002051163464784622 | Time: 4.54s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 23/200 | Train Loss: 0.002026170026510954 | Time: 4.3s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 24/200 | Train Loss: 0.002005775924772024 | Time: 4.06s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 25/200 | Train Loss: 0.0019930521957576275 | Time: 4.13s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 26/200 | Train Loss: 0.0019843645859509706 | Time: 4.06s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 27/200 | Train Loss: 0.001973938662558794 | Time: 4.16s (cuda) | LR: 3.2000000000000005e-05\nEpoch: 28/200 | Train Loss: 0.001984905917197466 | Time: 4.05s (cuda) | LR: 2.5600000000000006e-05\nEpoch: 29/200 | Train Loss: 0.001881810836493969 | Time: 4.13s (cuda) | LR: 2.5600000000000006e-05\nEpoch: 30/200 | Train Loss: 0.0018433468649163842 | Time: 4.23s (cuda) | LR: 2.5600000000000006e-05\nEpoch: 31/200 | Train Loss: 0.0018543307669460773 | Time: 4.23s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 32/200 | Train Loss: 0.001637771027162671 | Time: 4.33s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 33/200 | Train Loss: 0.0014512850902974606 | Time: 4.27s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 34/200 | Train Loss: 0.001368640223518014 | Time: 4.08s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 35/200 | Train Loss: 0.0013041598722338676 | Time: 4.2s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 36/200 | Train Loss: 0.0012175661977380514 | Time: 4.02s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 37/200 | Train Loss: 0.0011595095274969935 | Time: 4.43s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 38/200 | Train Loss: 0.0011065348517149687 | Time: 4.87s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 39/200 | Train Loss: 0.001058195368386805 | Time: 4.49s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 40/200 | Train Loss: 0.001023635733872652 | Time: 4.26s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 41/200 | Train Loss: 0.0009908440988510847 | Time: 4.08s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 42/200 | Train Loss: 0.0009664648096077144 | Time: 4.01s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 43/200 | Train Loss: 0.0009341592667624354 | Time: 4.03s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 44/200 | Train Loss: 0.0009048788342624903 | Time: 4.13s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 45/200 | Train Loss: 0.0008891999023035169 | Time: 4.25s (cuda) | LR: 1.6384000000000008e-05\nEpoch: 46/200 | Train Loss: 0.0008630952797830105 | Time: 4.12s (cuda) | LR: 1.3107200000000007e-05\nEpoch: 47/200 | Train Loss: 0.000765289762057364 | Time: 4.28s (cuda) | LR: 1.3107200000000007e-05\nEpoch: 48/200 | Train Loss: 0.0006501779425889254 | Time: 4.32s (cuda) | LR: 1.3107200000000007e-05\nEpoch: 49/200 | Train Loss: 0.0006122781196609139 | Time: 4.08s (cuda) | LR: 1.3107200000000007e-05\nEpoch: 50/200 | Train Loss: 0.0006159698823466897 | Time: 4.29s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 51/200 | Train Loss: 0.0005632028914988041 | Time: 4.18s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 52/200 | Train Loss: 0.0004924911772832274 | Time: 4.09s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 53/200 | Train Loss: 0.00046973858843557537 | Time: 4.46s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 54/200 | Train Loss: 0.00046937415027059615 | Time: 4.24s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 55/200 | Train Loss: 0.0004655344528146088 | Time: 4.36s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 56/200 | Train Loss: 0.0004544148105196655 | Time: 4.04s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 57/200 | Train Loss: 0.0004388141678646207 | Time: 4.24s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 58/200 | Train Loss: 0.0004253564984537661 | Time: 4.12s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 59/200 | Train Loss: 0.0004156510694883764 | Time: 4.23s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 60/200 | Train Loss: 0.00040907535003498197 | Time: 4.28s (cuda) | LR: 1.0485760000000006e-05\nEpoch: 61/200 | Train Loss: 0.0003974759310949594 | Time: 4.32s (cuda) | LR: 8.388608000000005e-06\nEpoch: 62/200 | Train Loss: 0.0003468999348115176 | Time: 4.16s (cuda) | LR: 8.388608000000005e-06\nEpoch: 63/200 | Train Loss: 0.0002946519525721669 | Time: 4.29s (cuda) | LR: 8.388608000000005e-06\nEpoch: 64/200 | Train Loss: 0.0002863100089598447 | Time: 4.31s (cuda) | LR: 8.388608000000005e-06\nEpoch: 65/200 | Train Loss: 0.0002944767475128174 | Time: 4.13s (cuda) | LR: 6.7108864000000044e-06\nEpoch: 66/200 | Train Loss: 0.0002676946169231087 | Time: 4.3s (cuda) | LR: 6.7108864000000044e-06\nEpoch: 67/200 | Train Loss: 0.00023264222545549273 | Time: 4.29s (cuda) | LR: 6.7108864000000044e-06\nEpoch: 68/200 | Train Loss: 0.0002260165347252041 | Time: 4.37s (cuda) | LR: 6.7108864000000044e-06\nEpoch: 69/200 | Train Loss: 0.00022765964968129992 | Time: 4.09s (cuda) | LR: 5.368709120000004e-06\nEpoch: 70/200 | Train Loss: 0.00020842089725192636 | Time: 4.07s (cuda) | LR: 5.368709120000004e-06\nEpoch: 71/200 | Train Loss: 0.00018361785623710603 | Time: 4.01s (cuda) | LR: 5.368709120000004e-06\nEpoch: 72/200 | Train Loss: 0.00017819817003328353 | Time: 4.08s (cuda) | LR: 5.368709120000004e-06\nEpoch: 73/200 | Train Loss: 0.00017961293633561581 | Time: 4.06s (cuda) | LR: 4.294967296000004e-06\nEpoch: 74/200 | Train Loss: 0.00016413672710768878 | Time: 4.07s (cuda) | LR: 4.294967296000004e-06\nEpoch: 75/200 | Train Loss: 0.00014727396774105728 | Time: 4.18s (cuda) | LR: 4.294967296000004e-06\nEpoch: 76/200 | Train Loss: 0.0001434294244972989 | Time: 4.22s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 77/200 | Train Loss: 0.00013307848712429404 | Time: 4.08s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 78/200 | Train Loss: 0.00012270270963199437 | Time: 4.05s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 79/200 | Train Loss: 0.00012053732643835247 | Time: 4.24s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 80/200 | Train Loss: 0.0001200725237140432 | Time: 4.07s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 81/200 | Train Loss: 0.00011933367204619572 | Time: 4.37s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 82/200 | Train Loss: 0.00011783233640016988 | Time: 4.05s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 83/200 | Train Loss: 0.0001159443927463144 | Time: 4.33s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 84/200 | Train Loss: 0.0001132703764596954 | Time: 4.07s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 85/200 | Train Loss: 0.00011148497287649661 | Time: 4.18s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 86/200 | Train Loss: 0.00010977011697832495 | Time: 4.06s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 87/200 | Train Loss: 0.00010772912355605513 | Time: 4.05s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 88/200 | Train Loss: 0.00010613490303512663 | Time: 4.08s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 89/200 | Train Loss: 0.00010500736971152946 | Time: 4.29s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 90/200 | Train Loss: 0.00010354272671975195 | Time: 4.25s (cuda) | LR: 3.4359738368000033e-06\nEpoch: 91/200 | Train Loss: 0.00010208232561126351 | Time: 4.49s (cuda) | LR: 2.7487790694400027e-06\nEpoch: 92/200 | Train Loss: 9.21567261684686e-05 | Time: 3.99s (cuda) | LR: 2.7487790694400027e-06\nEpoch: 93/200 | Train Loss: 8.284874638775364e-05 | Time: 4.25s (cuda) | LR: 2.7487790694400027e-06\nEpoch: 94/200 | Train Loss: 8.134737436193973e-05 | Time: 4.11s (cuda) | LR: 2.7487790694400027e-06\nEpoch: 95/200 | Train Loss: 8.212525426642969e-05 | Time: 4.08s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 96/200 | Train Loss: 7.661944255232811e-05 | Time: 4.04s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 97/200 | Train Loss: 7.138548244256526e-05 | Time: 4.03s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 98/200 | Train Loss: 7.053982699289918e-05 | Time: 4.53s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 99/200 | Train Loss: 7.046283280942589e-05 | Time: 4.02s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 100/200 | Train Loss: 7.02614925103262e-05 | Time: 4.09s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 101/200 | Train Loss: 6.999392644502223e-05 | Time: 4.06s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 102/200 | Train Loss: 6.91718450980261e-05 | Time: 4.05s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 103/200 | Train Loss: 6.833579391241074e-05 | Time: 4.06s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 104/200 | Train Loss: 6.749757449142635e-05 | Time: 4.06s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 105/200 | Train Loss: 6.68051652610302e-05 | Time: 4.03s (cuda) | LR: 2.1990232555520023e-06\nEpoch: 106/200 | Train Loss: 6.607125396840274e-05 | Time: 4.39s (cuda) | LR: 1.7592186044416019e-06\nEpoch: 107/200 | Train Loss: 6.090610258979723e-05 | Time: 4.17s (cuda) | LR: 1.7592186044416019e-06\nEpoch: 108/200 | Train Loss: 5.665262506227009e-05 | Time: 4.4s (cuda) | LR: 1.7592186044416019e-06\nEpoch: 109/200 | Train Loss: 5.592462548520416e-05 | Time: 4.12s (cuda) | LR: 1.7592186044416019e-06\nEpoch: 110/200 | Train Loss: 5.5976845033001155e-05 | Time: 4.34s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 111/200 | Train Loss: 5.28023301740177e-05 | Time: 4.08s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 112/200 | Train Loss: 5.05077769048512e-05 | Time: 4.09s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 113/200 | Train Loss: 5.0147034926339984e-05 | Time: 4.03s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 114/200 | Train Loss: 5.0004618969978765e-05 | Time: 4.24s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 115/200 | Train Loss: 4.978063952876255e-05 | Time: 4.2s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 116/200 | Train Loss: 4.948839341523126e-05 | Time: 3.66s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 117/200 | Train Loss: 4.904521119897254e-05 | Time: 3.77s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 118/200 | Train Loss: 4.8630128731019795e-05 | Time: 3.68s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 119/200 | Train Loss: 4.8192261601798236e-05 | Time: 3.62s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 120/200 | Train Loss: 4.7741239541210234e-05 | Time: 3.61s (cuda) | LR: 1.4073748835532816e-06\nEpoch: 121/200 | Train Loss: 4.73067193524912e-05 | Time: 3.7s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 122/200 | Train Loss: 4.4469008571468294e-05 | Time: 4.03s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 123/200 | Train Loss: 4.27048398705665e-05 | Time: 3.71s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 124/200 | Train Loss: 4.242164141032845e-05 | Time: 3.75s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 125/200 | Train Loss: 4.234148218529299e-05 | Time: 3.77s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 126/200 | Train Loss: 4.216505476506427e-05 | Time: 3.66s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 127/200 | Train Loss: 4.1957286157412454e-05 | Time: 3.7s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 128/200 | Train Loss: 4.165830978308804e-05 | Time: 3.84s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 129/200 | Train Loss: 4.136038842261769e-05 | Time: 3.72s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 130/200 | Train Loss: 4.102449383935891e-05 | Time: 3.78s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 131/200 | Train Loss: 4.072675801580772e-05 | Time: 4.49s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 132/200 | Train Loss: 4.041651118313894e-05 | Time: 3.94s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 133/200 | Train Loss: 4.012400677311234e-05 | Time: 3.86s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 134/200 | Train Loss: 3.9849503082223237e-05 | Time: 3.98s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 135/200 | Train Loss: 3.954518979298882e-05 | Time: 3.96s (cuda) | LR: 1.1258999068426254e-06\nEpoch: 136/200 | Train Loss: 3.9315706089837477e-05 | Time: 3.92s (cuda) | LR: 9.007199254741003e-07\nEpoch: 137/200 | Train Loss: 3.7235135096125305e-05 | Time: 3.83s (cuda) | LR: 9.007199254741003e-07\nEpoch: 138/200 | Train Loss: 3.606964310165495e-05 | Time: 3.87s (cuda) | LR: 9.007199254741003e-07\nEpoch: 139/200 | Train Loss: 3.588066465454176e-05 | Time: 4.27s (cuda) | LR: 9.007199254741003e-07\nEpoch: 140/200 | Train Loss: 3.5835488233715296e-05 | Time: 4.02s (cuda) | LR: 9.007199254741003e-07\nEpoch: 141/200 | Train Loss: 3.569621912902221e-05 | Time: 4.17s (cuda) | LR: 9.007199254741003e-07\nEpoch: 142/200 | Train Loss: 3.554489740054123e-05 | Time: 4.06s (cuda) | LR: 9.007199254741003e-07\nEpoch: 143/200 | Train Loss: 3.535165888024494e-05 | Time: 4.06s (cuda) | LR: 9.007199254741003e-07\nEpoch: 144/200 | Train Loss: 3.516339711495675e-05 | Time: 3.99s (cuda) | LR: 9.007199254741003e-07\nEpoch: 145/200 | Train Loss: 3.495405690046027e-05 | Time: 3.89s (cuda) | LR: 9.007199254741003e-07\nEpoch: 146/200 | Train Loss: 3.474295954219997e-05 | Time: 4.02s (cuda) | LR: 9.007199254741003e-07\nEpoch: 147/200 | Train Loss: 3.45457301591523e-05 | Time: 3.95s (cuda) | LR: 9.007199254741003e-07\nEpoch: 148/200 | Train Loss: 3.432200173847377e-05 | Time: 3.99s (cuda) | LR: 9.007199254741003e-07\nEpoch: 149/200 | Train Loss: 3.412337173358537e-05 | Time: 4.17s (cuda) | LR: 9.007199254741003e-07\nEpoch: 150/200 | Train Loss: 3.393538645468652e-05 | Time: 3.99s (cuda) | LR: 9.007199254741003e-07\nEpoch: 151/200 | Train Loss: 3.3740892831701785e-05 | Time: 4.07s (cuda) | LR: 7.205759403792803e-07\nEpoch: 152/200 | Train Loss: 3.219117206754163e-05 | Time: 4.12s (cuda) | LR: 7.205759403792803e-07\nEpoch: 153/200 | Train Loss: 3.146251037833281e-05 | Time: 4.03s (cuda) | LR: 7.205759403792803e-07\nEpoch: 154/200 | Train Loss: 3.135908264084719e-05 | Time: 4.34s (cuda) | LR: 7.205759403792803e-07\nEpoch: 155/200 | Train Loss: 3.130175537080504e-05 | Time: 4.2s (cuda) | LR: 7.205759403792803e-07\nEpoch: 156/200 | Train Loss: 3.119807661278173e-05 | Time: 4.07s (cuda) | LR: 7.205759403792803e-07\nEpoch: 157/200 | Train Loss: 3.1086005037650466e-05 | Time: 4.03s (cuda) | LR: 7.205759403792803e-07\nEpoch: 158/200 | Train Loss: 3.095214196946472e-05 | Time: 4.08s (cuda) | LR: 7.205759403792803e-07\nEpoch: 159/200 | Train Loss: 3.080848546233028e-05 | Time: 4.44s (cuda) | LR: 7.205759403792803e-07\nEpoch: 160/200 | Train Loss: 3.064743577851914e-05 | Time: 4.41s (cuda) | LR: 7.205759403792803e-07\nEpoch: 161/200 | Train Loss: 3.052249303436838e-05 | Time: 4.05s (cuda) | LR: 7.205759403792803e-07\nEpoch: 162/200 | Train Loss: 3.0369092200999148e-05 | Time: 3.92s (cuda) | LR: 7.205759403792803e-07\nEpoch: 163/200 | Train Loss: 3.0208433599909768e-05 | Time: 3.75s (cuda) | LR: 7.205759403792803e-07\nEpoch: 164/200 | Train Loss: 3.0070093998801894e-05 | Time: 3.94s (cuda) | LR: 7.205759403792803e-07\nEpoch: 165/200 | Train Loss: 2.9937838917248882e-05 | Time: 3.74s (cuda) | LR: 7.205759403792803e-07\nEpoch: 166/200 | Train Loss: 2.980427052534651e-05 | Time: 3.95s (cuda) | LR: 5.764607523034243e-07\nEpoch: 167/200 | Train Loss: 2.8609676519408822e-05 | Time: 3.97s (cuda) | LR: 5.764607523034243e-07\nEpoch: 168/200 | Train Loss: 2.8175038096378557e-05 | Time: 3.82s (cuda) | LR: 5.764607523034243e-07\nEpoch: 169/200 | Train Loss: 2.810497971950099e-05 | Time: 3.73s (cuda) | LR: 5.764607523034243e-07\nEpoch: 170/200 | Train Loss: 2.803808638418559e-05 | Time: 3.96s (cuda) | LR: 5.764607523034243e-07\nEpoch: 171/200 | Train Loss: 2.795058571791742e-05 | Time: 3.73s (cuda) | LR: 5.764607523034243e-07\nEpoch: 172/200 | Train Loss: 2.7871268684975803e-05 | Time: 3.82s (cuda) | LR: 5.764607523034243e-07\nEpoch: 173/200 | Train Loss: 2.7774971385952085e-05 | Time: 3.84s (cuda) | LR: 5.764607523034243e-07\nEpoch: 174/200 | Train Loss: 2.766013130894862e-05 | Time: 3.77s (cuda) | LR: 5.764607523034243e-07\nEpoch: 175/200 | Train Loss: 2.755048990366049e-05 | Time: 3.77s (cuda) | LR: 5.764607523034243e-07\nEpoch: 176/200 | Train Loss: 2.744596167758573e-05 | Time: 3.82s (cuda) | LR: 5.764607523034243e-07\nEpoch: 177/200 | Train Loss: 2.7340447559254244e-05 | Time: 3.79s (cuda) | LR: 5.764607523034243e-07\nEpoch: 178/200 | Train Loss: 2.7236839741817676e-05 | Time: 3.84s (cuda) | LR: 5.764607523034243e-07\nEpoch: 179/200 | Train Loss: 2.712339482968673e-05 | Time: 3.98s (cuda) | LR: 5.764607523034243e-07\nEpoch: 180/200 | Train Loss: 2.7034686354454607e-05 | Time: 3.78s (cuda) | LR: 5.764607523034243e-07\nEpoch: 181/200 | Train Loss: 2.6924280973616987e-05 | Time: 3.76s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 182/200 | Train Loss: 2.602085078251548e-05 | Time: 3.85s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 183/200 | Train Loss: 2.5746274332050234e-05 | Time: 3.73s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 184/200 | Train Loss: 2.569510616012849e-05 | Time: 3.8s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 185/200 | Train Loss: 2.564062742749229e-05 | Time: 3.96s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 186/200 | Train Loss: 2.5576850021025166e-05 | Time: 3.86s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 187/200 | Train Loss: 2.5503770302748308e-05 | Time: 4.15s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 188/200 | Train Loss: 2.543412847444415e-05 | Time: 3.93s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 189/200 | Train Loss: 2.5358665880048648e-05 | Time: 3.96s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 190/200 | Train Loss: 2.527147444197908e-05 | Time: 4.01s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 191/200 | Train Loss: 2.52006921073189e-05 | Time: 3.97s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 192/200 | Train Loss: 2.5113182346103713e-05 | Time: 3.99s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 193/200 | Train Loss: 2.5039324100362137e-05 | Time: 3.94s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 194/200 | Train Loss: 2.4965811462607235e-05 | Time: 4.03s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 195/200 | Train Loss: 2.4886894607334398e-05 | Time: 4.14s (cuda) | LR: 4.6116860184273944e-07\nEpoch: 196/200 | Train Loss: 2.4811713956296444e-05 | Time: 3.98s (cuda) | LR: 3.689348814741916e-07\nEpoch: 197/200 | Train Loss: 2.4105418560793623e-05 | Time: 3.95s (cuda) | LR: 3.689348814741916e-07\nEpoch: 198/200 | Train Loss: 2.393302565906197e-05 | Time: 3.69s (cuda) | LR: 3.689348814741916e-07\nEpoch: 199/200 | Train Loss: 2.3888018404250033e-05 | Time: 3.78s (cuda) | LR: 3.689348814741916e-07\nEpoch: 200/200 | Train Loss: 2.384720392001327e-05 | Time: 3.79s (cuda) | LR: 3.689348814741916e-07\n\nEpoch with Least Loss: 200 | Loss: 2.3847204e-05 \n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Initialize prediction lists\nprediction_list = [[] for _ in range(total_vars)]\n\n# Inference loop\nwith torch.no_grad():\n    for i in range(0, torch_coords.shape[0], group_size):\n        coords = torch_coords[i:min(i + group_size, torch_coords.shape[0])].type(torch.float32).to(device)\n        vals = model(coords)\n        vals = vals.to('cpu')\n\n        for j in range(total_vars):\n            prediction_list[j].append(vals[:, j])\n\n# Extract and concatenate predictions\nextracted_list = [[] for _ in range(total_vars)]\nfor i in range(len(prediction_list[0])):\n    for j in range(total_vars):\n        el = prediction_list[j][i].detach().numpy()\n        extracted_list[j].append(el)\n\nfor j in range(total_vars):\n    extracted_list[j] = np.concatenate(extracted_list[j], dtype='float32')\n\n# Final prediction (normalized)\nn_predictions = np.array(extracted_list).T\n\n# Compute PSNR\nfindMultiVariatePSNR(var_name, total_vars, n_val, n_predictions)\n\n# Compute RMSE\nrmse = compute_rmse(n_val, n_predictions)\nprint(\"RMSE:\", rmse)","metadata":{"_uuid":"07a36d31-db47-4d2c-b11d-5339aa8cb204","_cell_guid":"cc00efa1-e7a5-4a3a-aa44-d056d987dcb8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:39:04.914949Z","iopub.execute_input":"2025-05-26T20:39:04.915203Z","iopub.status.idle":"2025-05-26T20:39:05.385328Z","shell.execute_reply.started":"2025-05-26T20:39:04.915181Z","shell.execute_reply":"2025-05-26T20:39:05.384428Z"}},"outputs":[{"name":"stdout","text":"GMM_Mean0  PSNR: 52.68794737380751\nGMM_Mean1  PSNR: 52.63045912726122\nGMM_Mean2  PSNR: 52.668430114274315\nGMM_Std0  PSNR: 52.376027636035396\nGMM_Std1  PSNR: 51.26992743348943\nGMM_Std2  PSNR: 51.87284498516785\nGMM_Weight0  PSNR: 53.63959011755643\nGMM_Weight1  PSNR: 53.50165143139896\nGMM_Weight2  PSNR: 53.640972809700294\n\nAverage psnr :  52.69865011429905\nRMSE: 0.004672115798452637\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# !rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T20:39:05.386663Z","iopub.execute_input":"2025-05-26T20:39:05.386998Z","iopub.status.idle":"2025-05-26T20:39:05.390530Z","shell.execute_reply.started":"2025-05-26T20:39:05.386966Z","shell.execute_reply":"2025-05-26T20:39:05.389710Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(os.path.getsize('/kaggle/working/models/train_3d_data_200ep_6rb_320n_512bs_5e-05lr_Truedecay_0.8dr_decayingAtInterval15.pth') / (1024 ** 2), 'MB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T20:42:22.099537Z","iopub.execute_input":"2025-05-26T20:42:22.099844Z","iopub.status.idle":"2025-05-26T20:42:22.105129Z","shell.execute_reply.started":"2025-05-26T20:42:22.099821Z","shell.execute_reply":"2025-05-26T20:42:22.104165Z"}},"outputs":[{"name":"stdout","text":"4.731752395629883 MB\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# # vti saving path\n# vti_path = args.vti_path\n# if not os.path.exists(vti_path):\n#     os.makedirs(vti_path)\n# # vti name\n# vti_name = args.vti_name\n# makeVTI(data, nn_val, n_predictions, n_pts, total_vars, var_name, dim, isMaskPresent, mask_arr, vti_path, vti_name)","metadata":{"_uuid":"8987b9b8-de2f-448c-995c-28bee1a033eb","_cell_guid":"6f9d6b16-aa9c-4808-81e8-a4d1e00459d2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-05-26T20:39:05.443004Z","iopub.status.idle":"2025-05-26T20:39:05.443267Z","shell.execute_reply":"2025-05-26T20:39:05.443158Z"}},"outputs":[],"execution_count":null}]}